{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d3416e-3aee-496b-9a46-d86fab3fe008",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70e0cff-cbad-458a-b79f-8c7f78b4d532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.layers import Input, concatenate, Embedding, Dropout, Dense, Conv1D, Activation, GRU, MaxPooling1D, GlobalMaxPooling1D, LeakyReLU\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dir_parts = os.getcwd().split(os.path.sep)\n",
    "root_index = dir_parts.index('MyHaSpeeDe-1')\n",
    "root_path = os.path.sep.join(dir_parts[:root_index + 1])\n",
    "sys.path.append(root_path + '/code/')\n",
    "from hyperparameter_tuning import bayesian_optimization, random_search\n",
    "from training.metrics import avg_f1\n",
    "from training.solver import Solver\n",
    "from sentence_statistics import average_sentence_length, max_sentence_length\n",
    "from word_embedding import get_key_index_mappings, get_embedding_matrix, get_key_index_pos_mappings, get_pos_matrix, sentence_to_embedding, data_to_embedding, pos_to_embedding\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84564330-d564-4700-9865-88576073b71b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a63dc0-4fdc-4705-9a04-0ac21931a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "fb_dir = root_path + '/data/facebook/'\n",
    "tw_dir = root_path + '/data/twitter/'\n",
    "preprocessed_dir = 'preprocessed/'\n",
    "w2v_dir = root_path + '/data/word2vec/'\n",
    "results_dir = root_path + '/results/Convolution-GRU/'\n",
    "\n",
    "# Filepaths (Facebook dataset)\n",
    "fb_dev_preprocessed_path = fb_dir + 'dev/' + preprocessed_dir + 'fb_dev_preprocessed.csv'\n",
    "fb_test_preprocessed_path = fb_dir + 'test/' + preprocessed_dir + 'fb_test_preprocessed.csv'\n",
    "\n",
    "# Filepaths (Twitter dataset)\n",
    "tw_dev_preprocessed_path = tw_dir + 'dev/' + preprocessed_dir + 'tw_dev_preprocessed.csv'\n",
    "tw_test_preprocessed_path = tw_dir + 'test/' + preprocessed_dir + 'tw_test_preprocessed.csv'\n",
    "\n",
    "# W2V + Corpus\n",
    "w2v_pretrained_path = w2v_dir + 'twitter128.bin' # w2v\n",
    "dictionary_path = root_path + '/data/italian_words.txt' # vocabulary\n",
    "bad_words_path = root_path + '/data/italian_bad_words.txt' # bad words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e8bfc-eca0-4202-8838-2fc0473d09ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task selection\n",
    "The model will be evaluated and fine-tuned w.r.t the three HaSpeeDe-1 tasks:\n",
    "- **Task 1 (HaSpeeDe-FB)**: only the FB dataset can be used to classify the FB test set;\n",
    "- **Task 2 (HaSpeeDe-TW)**: only the TW dataset can be used to classify the TW test set;\n",
    "- **Task 2 (Cross-HaspeeDe)**: only the FB dataset can be used to clasify the TW data set and viceversa (i.e. Cross-HaSpeeDe-FB and Cross-HasPeeDe-TW respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907756af-1ced-4edd-ad48-adba634c40a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Task(Enum):\n",
    "    HASPEEDE_FB = ('haspeede-fb', fb_dev_preprocessed_path, fb_test_preprocessed_path)\n",
    "    HASPEEDE_TW = ('haspeede-tw', tw_dev_preprocessed_path, tw_test_preprocessed_path)\n",
    "    CROSS_HASPEEDE_FB = ('cross-haspeede-fb', fb_dev_preprocessed_path, tw_test_preprocessed_path)\n",
    "    CROSS_HASPEEDE_TW = ('cross-haspeede-tw', tw_dev_preprocessed_path, fb_test_preprocessed_path)\n",
    "\n",
    "    def __init__(self, task_name, dev_path, test_path):\n",
    "        self.task_name = task_name\n",
    "        self.dev_path = dev_path\n",
    "        self.test_path = test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39704978-e26f-48a1-8074-c058a64e2853",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose task\n",
    "TASK = Task.HASPEEDE_FB\n",
    "#TASK = Task.HASPEEDE_TW\n",
    "#TASK = Task.CROSS_HASPEEDE_FB\n",
    "#TASK = Task.CROSS_HASPEEDE_TW\n",
    "\n",
    "task_name = TASK.task_name\n",
    "dev_path = TASK.dev_path\n",
    "test_path = TASK.test_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e06dd7-2865-47e4-88bc-45125d43f219",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a996d178-64df-4e0c-8abc-26fa0c0be8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72262d4b-d472-41e3-a19b-b4cc6af664a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Twitter dev/test dataset\n",
    "dev_inf = open(dev_path, encoding='utf-8')\n",
    "dev_data = pd.read_csv(dev_inf, sep=',', converters={'tokens': pd.eval, 'lemmas': pd.eval})\n",
    "\n",
    "test_inf = open(test_path, encoding='utf-8')\n",
    "test_data = pd.read_csv(test_inf, sep=',', converters={'tokens': pd.eval, 'lemmas': pd.eval})\n",
    "\n",
    "# Separate extra features\n",
    "dev_data_extra = dev_data.drop(['id', 'text', 'label', 'hashtags', 'tokens', 'lemmas', 'PoS', 'text_en'], axis=1, errors='ignore')\n",
    "test_data_extra = test_data.drop(['id', 'text', 'label', 'hashtags', 'tokens', 'lemmas', 'PoS', 'text_en'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65da1ac-fb2d-4b44-8993-5d7477b3b102",
   "metadata": {
    "tags": []
   },
   "source": [
    "## W2V Embedding\n",
    "Load pre-trained W2V model of Italian Twitter embeddings from the Italian NLP Lab [[1]](http://www.italianlp.it/resources/italian-word-embeddings/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872d44a6-fdf9-49b8-88d3-6ce66e0937b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OOV_TOKEN = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e841f4-8b46-430a-b245-7633b34cd783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W2V embedding\n",
    "w2v = KeyedVectors.load_word2vec_format(w2v_pretrained_path, binary=True)\n",
    "\n",
    "key_to_idx, idx_to_key = get_key_index_mappings(w2v, OOV_TOKEN)\n",
    "embedding_matrix, vocab_size = get_embedding_matrix(w2v, idx_to_key, OOV_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e108362-5c14-4fc9-8f81-d4121e2df2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = vocab_size\n",
    "EMB_DIMS = embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1a68b8-b436-4180-aa57-545a856552b8",
   "metadata": {},
   "source": [
    "## PoS Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6594c-1cd2-4488-9ea9-e58bd2639274",
   "metadata": {},
   "outputs": [],
   "source": [
    "OOV_TOKEN = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83190a89-7483-4f41-962f-55a562a9c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_idx_pos, idx_to_key_pos = get_key_index_pos_mappings(dev_data[\"PoS\"], OOV_TOKEN)\n",
    "idx_to_onehot_pos = get_pos_matrix(idx_to_key_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a0ec22-eea4-4c38-8af4-e4002f620698",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f6ced2-fa52-43e4-8d9d-0c92caaa46ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = max_sentence_length(dev_data['tokens']) # max sentence length for truncating/padding\n",
    "VAL_SPLIT = 0.2 # val set percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210012fe-3ea4-4ffe-aeaf-9e15e903d2a3",
   "metadata": {},
   "source": [
    "## Data to embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0855a73-7a4d-4a29-9ba3-91057520251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = data_to_embedding(dev_data['tokens'], embedding_matrix, key_to_idx, truncation=MAX_LEN, padding=True)\n",
    "X_dev_pos = pos_to_embedding(dev_data['PoS'], key_to_idx_pos, max_text_len=MAX_LEN)\n",
    "\n",
    "X_test = data_to_embedding(test_data['tokens'], embedding_matrix, key_to_idx, truncation=MAX_LEN, padding=True)\n",
    "X_test_pos = pos_to_embedding(test_data['PoS'], key_to_idx_pos, max_text_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9ada02-9501-4e08-8fa9-a3b318b27b1f",
   "metadata": {},
   "source": [
    "## Split Train-Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a75e888-c79d-4738-89d6-07b84e0e9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_train_pos, x_val_pos, x_train_extra, x_val_extra, y_train, y_val = train_test_split(X_dev, X_dev_pos, dev_data_extra.values , dev_data['label'], \n",
    "                                                                                                      test_size=VAL_SPLIT, random_state=128, stratify=dev_data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb1a507-b763-4ee0-b545-a0134f0a32c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Convolution-GRU\n",
    "Convolution-GRU based architecture inspired by the paper from Zhang et al. [[1]](https://link.springer.com/chapter/10.1007/978-3-319-93417-4_48).\n",
    "\n",
    "There are some changes to the architecture in the paper:\n",
    "\n",
    "**Word embedding**\n",
    "\n",
    "In the original paper sequences of text are truncated or padded to $100$ words in length. However, our datasets have maximum sentence length $< 100$, hence we apply padding to our sequences of text based on such max length. In this way we avoid losing information, and since our datasets are not excessively large we don't incur in time/space complexity issues.\n",
    "\n",
    "**Output Activation**\n",
    "\n",
    "We use *sigmoid* instead of *softmax* as a binary classification task is being considered (i.e. hatespeech or non-hatespeech)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8604d562-53b0-4b2a-a7ba-0964e82948fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'val_avg_f1' # optimization target\n",
    "\n",
    "# Train data\n",
    "input_train = {'text': x_train, 'PoS': x_train_pos, 'extra': x_train_extra}\n",
    "\n",
    "# Val data\n",
    "input_val = {'text': x_val, 'PoS': x_val_pos, 'extra': x_val_extra}\n",
    "\n",
    "# Dataset-specific dimensions\n",
    "POS_SHAPE = x_train_pos.shape\n",
    "EXTRA_SHAPE = dev_data_extra.shape\n",
    "\n",
    "# To-tune hyperparameters\n",
    "hparams = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e354c81-1cec-4519-98ed-be7952adcfc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1476f635-8f0f-45af-8c9f-261f33c8ad00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_convGRU_model(hparams):\n",
    "    \"\"\"Build the Convolution+GRU model.\"\"\"\n",
    "    \n",
    "    # Input\n",
    "    in_text = Input(name='text', shape=(MAX_LEN, EMB_DIMS,))\n",
    "    in_pos = Input(name='PoS', shape=(POS_SHAPE[1],))\n",
    "    in_extra = Input(name='extra', shape=(EXTRA_SHAPE[1],))\n",
    "        \n",
    "    # Embedding PoS layer\n",
    "    pos_emb = Embedding(24, 23, input_length=MAX_LEN)(in_pos)\n",
    "    \n",
    "    # Dropout + Conv + MaxPooling + GRU + GlobalMaxPooling (text)\n",
    "    l_conv_text = Dropout(hparams['dropout'])(in_text)\n",
    "    l_conv_text = Conv1D(filters=hparams['filters'], kernel_size=4)(l_conv_text)\n",
    "    l_conv_text = LeakyReLU(alpha=0.1)(l_conv_text)\n",
    "    l_conv_text = MaxPooling1D(pool_size=4)(l_conv_text)\n",
    "    gru_text = GRU(units=hparams['gru_units'], return_sequences=True)(l_conv_text)\n",
    "    gru_text = GlobalMaxPooling1D()(gru_text)\n",
    "    \n",
    "    # Dropout + Conv + MaxPooling + GRU + GlobalMaxPooling (PoS)\n",
    "    l_conv_pos = Dropout(hparams['dropout'])(pos_emb)\n",
    "    l_conv_pos = Conv1D(filters=hparams['filters'], kernel_size=4)(l_conv_pos)\n",
    "    l_conv_pos = LeakyReLU(alpha=0.1)(l_conv_pos)\n",
    "    l_conv_pos = MaxPooling1D(pool_size=4)(l_conv_pos)\n",
    "    gru_pos = GRU(units=hparams['gru_units'], return_sequences=True)(l_conv_pos)\n",
    "    gru_pos = GlobalMaxPooling1D()(gru_pos)\n",
    "    \n",
    "    # Concat text - PoS - extra features\n",
    "    input_merge = concatenate([gru_text, gru_pos, in_extra])\n",
    "    \n",
    "    # Dense layer\n",
    "    l_dense = Dropout(hparams['dropout'])(input_merge)\n",
    "    l_dense = Dense(units=hparams['h_dim'])(l_dense)\n",
    "    l_dense = LeakyReLU(alpha=0.1)(l_dense)\n",
    "    l_dense = Dropout(hparams['dropout'])(l_dense)\n",
    "    \n",
    "    \n",
    "    # Fully connected layer for binary classification with regularization (L2)\n",
    "    output_layer = Dense(1, activation='sigmoid', kernel_regularizer=l1_l2(l1=hparams['reg'], l2=hparams['reg']))(l_dense)\n",
    "    \n",
    "    model = Model(inputs=[in_text, in_pos, in_extra], outputs=output_layer)\n",
    "    optimizer = Adam(learning_rate=hparams['learning_rate'])\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[avg_f1])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f6e012-85ca-4278-9774-219dc1d887bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Hyper-parameters Tuning\n",
    "A common approach is to start with a coarse random searcg across a wide range of values to find promising sub-ranges of our parameter space. Then, we can zoom into these ranges and perform another random search (or a grid search) to finetune the configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcd4283-cd0c-4943-bebd-da224c9b2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_spaces = {\n",
    "    'learning_rate': ([1e-5, 1e-1], 'log'),\n",
    "    'filters': ([25, 50, 100], 'item'),\n",
    "    'gru_units': ([16, 32, 64, 128], 'item'),\n",
    "    'h_dim': ([16, 32, 64, 128], 'item'),\n",
    "    'dropout': ([0.0, 0.5], 'float'),\n",
    "    'reg': ([1e-5, 1e-1], 'log'),\n",
    "    'batch_size': ([16, 32, 64, 128], 'item')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e30b09-91c9-4782-b46d-d661772876ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b72746-077a-4d88-b2d2-d47c55ddcd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Optimization\n",
    "best_target, best_config = bayesian_optimization(\n",
    "    get_convGRU_model, input_train, y_train, input_val, y_val, \n",
    "    bayesian_optimization_spaces=hparams_spaces, TARGET=TARGET, N_TRIALS=50, EPOCHS=25, PATIENCE=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fbac69-80d3-415d-9ea4-5a5697f7bb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = best_config\n",
    "\n",
    "# set new intervals for fine-tune random search\n",
    "lr = hparams['learning_rate']\n",
    "filters = hparams['filters']\n",
    "gru_units = hparams['gru_units']\n",
    "h_dim = hparams['h_dim']\n",
    "dropout = hparams['dropout']\n",
    "reg = hparams['reg']\n",
    "batch_size = hparams['batch_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cc4ca3-da37-4944-9ba8-a90e63da6198",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Random search (fine-tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325d2528-0979-4deb-8871-f7554ec31646",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.2\n",
    "random_search_spaces_finetune = {\n",
    "    'learning_rate': ([10 ** (np.log10(lr) - epsilon), 10 ** (np.log10(lr) + epsilon)], 'float'),\n",
    "    'gru_units': ([gru_units], 'item'),\n",
    "    'filters': ([filters], 'item'),\n",
    "    'h_dim': ([h_dim], 'item'),\n",
    "    'dropout': ([10 ** (np.log10(dropout) - epsilon), 10 ** (np.log10(dropout) + epsilon)], 'float'),\n",
    "    'reg': ([10 ** (np.log10(reg) - epsilon), 10 ** (np.log10(reg) + epsilon)], 'float'),\n",
    "    'batch_size': ([batch_size], 'item'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada58c9a-c4c0-4333-87e3-4fd7e01a7f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search (fine-tune)\n",
    "best_config, best_model, results = random_search(\n",
    "    get_convGRU_model, input_train, y_train, input_val, y_val,\n",
    "    random_search_spaces=random_search_spaces_finetune, TARGET=TARGET, NUM_SEARCH=30, EPOCHS=25, PATIENCE=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1681ff12-8040-4001-a195-8741ed058a50",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save best configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bda701-48a7-41ad-bfa2-8f39f700b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TARGET == 'val_avg_f1':\n",
    "    new_best_target = max(results, key=lambda x: x[1][TARGET])[1][TARGET]\n",
    "    if new_best_target > best_target:\n",
    "        best_target = new_best_target\n",
    "        hparams = max(results, key=lambda x: x[1][TARGET])[0]\n",
    "        \n",
    "tuning_result = hparams.copy()\n",
    "tuning_result[TARGET] = best_target\n",
    "tuning_result['filters'] = int(tuning_result['filters'])\n",
    "tuning_result['gru_units'] = int(tuning_result['gru_units'])\n",
    "tuning_result['h_dim'] = int(tuning_result['h_dim'])\n",
    "tuning_result['batch_size'] = int(tuning_result['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14188b23-cad8-4aba-878a-83fb95c60a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store it\n",
    "output_path = results_dir + task_name + '/best_hparams.json'\n",
    "with open(output_path, 'w') as outf:\n",
    "    json.dump(tuning_result, outf, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def0e078-4969-4a52-ac3b-b4a217b958d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a61dfa9-a787-41d5-9e23-c7fdf941cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_dir + task_name + '/best_hparams.json', 'r') as inf:\n",
    "    hparams = json.load(inf)\n",
    "del hparams[TARGET]\n",
    "\n",
    "print(f'Config: {hparams}')\n",
    "model = get_convGRU_model(hparams)\n",
    "solver = Solver(model, input_train, y_train, input_val, y_val, TARGET)\n",
    "solver.train(epochs=50, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73273b73-a9a8-4618-b955-9277417a1ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = results_dir + task_name + '/history.png'\n",
    "solver.plot_history(out_path=out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5455d92-829c-4a74-9453-57cc2304b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(results_dir + task_name + '/best_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bc9af5-64e0-48a7-92c9-3e388d9374f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51ecb55-ca7e-468c-8b04-60db7a4aacc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = tf.keras.models.load_model(results_dir + task_name + '/best_model.keras', custom_objects={\"avg_f1\": avg_f1})\n",
    "\n",
    "input_test = {'text': X_test, 'PoS': X_test_pos, 'extra': test_data_extra.values}\n",
    "y_test = test_data['label']\n",
    "\n",
    "loss, metric = model.evaluate(input_test, y_test)\n",
    "print(f'Test loss: {loss} - Test {TARGET}: {metric}')\n",
    "string = f'Test loss: {loss} - Test {TARGET}: {metric}'\n",
    "\n",
    "y_pred = np.where(model.predict(input_test) > 0.5, 1, 0)\n",
    "report = classification_report(y_test, y_pred, digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0a1fef-5a08-49f1-9726-fdb72f6ced03",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_dir + task_name + '/test_eval.txt', 'w') as outf:\n",
    "    string = f\"Test Loss - Average F1 Score: {loss:.5f} - {metric:.5f}\\n {report}\"\n",
    "    outf.write(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede7a26c-8391-4a45-a3ff-b1dbab06ddbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## KFold + Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb239ce-8970-452c-9e96-25d8ef1acd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_dir + task_name + '/best_hparams.json', 'r') as inf:\n",
    "    hparams = json.load(inf)\n",
    "del hparams[TARGET]\n",
    "\n",
    "input_dev = {'text': X_dev, 'PoS': X_dev_pos, 'extra': dev_data_extra.values, 'label': dev_data['label']}\n",
    "\n",
    "print(f'Config: {hparams}')\n",
    "solver = Solver(None, input_train, y_train, input_val, y_val, TARGET)\n",
    "solver.train_with_kfold(get_convGRU_model, hparams, input_dev, n_splits=5, epochs=30, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3509e4f-fb66-4f62-ab36-36e9e2a04404",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test = {'text': X_test, 'PoS': X_test_pos, 'extra': test_data_extra.values}\n",
    "y_test = test_data['label']\n",
    "\n",
    "predictions = solver.ensemble_predict(input_test)\n",
    "report = classification_report(y_test, predictions, digits=4)\n",
    "avg_f1 = f1_score(y_test, predictions)\n",
    "\n",
    "print(f'Average F1 Score for Ensemble: {avg_f1:.5f}')\n",
    "print(f'\\n{report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ab9da-a903-4112-bb36-c09ca0b5b59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_dir + task_name + '/test_kfold_eval.txt', 'w') as outf:\n",
    "    string = f\"Average F1 Score for Ensemble: {avg_f1:.5f}\\n {report}\"\n",
    "    outf.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e5b94-b125-477a-9412-d716002e20af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
