{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d3416e-3aee-496b-9a46-d86fab3fe008",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4221e8e2-48b9-4a08-b95b-8f978a1a71c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from enum import Enum\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.layers import LeakyReLU, Reshape, GlobalMaxPooling1D, Input, concatenate, Embedding, Flatten, Dropout, Dense, Conv1D, Activation, BatchNormalization\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "dir_parts = os.getcwd().split(os.path.sep)\n",
    "root_index = dir_parts.index('MyHaSpeeDe-1')\n",
    "root_path = os.path.sep.join(dir_parts[:root_index + 1])\n",
    "sys.path.append(root_path + '/code/')\n",
    "from training.metrics import avg_f1\n",
    "from sentence_statistics import max_sentence_length, average_sentence_length\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84564330-d564-4700-9865-88576073b71b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "0b342ae0-1ef2-4bdb-9128-cd10ac83adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "fb_dir = root_path + '/data/facebook/'\n",
    "tw_dir = root_path + '/data/twitter/'\n",
    "results_dir = root_path + '/results/'\n",
    "\n",
    "# Filepaths (Facebook dataset)\n",
    "fb_dev_path = fb_dir + 'dev/' + 'fb_dev.csv'\n",
    "fb_test_path = fb_dir + 'test/' + 'fb_test.csv'\n",
    "\n",
    "# Filepaths (Twitter dataset)\n",
    "tw_dev_path = tw_dir + 'dev/' + 'tw_dev.csv'\n",
    "tw_test_path = tw_dir + 'test/' + 'tw_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e8bfc-eca0-4202-8838-2fc0473d09ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Task selection\n",
    "The model will be evaluated and fine-tuned w.r.t the three HaSpeeDe-1 tasks:\n",
    "- **Task 1 (HaSpeeDe-FB)**: only the FB dataset can be used to classify the FB test set;\n",
    "- **Task 2 (HaSpeeDe-TW)**: only the TW dataset can be used to classify the TW test set;\n",
    "- **Task 2 (Cross-HaspeeDe)**: only the FB dataset can be used to clasify the TW data set and viceversa (i.e. Cross-HaSpeeDe-FB and Cross-HasPeeDe-TW respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "907756af-1ced-4edd-ad48-adba634c40a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Task(Enum):\n",
    "    HASPEEDE_FB = ('haspeede-fb', fb_dev_path, fb_test_path)\n",
    "    HASPEEDE_TW = ('haspeede-tw', tw_dev_path, tw_test_path)\n",
    "    CROSS_HASPEEDE_FB = ('cross-haspeede-fb', fb_dev_path, tw_test_path)\n",
    "    CROSS_HASPEEDE_TW = ('cross-haspeede-tw', tw_dev_path, fb_test_path)\n",
    "\n",
    "    def __init__(self, task_name, dev_path, test_path):\n",
    "        self.task_name = task_name\n",
    "        self.dev_path = dev_path\n",
    "        self.test_path = test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "39704978-e26f-48a1-8074-c058a64e2853",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose task\n",
    "TASK = Task.HASPEEDE_FB\n",
    "#TASK = Task.HASPEEDE_TW\n",
    "#TASK = Task.CROSS_HASPEEDE_FB\n",
    "#TASK = Task.CROSS_HASPEEDE_TW\n",
    "\n",
    "task_name = TASK.task_name\n",
    "dev_path = TASK.dev_path\n",
    "test_path = TASK.test_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e06dd7-2865-47e4-88bc-45125d43f219",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "a996d178-64df-4e0c-8abc-26fa0c0be8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "8b9fcbd0-84c8-440d-ba19-4c5b454729f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Twitter dev/test dataset\n",
    "dev_inf = open(dev_path, encoding='utf-8')\n",
    "dev_data = pd.read_csv(dev_inf, sep=',', converters={'tokens': pd.eval, 'lemmas': pd.eval})\n",
    "dev_data = dev_data[['text', 'label']]\n",
    "\n",
    "test_inf = open(test_path, encoding='utf-8')\n",
    "test_data = pd.read_csv(test_inf, sep=',', converters={'tokens': pd.eval, 'lemmas': pd.eval})\n",
    "test_data = test_data[['text', 'label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9ada02-9501-4e08-8fa9-a3b318b27b1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Split Train-Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "88be881e-6cb8-4d68-a377-796527b43ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SPLIT = 0.2 # val set percentage\n",
    "x_train, x_val, y_train, y_val = train_test_split(dev_data['text'], dev_data['label'], stratify=dev_data['label'], test_size=VAL_SPLIT, random_state=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d454495f-b65f-4755-86f0-0ff046c450eb",
   "metadata": {},
   "source": [
    "# DBMDZ-italian-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "11ce61c2-0cbe-4cdb-acb8-a7d0473f8100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-italian-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_id = 'DBMDZ'\n",
    "model_name = 'dbmdz/bert-base-italian-uncased'\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fa323a-d574-4184-adc9-696e1ba956bd",
   "metadata": {},
   "source": [
    "## BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "6b37c122-59fc-4432-b561-837f03f1bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "d8d9d8b6-24f3-4145-8392-163c3b584769",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 64\n",
    "\n",
    "x_train_e = tokenizer(x_train.tolist(), truncation=True, padding=True, max_length=MAX_LEN, return_tensors='pt')\n",
    "x_val_e = tokenizer(x_val.tolist(), truncation=True, padding=True, max_length=MAX_LEN, return_tensors='pt')\n",
    "x_test_e = tokenizer(test_data['text'].tolist(), truncation=True, padding=True, max_length=MAX_LEN, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "2da298ef-cc58-4bad-a4bf-eda517c2bc37",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3, 3000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[324], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m VAL_SPLIT \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n\u001b[0;32m----> 3\u001b[0m train_texts, val_texts, train_labels, val_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdev_data_e\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdev_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVAL_SPLIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2646\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2644\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2646\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2648\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2649\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2650\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[1;32m   2651\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/sklearn/utils/validation.py:453\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    452\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[0;32m--> 453\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/sklearn/utils/validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    410\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3, 3000]"
     ]
    }
   ],
   "source": [
    "VAL_SPLIT = 0.2\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    dev_data_e,\n",
    "    dev_data['label'].tolist(),\n",
    "    test_size=VAL_SPLIT,\n",
    "    random_state=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26b95df-14af-4b41-ba5c-e45377d2749a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## To PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302b2eb4-f7d8-4dd6-94e5-215c59734df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, tokenized_inputs, labels=None):\n",
    "        self.tokenized_inputs = tokenized_inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': self.tokenized_inputs['input_ids'][idx],\n",
    "            'attention_mask': self.tokenized_inputs['attention_mask'][idx],\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "984c1317-5ec6-4c1e-b38a-4358d4447278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "train_dataset = HateSpeechDataset(x_train_e, y_train.tolist())\n",
    "val_dataset = HateSpeechDataset(x_val_e, y_val.tolist())\n",
    "test_dataset = HateSpeechDataset(x_test_e, test_data['label'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f9a39-29ce-40fb-b884-849642825238",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "4c8998d1-7313-4b1e-9e74-4f7da69ac474",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=16\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b85af-6c2b-48a5-a5c0-9d1a601bfc3e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "1bc25439-d19a-4cfa-9c49-a72a982fb23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Training:   0%|                                 | 0/150 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1 Training:   1%|▏                        | 1/150 [00:00<02:10,  1.14it/s]\u001b[A\n",
      "Epoch 1 Training:   1%|▎                        | 2/150 [00:01<02:02,  1.21it/s]\u001b[A\n",
      "Epoch 1 Training:   2%|▌                        | 3/150 [00:02<01:58,  1.24it/s]\u001b[A\n",
      "Epoch 1 Training:   3%|▋                        | 4/150 [00:03<01:55,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:   3%|▊                        | 5/150 [00:03<01:53,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:   4%|█                        | 6/150 [00:04<01:52,  1.28it/s]\u001b[A\n",
      "Epoch 1 Training:   5%|█▏                       | 7/150 [00:05<01:51,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:   5%|█▎                       | 8/150 [00:06<01:50,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:   6%|█▌                       | 9/150 [00:07<01:49,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:   7%|█▌                      | 10/150 [00:07<01:48,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:   7%|█▊                      | 11/150 [00:08<01:48,  1.28it/s]\u001b[A\n",
      "Epoch 1 Training:   8%|█▉                      | 12/150 [00:09<01:48,  1.28it/s]\u001b[A\n",
      "Epoch 1 Training:   9%|██                      | 13/150 [00:10<01:47,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:   9%|██▏                     | 14/150 [00:11<01:48,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  10%|██▍                     | 15/150 [00:11<01:48,  1.25it/s]\u001b[A\n",
      "Epoch 1 Training:  11%|██▌                     | 16/150 [00:12<01:47,  1.25it/s]\u001b[A\n",
      "Epoch 1 Training:  11%|██▋                     | 17/150 [00:13<01:47,  1.23it/s]\u001b[A\n",
      "Epoch 1 Training:  12%|██▉                     | 18/150 [00:14<01:45,  1.25it/s]\u001b[A\n",
      "Epoch 1 Training:  13%|███                     | 19/150 [00:15<01:43,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  13%|███▏                    | 20/150 [00:15<01:41,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  14%|███▎                    | 21/150 [00:16<01:40,  1.28it/s]\u001b[A\n",
      "Epoch 1 Training:  15%|███▌                    | 22/150 [00:17<01:39,  1.28it/s]\u001b[A\n",
      "Epoch 1 Training:  15%|███▋                    | 23/150 [00:18<01:39,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  16%|███▊                    | 24/150 [00:18<01:39,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  17%|████                    | 25/150 [00:19<01:38,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  17%|████▏                   | 26/150 [00:20<01:37,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  18%|████▎                   | 27/150 [00:21<01:37,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  19%|████▍                   | 28/150 [00:22<01:36,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  19%|████▋                   | 29/150 [00:22<01:36,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  20%|████▊                   | 30/150 [00:23<01:35,  1.25it/s]\u001b[A\n",
      "Epoch 1 Training:  21%|████▉                   | 31/150 [00:24<01:36,  1.23it/s]\u001b[A\n",
      "Epoch 1 Training:  21%|█████                   | 32/150 [00:25<01:35,  1.24it/s]\u001b[A\n",
      "Epoch 1 Training:  22%|█████▎                  | 33/150 [00:26<01:33,  1.25it/s]\u001b[A\n",
      "Epoch 1 Training:  23%|█████▍                  | 34/150 [00:26<01:32,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  23%|█████▌                  | 35/150 [00:27<01:30,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  24%|█████▊                  | 36/150 [00:28<01:30,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  25%|█████▉                  | 37/150 [00:29<01:28,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  25%|██████                  | 38/150 [00:30<01:27,  1.28it/s]\u001b[A\n",
      "Epoch 1 Training:  26%|██████▏                 | 39/150 [00:30<01:26,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  27%|██████▍                 | 40/150 [00:31<01:25,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  27%|██████▌                 | 41/150 [00:32<01:24,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  28%|██████▋                 | 42/150 [00:33<01:23,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  29%|██████▉                 | 43/150 [00:33<01:22,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  29%|███████                 | 44/150 [00:34<01:21,  1.30it/s]\u001b[A\n",
      "Epoch 1 Training:  30%|███████▏                | 45/150 [00:35<01:20,  1.30it/s]\u001b[A\n",
      "Epoch 1 Training:  31%|███████▎                | 46/150 [00:36<01:19,  1.30it/s]\u001b[A\n",
      "Epoch 1 Training:  31%|███████▌                | 47/150 [00:36<01:19,  1.30it/s]\u001b[A\n",
      "Epoch 1 Training:  32%|███████▋                | 48/150 [00:37<01:18,  1.30it/s]\u001b[A\n",
      "Epoch 1 Training:  33%|███████▊                | 49/150 [00:38<01:17,  1.31it/s]\u001b[A\n",
      "Epoch 1 Training:  33%|████████                | 50/150 [00:39<01:18,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  34%|████████▏               | 51/150 [00:40<01:18,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  35%|████████▎               | 52/150 [00:40<01:18,  1.25it/s]\u001b[A\n",
      "Epoch 1 Training:  35%|████████▍               | 53/150 [00:41<01:16,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  36%|████████▋               | 54/150 [00:42<01:15,  1.28it/s]\u001b[A\n",
      "Epoch 1 Training:  37%|████████▊               | 55/150 [00:43<01:14,  1.28it/s]\u001b[A\n",
      "Epoch 1 Training:  37%|████████▉               | 56/150 [00:44<01:14,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  38%|█████████               | 57/150 [00:44<01:12,  1.28it/s]\u001b[A\n",
      "Epoch 1 Training:  39%|█████████▎              | 58/150 [00:45<01:12,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  39%|█████████▍              | 59/150 [00:46<01:11,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  40%|█████████▌              | 60/150 [00:47<01:10,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  41%|█████████▊              | 61/150 [00:48<01:10,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  41%|█████████▉              | 62/150 [00:48<01:10,  1.25it/s]\u001b[A\n",
      "Epoch 1 Training:  42%|██████████              | 63/150 [00:49<01:08,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  43%|██████████▏             | 64/150 [00:50<01:07,  1.28it/s]\u001b[A\n",
      "Epoch 1 Training:  43%|██████████▍             | 65/150 [00:51<01:06,  1.28it/s]\u001b[A\n",
      "Epoch 1 Training:  44%|██████████▌             | 66/150 [00:51<01:05,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  45%|██████████▋             | 67/150 [00:52<01:04,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  45%|██████████▉             | 68/150 [00:53<01:03,  1.30it/s]\u001b[A\n",
      "Epoch 1 Training:  46%|███████████             | 69/150 [00:54<01:02,  1.30it/s]\u001b[A\n",
      "Epoch 1 Training:  47%|███████████▏            | 70/150 [00:55<01:01,  1.30it/s]\u001b[A\n",
      "Epoch 1 Training:  47%|███████████▎            | 71/150 [00:55<01:00,  1.30it/s]\u001b[A\n",
      "Epoch 1 Training:  48%|███████████▌            | 72/150 [00:56<01:00,  1.30it/s]\u001b[A\n",
      "Epoch 1 Training:  49%|███████████▋            | 73/150 [00:57<00:59,  1.30it/s]\u001b[A\n",
      "Epoch 1 Training:  49%|███████████▊            | 74/150 [00:58<00:58,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  50%|████████████            | 75/150 [00:58<00:58,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  51%|████████████▏           | 76/150 [00:59<00:57,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  51%|████████████▎           | 77/150 [01:00<00:56,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  52%|████████████▍           | 78/150 [01:01<00:55,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  53%|████████████▋           | 79/150 [01:01<00:55,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  53%|████████████▊           | 80/150 [01:02<00:54,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  54%|████████████▉           | 81/150 [01:03<00:53,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  55%|█████████████           | 82/150 [01:04<00:52,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  55%|█████████████▎          | 83/150 [01:05<00:52,  1.28it/s]\u001b[A\n",
      "Epoch 1 Training:  56%|█████████████▍          | 84/150 [01:05<00:51,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  57%|█████████████▌          | 85/150 [01:06<00:50,  1.30it/s]\u001b[A\n",
      "Epoch 1 Training:  57%|█████████████▊          | 86/150 [01:07<00:49,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  58%|█████████████▉          | 87/150 [01:08<00:48,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  59%|██████████████          | 88/150 [01:08<00:48,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  59%|██████████████▏         | 89/150 [01:09<00:48,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  60%|██████████████▍         | 90/150 [01:10<00:47,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  61%|██████████████▌         | 91/150 [01:11<00:46,  1.28it/s]\u001b[A\n",
      "Epoch 1 Training:  61%|██████████████▋         | 92/150 [01:12<00:45,  1.28it/s]\u001b[A\n",
      "Epoch 1 Training:  62%|██████████████▉         | 93/150 [01:12<00:44,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  63%|███████████████         | 94/150 [01:13<00:43,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  63%|███████████████▏        | 95/150 [01:14<00:42,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  64%|███████████████▎        | 96/150 [01:15<00:41,  1.30it/s]\u001b[A\n",
      "Epoch 1 Training:  65%|███████████████▌        | 97/150 [01:15<00:40,  1.30it/s]\u001b[A\n",
      "Epoch 1 Training:  65%|███████████████▋        | 98/150 [01:16<00:40,  1.30it/s]\u001b[A\n",
      "Epoch 1 Training:  66%|███████████████▊        | 99/150 [01:17<00:39,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  67%|███████████████▎       | 100/150 [01:18<00:39,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  67%|███████████████▍       | 101/150 [01:19<00:39,  1.25it/s]\u001b[A\n",
      "Epoch 1 Training:  68%|███████████████▋       | 102/150 [01:19<00:38,  1.25it/s]\u001b[A\n",
      "Epoch 1 Training:  69%|███████████████▊       | 103/150 [01:20<00:37,  1.25it/s]\u001b[A\n",
      "Epoch 1 Training:  69%|███████████████▉       | 104/150 [01:21<00:36,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  70%|████████████████       | 105/150 [01:22<00:35,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  71%|████████████████▎      | 106/150 [01:23<00:34,  1.28it/s]\u001b[A\n",
      "Epoch 1 Training:  71%|████████████████▍      | 107/150 [01:23<00:33,  1.28it/s]\u001b[A\n",
      "Epoch 1 Training:  72%|████████████████▌      | 108/150 [01:24<00:32,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  73%|████████████████▋      | 109/150 [01:25<00:31,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  73%|████████████████▊      | 110/150 [01:26<00:31,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  74%|█████████████████      | 111/150 [01:26<00:30,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  75%|█████████████████▏     | 112/150 [01:27<00:29,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  75%|█████████████████▎     | 113/150 [01:28<00:28,  1.28it/s]\u001b[A\n",
      "Epoch 1 Training:  76%|█████████████████▍     | 114/150 [01:29<00:27,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  77%|█████████████████▋     | 115/150 [01:30<00:27,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  77%|█████████████████▊     | 116/150 [01:30<00:26,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  78%|█████████████████▉     | 117/150 [01:31<00:25,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  79%|██████████████████     | 118/150 [01:32<00:24,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  79%|██████████████████▏    | 119/150 [01:33<00:24,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  80%|██████████████████▍    | 120/150 [01:34<00:24,  1.25it/s]\u001b[A\n",
      "Epoch 1 Training:  81%|██████████████████▌    | 121/150 [01:34<00:23,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  81%|██████████████████▋    | 122/150 [01:35<00:22,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  82%|██████████████████▊    | 123/150 [01:36<00:21,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  83%|███████████████████    | 124/150 [01:37<00:20,  1.25it/s]\u001b[A\n",
      "Epoch 1 Training:  83%|███████████████████▏   | 125/150 [01:37<00:19,  1.25it/s]\u001b[A\n",
      "Epoch 1 Training:  84%|███████████████████▎   | 126/150 [01:38<00:19,  1.25it/s]\u001b[A\n",
      "Epoch 1 Training:  85%|███████████████████▍   | 127/150 [01:39<00:18,  1.25it/s]\u001b[A\n",
      "Epoch 1 Training:  85%|███████████████████▋   | 128/150 [01:40<00:17,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  86%|███████████████████▊   | 129/150 [01:41<00:16,  1.25it/s]\u001b[A\n",
      "Epoch 1 Training:  87%|███████████████████▉   | 130/150 [01:41<00:15,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  87%|████████████████████   | 131/150 [01:42<00:14,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  88%|████████████████████▏  | 132/150 [01:43<00:14,  1.28it/s]\u001b[A\n",
      "Epoch 1 Training:  89%|████████████████████▍  | 133/150 [01:44<00:13,  1.28it/s]\u001b[A\n",
      "Epoch 1 Training:  89%|████████████████████▌  | 134/150 [01:45<00:12,  1.29it/s]\u001b[A\n",
      "Epoch 1 Training:  90%|████████████████████▋  | 135/150 [01:45<00:11,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  91%|████████████████████▊  | 136/150 [01:46<00:11,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  91%|█████████████████████  | 137/150 [01:47<00:10,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  92%|█████████████████████▏ | 138/150 [01:48<00:09,  1.25it/s]\u001b[A\n",
      "Epoch 1 Training:  93%|█████████████████████▎ | 139/150 [01:49<00:08,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  93%|█████████████████████▍ | 140/150 [01:49<00:07,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  94%|█████████████████████▌ | 141/150 [01:50<00:07,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  95%|█████████████████████▊ | 142/150 [01:51<00:06,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  95%|█████████████████████▉ | 143/150 [01:52<00:05,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  96%|██████████████████████ | 144/150 [01:53<00:04,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  97%|██████████████████████▏| 145/150 [01:53<00:03,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training:  97%|██████████████████████▍| 146/150 [01:54<00:03,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  98%|██████████████████████▌| 147/150 [01:55<00:02,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  99%|██████████████████████▋| 148/150 [01:56<00:01,  1.26it/s]\u001b[A\n",
      "Epoch 1 Training:  99%|██████████████████████▊| 149/150 [01:56<00:00,  1.27it/s]\u001b[A\n",
      "Epoch 1 Training: 100%|███████████████████████| 150/150 [01:57<00:00,  1.27it/s]\u001b[A\n",
      "\n",
      "Epoch 1 Validation:   0%|                                | 0/38 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1 Validation:   3%|▋                       | 1/38 [00:00<00:08,  4.35it/s]\u001b[A\n",
      "Epoch 1 Validation:   5%|█▎                      | 2/38 [00:00<00:07,  4.53it/s]\u001b[A\n",
      "Epoch 1 Validation:   8%|█▉                      | 3/38 [00:00<00:07,  4.83it/s]\u001b[A\n",
      "Epoch 1 Validation:  11%|██▌                     | 4/38 [00:00<00:06,  5.03it/s]\u001b[A\n",
      "Epoch 1 Validation:  13%|███▏                    | 5/38 [00:01<00:06,  5.17it/s]\u001b[A\n",
      "Epoch 1 Validation:  16%|███▊                    | 6/38 [00:01<00:06,  5.23it/s]\u001b[A\n",
      "Epoch 1 Validation:  18%|████▍                   | 7/38 [00:01<00:05,  5.31it/s]\u001b[A\n",
      "Epoch 1 Validation:  21%|█████                   | 8/38 [00:01<00:05,  5.38it/s]\u001b[A\n",
      "Epoch 1 Validation:  24%|█████▋                  | 9/38 [00:01<00:05,  5.41it/s]\u001b[A\n",
      "Epoch 1 Validation:  26%|██████                 | 10/38 [00:01<00:05,  5.42it/s]\u001b[A\n",
      "Epoch 1 Validation:  29%|██████▋                | 11/38 [00:02<00:04,  5.42it/s]\u001b[A\n",
      "Epoch 1 Validation:  32%|███████▎               | 12/38 [00:02<00:04,  5.42it/s]\u001b[A\n",
      "Epoch 1 Validation:  34%|███████▊               | 13/38 [00:02<00:04,  5.42it/s]\u001b[A\n",
      "Epoch 1 Validation:  37%|████████▍              | 14/38 [00:02<00:04,  5.42it/s]\u001b[A\n",
      "Epoch 1 Validation:  39%|█████████              | 15/38 [00:02<00:04,  5.44it/s]\u001b[A\n",
      "Epoch 1 Validation:  42%|█████████▋             | 16/38 [00:03<00:04,  5.44it/s]\u001b[A\n",
      "Epoch 1 Validation:  45%|██████████▎            | 17/38 [00:03<00:03,  5.46it/s]\u001b[A\n",
      "Epoch 1 Validation:  47%|██████████▉            | 18/38 [00:03<00:03,  5.48it/s]\u001b[A\n",
      "Epoch 1 Validation:  50%|███████████▌           | 19/38 [00:03<00:03,  5.41it/s]\u001b[A\n",
      "Epoch 1 Validation:  53%|████████████           | 20/38 [00:03<00:03,  5.36it/s]\u001b[A\n",
      "Epoch 1 Validation:  55%|████████████▋          | 21/38 [00:03<00:03,  5.39it/s]\u001b[A\n",
      "Epoch 1 Validation:  58%|█████████████▎         | 22/38 [00:04<00:02,  5.41it/s]\u001b[A\n",
      "Epoch 1 Validation:  61%|█████████████▉         | 23/38 [00:04<00:02,  5.37it/s]\u001b[A\n",
      "Epoch 1 Validation:  63%|██████████████▌        | 24/38 [00:04<00:02,  5.38it/s]\u001b[A\n",
      "Epoch 1 Validation:  66%|███████████████▏       | 25/38 [00:04<00:02,  5.38it/s]\u001b[A\n",
      "Epoch 1 Validation:  68%|███████████████▋       | 26/38 [00:04<00:02,  5.33it/s]\u001b[A\n",
      "Epoch 1 Validation:  71%|████████████████▎      | 27/38 [00:05<00:02,  5.32it/s]\u001b[A\n",
      "Epoch 1 Validation:  74%|████████████████▉      | 28/38 [00:05<00:01,  5.31it/s]\u001b[A\n",
      "Epoch 1 Validation:  76%|█████████████████▌     | 29/38 [00:05<00:01,  5.34it/s]\u001b[A\n",
      "Epoch 1 Validation:  79%|██████████████████▏    | 30/38 [00:05<00:01,  5.29it/s]\u001b[A\n",
      "Epoch 1 Validation:  82%|██████████████████▊    | 31/38 [00:05<00:01,  5.29it/s]\u001b[A\n",
      "Epoch 1 Validation:  84%|███████████████████▎   | 32/38 [00:06<00:01,  5.27it/s]\u001b[A\n",
      "Epoch 1 Validation:  87%|███████████████████▉   | 33/38 [00:06<00:00,  5.24it/s]\u001b[A\n",
      "Epoch 1 Validation:  89%|████████████████████▌  | 34/38 [00:06<00:00,  5.24it/s]\u001b[A\n",
      "Epoch 1 Validation:  92%|█████████████████████▏ | 35/38 [00:06<00:00,  4.98it/s]\u001b[A\n",
      "Epoch 1 Validation:  95%|█████████████████████▊ | 36/38 [00:06<00:00,  5.04it/s]\u001b[A\n",
      "Epoch 1 Validation:  97%|██████████████████████▍| 37/38 [00:07<00:00,  5.04it/s]\u001b[A\n",
      "Epoch 1 Validation: 100%|███████████████████████| 38/38 [00:07<00:00,  5.33it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.1006, Training Macro F1: 0.9782, Validation Loss: 0.6224, Validation Macro F1: 0.8367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Training:   0%|                                 | 0/150 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2 Training:   1%|▏                        | 1/150 [00:00<01:58,  1.26it/s]\u001b[A\n",
      "Epoch 2 Training:   1%|▎                        | 2/150 [00:01<01:57,  1.26it/s]\u001b[A\n",
      "Epoch 2 Training:   2%|▌                        | 3/150 [00:02<01:55,  1.27it/s]\u001b[A\n",
      "Epoch 2 Training:   3%|▋                        | 4/150 [00:03<01:53,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:   3%|▊                        | 5/150 [00:03<01:52,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:   4%|█                        | 6/150 [00:04<01:51,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:   5%|█▏                       | 7/150 [00:05<01:51,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:   5%|█▎                       | 8/150 [00:06<01:50,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:   6%|█▌                       | 9/150 [00:07<01:49,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:   7%|█▌                      | 10/150 [00:07<01:48,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:   7%|█▊                      | 11/150 [00:08<01:47,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:   8%|█▉                      | 12/150 [00:09<01:46,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:   9%|██                      | 13/150 [00:10<01:47,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:   9%|██▏                     | 14/150 [00:10<01:46,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  10%|██▍                     | 15/150 [00:11<01:45,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  11%|██▌                     | 16/150 [00:12<01:45,  1.27it/s]\u001b[A\n",
      "Epoch 2 Training:  11%|██▋                     | 17/150 [00:13<01:44,  1.27it/s]\u001b[A\n",
      "Epoch 2 Training:  12%|██▉                     | 18/150 [00:14<01:43,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  13%|███                     | 19/150 [00:14<01:42,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  13%|███▏                    | 20/150 [00:15<01:41,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  14%|███▎                    | 21/150 [00:16<01:40,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  15%|███▌                    | 22/150 [00:17<01:39,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  15%|███▋                    | 23/150 [00:17<01:38,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  16%|███▊                    | 24/150 [00:18<01:38,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  17%|████                    | 25/150 [00:19<01:38,  1.27it/s]\u001b[A\n",
      "Epoch 2 Training:  17%|████▏                   | 26/150 [00:20<01:36,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  18%|████▎                   | 27/150 [00:21<01:36,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  19%|████▍                   | 28/150 [00:21<01:35,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  19%|████▋                   | 29/150 [00:22<01:34,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  20%|████▊                   | 30/150 [00:23<01:33,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  21%|████▉                   | 31/150 [00:24<01:32,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  21%|█████                   | 32/150 [00:24<01:31,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  22%|█████▎                  | 33/150 [00:25<01:30,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  23%|█████▍                  | 34/150 [00:26<01:30,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  23%|█████▌                  | 35/150 [00:27<01:29,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  24%|█████▊                  | 36/150 [00:28<01:28,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  25%|█████▉                  | 37/150 [00:28<01:27,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  25%|██████                  | 38/150 [00:29<01:26,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  26%|██████▏                 | 39/150 [00:30<01:26,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  27%|██████▍                 | 40/150 [00:31<01:25,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  27%|██████▌                 | 41/150 [00:31<01:24,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  28%|██████▋                 | 42/150 [00:32<01:23,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  29%|██████▉                 | 43/150 [00:33<01:22,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  29%|███████                 | 44/150 [00:34<01:22,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  30%|███████▏                | 45/150 [00:35<01:21,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  31%|███████▎                | 46/150 [00:35<01:20,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  31%|███████▌                | 47/150 [00:36<01:19,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  32%|███████▋                | 48/150 [00:37<01:19,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  33%|███████▊                | 49/150 [00:38<01:18,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  33%|████████                | 50/150 [00:38<01:17,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  34%|████████▏               | 51/150 [00:39<01:16,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  35%|████████▎               | 52/150 [00:40<01:16,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  35%|████████▍               | 53/150 [00:41<01:16,  1.27it/s]\u001b[A\n",
      "Epoch 2 Training:  36%|████████▋               | 54/150 [00:42<01:15,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  37%|████████▊               | 55/150 [00:42<01:14,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  37%|████████▉               | 56/150 [00:43<01:13,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  38%|█████████               | 57/150 [00:44<01:12,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  39%|█████████▎              | 58/150 [00:45<01:11,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  39%|█████████▍              | 59/150 [00:45<01:11,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  40%|█████████▌              | 60/150 [00:46<01:10,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  41%|█████████▊              | 61/150 [00:47<01:09,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  41%|█████████▉              | 62/150 [00:48<01:09,  1.27it/s]\u001b[A\n",
      "Epoch 2 Training:  42%|██████████              | 63/150 [00:49<01:08,  1.27it/s]\u001b[A\n",
      "Epoch 2 Training:  43%|██████████▏             | 64/150 [00:49<01:08,  1.26it/s]\u001b[A\n",
      "Epoch 2 Training:  43%|██████████▍             | 65/150 [00:50<01:07,  1.26it/s]\u001b[A\n",
      "Epoch 2 Training:  44%|██████████▌             | 66/150 [00:51<01:06,  1.27it/s]\u001b[A\n",
      "Epoch 2 Training:  45%|██████████▋             | 67/150 [00:52<01:05,  1.26it/s]\u001b[A\n",
      "Epoch 2 Training:  45%|██████████▉             | 68/150 [00:53<01:04,  1.27it/s]\u001b[A\n",
      "Epoch 2 Training:  46%|███████████             | 69/150 [00:53<01:03,  1.27it/s]\u001b[A\n",
      "Epoch 2 Training:  47%|███████████▏            | 70/150 [00:54<01:03,  1.26it/s]\u001b[A\n",
      "Epoch 2 Training:  47%|███████████▎            | 71/150 [00:55<01:02,  1.26it/s]\u001b[A\n",
      "Epoch 2 Training:  48%|███████████▌            | 72/150 [00:56<01:02,  1.26it/s]\u001b[A\n",
      "Epoch 2 Training:  49%|███████████▋            | 73/150 [00:57<01:01,  1.24it/s]\u001b[A\n",
      "Epoch 2 Training:  49%|███████████▊            | 74/150 [00:57<01:00,  1.25it/s]\u001b[A\n",
      "Epoch 2 Training:  50%|████████████            | 75/150 [00:58<00:59,  1.26it/s]\u001b[A\n",
      "Epoch 2 Training:  51%|████████████▏           | 76/150 [00:59<00:58,  1.27it/s]\u001b[A\n",
      "Epoch 2 Training:  51%|████████████▎           | 77/150 [01:00<00:57,  1.27it/s]\u001b[A\n",
      "Epoch 2 Training:  52%|████████████▍           | 78/150 [01:01<00:57,  1.26it/s]\u001b[A\n",
      "Epoch 2 Training:  53%|████████████▋           | 79/150 [01:01<00:57,  1.23it/s]\u001b[A\n",
      "Epoch 2 Training:  53%|████████████▊           | 80/150 [01:02<00:56,  1.23it/s]\u001b[A\n",
      "Epoch 2 Training:  54%|████████████▉           | 81/150 [01:03<00:55,  1.25it/s]\u001b[A\n",
      "Epoch 2 Training:  55%|█████████████           | 82/150 [01:04<00:53,  1.27it/s]\u001b[A\n",
      "Epoch 2 Training:  55%|█████████████▎          | 83/150 [01:05<00:52,  1.27it/s]\u001b[A\n",
      "Epoch 2 Training:  56%|█████████████▍          | 84/150 [01:05<00:51,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  57%|█████████████▌          | 85/150 [01:06<00:50,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  57%|█████████████▊          | 86/150 [01:07<00:49,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  58%|█████████████▉          | 87/150 [01:08<00:48,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  59%|██████████████          | 88/150 [01:08<00:47,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  59%|██████████████▏         | 89/150 [01:09<00:47,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  60%|██████████████▍         | 90/150 [01:10<00:46,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  61%|██████████████▌         | 91/150 [01:11<00:45,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  61%|██████████████▋         | 92/150 [01:11<00:44,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  62%|██████████████▉         | 93/150 [01:12<00:44,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  63%|███████████████         | 94/150 [01:13<00:43,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  63%|███████████████▏        | 95/150 [01:14<00:42,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  64%|███████████████▎        | 96/150 [01:15<00:41,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  65%|███████████████▌        | 97/150 [01:15<00:41,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  65%|███████████████▋        | 98/150 [01:16<00:40,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  66%|███████████████▊        | 99/150 [01:17<00:39,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  67%|███████████████▎       | 100/150 [01:18<00:38,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  67%|███████████████▍       | 101/150 [01:18<00:38,  1.27it/s]\u001b[A\n",
      "Epoch 2 Training:  68%|███████████████▋       | 102/150 [01:19<00:37,  1.27it/s]\u001b[A\n",
      "Epoch 2 Training:  69%|███████████████▊       | 103/150 [01:20<00:37,  1.26it/s]\u001b[A\n",
      "Epoch 2 Training:  69%|███████████████▉       | 104/150 [01:21<00:36,  1.27it/s]\u001b[A\n",
      "Epoch 2 Training:  70%|████████████████       | 105/150 [01:22<00:35,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  71%|████████████████▎      | 106/150 [01:22<00:34,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  71%|████████████████▍      | 107/150 [01:23<00:33,  1.28it/s]\u001b[A\n",
      "Epoch 2 Training:  72%|████████████████▌      | 108/150 [01:24<00:32,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  73%|████████████████▋      | 109/150 [01:25<00:31,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  73%|████████████████▊      | 110/150 [01:25<00:30,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  74%|█████████████████      | 111/150 [01:26<00:30,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  75%|█████████████████▏     | 112/150 [01:27<00:29,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  75%|█████████████████▎     | 113/150 [01:28<00:28,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  76%|█████████████████▍     | 114/150 [01:29<00:27,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  77%|█████████████████▋     | 115/150 [01:29<00:27,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  77%|█████████████████▊     | 116/150 [01:30<00:26,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  78%|█████████████████▉     | 117/150 [01:31<00:25,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  79%|██████████████████     | 118/150 [01:32<00:24,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  79%|██████████████████▏    | 119/150 [01:32<00:23,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  80%|██████████████████▍    | 120/150 [01:33<00:23,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  81%|██████████████████▌    | 121/150 [01:34<00:22,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  81%|██████████████████▋    | 122/150 [01:35<00:21,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  82%|██████████████████▊    | 123/150 [01:36<00:20,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  83%|███████████████████    | 124/150 [01:36<00:20,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  83%|███████████████████▏   | 125/150 [01:37<00:19,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  84%|███████████████████▎   | 126/150 [01:38<00:18,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  85%|███████████████████▍   | 127/150 [01:39<00:17,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  85%|███████████████████▋   | 128/150 [01:39<00:16,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  86%|███████████████████▊   | 129/150 [01:40<00:16,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  87%|███████████████████▉   | 130/150 [01:41<00:15,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  87%|████████████████████   | 131/150 [01:42<00:14,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  88%|████████████████████▏  | 132/150 [01:42<00:13,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  89%|████████████████████▍  | 133/150 [01:43<00:13,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  89%|████████████████████▌  | 134/150 [01:44<00:12,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  90%|████████████████████▋  | 135/150 [01:45<00:11,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  91%|████████████████████▊  | 136/150 [01:46<00:10,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  91%|█████████████████████  | 137/150 [01:46<00:10,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  92%|█████████████████████▏ | 138/150 [01:47<00:09,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  93%|█████████████████████▎ | 139/150 [01:48<00:08,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  93%|█████████████████████▍ | 140/150 [01:49<00:07,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  94%|█████████████████████▌ | 141/150 [01:49<00:06,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  95%|█████████████████████▊ | 142/150 [01:50<00:06,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  95%|█████████████████████▉ | 143/150 [01:51<00:05,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  96%|██████████████████████ | 144/150 [01:52<00:04,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  97%|██████████████████████▏| 145/150 [01:52<00:03,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  97%|██████████████████████▍| 146/150 [01:53<00:03,  1.30it/s]\u001b[A\n",
      "Epoch 2 Training:  98%|██████████████████████▌| 147/150 [01:54<00:02,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  99%|██████████████████████▋| 148/150 [01:55<00:01,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training:  99%|██████████████████████▊| 149/150 [01:56<00:00,  1.29it/s]\u001b[A\n",
      "Epoch 2 Training: 100%|███████████████████████| 150/150 [01:56<00:00,  1.28it/s]\u001b[A\n",
      "\n",
      "Epoch 2 Validation:   0%|                                | 0/38 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2 Validation:   3%|▋                       | 1/38 [00:00<00:06,  5.51it/s]\u001b[A\n",
      "Epoch 2 Validation:   5%|█▎                      | 2/38 [00:00<00:06,  5.52it/s]\u001b[A\n",
      "Epoch 2 Validation:   8%|█▉                      | 3/38 [00:00<00:06,  5.50it/s]\u001b[A\n",
      "Epoch 2 Validation:  11%|██▌                     | 4/38 [00:00<00:06,  5.52it/s]\u001b[A\n",
      "Epoch 2 Validation:  13%|███▏                    | 5/38 [00:00<00:05,  5.52it/s]\u001b[A\n",
      "Epoch 2 Validation:  16%|███▊                    | 6/38 [00:01<00:05,  5.53it/s]\u001b[A\n",
      "Epoch 2 Validation:  18%|████▍                   | 7/38 [00:01<00:05,  5.52it/s]\u001b[A\n",
      "Epoch 2 Validation:  21%|█████                   | 8/38 [00:01<00:05,  5.52it/s]\u001b[A\n",
      "Epoch 2 Validation:  24%|█████▋                  | 9/38 [00:01<00:05,  5.53it/s]\u001b[A\n",
      "Epoch 2 Validation:  26%|██████                 | 10/38 [00:01<00:05,  5.53it/s]\u001b[A\n",
      "Epoch 2 Validation:  29%|██████▋                | 11/38 [00:01<00:04,  5.53it/s]\u001b[A\n",
      "Epoch 2 Validation:  32%|███████▎               | 12/38 [00:02<00:04,  5.52it/s]\u001b[A\n",
      "Epoch 2 Validation:  34%|███████▊               | 13/38 [00:02<00:04,  5.52it/s]\u001b[A\n",
      "Epoch 2 Validation:  37%|████████▍              | 14/38 [00:02<00:04,  5.53it/s]\u001b[A\n",
      "Epoch 2 Validation:  39%|█████████              | 15/38 [00:02<00:04,  5.53it/s]\u001b[A\n",
      "Epoch 2 Validation:  42%|█████████▋             | 16/38 [00:02<00:03,  5.54it/s]\u001b[A\n",
      "Epoch 2 Validation:  45%|██████████▎            | 17/38 [00:03<00:03,  5.54it/s]\u001b[A\n",
      "Epoch 2 Validation:  47%|██████████▉            | 18/38 [00:03<00:03,  5.54it/s]\u001b[A\n",
      "Epoch 2 Validation:  50%|███████████▌           | 19/38 [00:03<00:03,  5.52it/s]\u001b[A\n",
      "Epoch 2 Validation:  53%|████████████           | 20/38 [00:03<00:03,  5.52it/s]\u001b[A\n",
      "Epoch 2 Validation:  55%|████████████▋          | 21/38 [00:03<00:03,  5.52it/s]\u001b[A\n",
      "Epoch 2 Validation:  58%|█████████████▎         | 22/38 [00:03<00:02,  5.52it/s]\u001b[A\n",
      "Epoch 2 Validation:  61%|█████████████▉         | 23/38 [00:04<00:02,  5.52it/s]\u001b[A\n",
      "Epoch 2 Validation:  63%|██████████████▌        | 24/38 [00:04<00:02,  5.53it/s]\u001b[A\n",
      "Epoch 2 Validation:  66%|███████████████▏       | 25/38 [00:04<00:02,  5.52it/s]\u001b[A\n",
      "Epoch 2 Validation:  68%|███████████████▋       | 26/38 [00:04<00:02,  5.53it/s]\u001b[A\n",
      "Epoch 2 Validation:  71%|████████████████▎      | 27/38 [00:04<00:01,  5.54it/s]\u001b[A\n",
      "Epoch 2 Validation:  74%|████████████████▉      | 28/38 [00:05<00:01,  5.52it/s]\u001b[A\n",
      "Epoch 2 Validation:  76%|█████████████████▌     | 29/38 [00:05<00:01,  5.54it/s]\u001b[A\n",
      "Epoch 2 Validation:  79%|██████████████████▏    | 30/38 [00:05<00:01,  5.54it/s]\u001b[A\n",
      "Epoch 2 Validation:  82%|██████████████████▊    | 31/38 [00:05<00:01,  5.55it/s]\u001b[A\n",
      "Epoch 2 Validation:  84%|███████████████████▎   | 32/38 [00:05<00:01,  5.55it/s]\u001b[A\n",
      "Epoch 2 Validation:  87%|███████████████████▉   | 33/38 [00:05<00:00,  5.56it/s]\u001b[A\n",
      "Epoch 2 Validation:  89%|████████████████████▌  | 34/38 [00:06<00:00,  5.37it/s]\u001b[A\n",
      "Epoch 2 Validation:  92%|█████████████████████▏ | 35/38 [00:06<00:00,  5.41it/s]\u001b[A\n",
      "Epoch 2 Validation:  95%|█████████████████████▊ | 36/38 [00:06<00:00,  5.44it/s]\u001b[A\n",
      "Epoch 2 Validation:  97%|██████████████████████▍| 37/38 [00:06<00:00,  5.46it/s]\u001b[A\n",
      "Epoch 2 Validation: 100%|███████████████████████| 38/38 [00:06<00:00,  5.58it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training Loss: 0.3204, Training Macro F1: 0.8308, Validation Loss: 0.5703, Validation Macro F1: 0.6551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Training:   0%|                                 | 0/150 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3 Training:   1%|▏                        | 1/150 [00:00<01:54,  1.30it/s]\u001b[A\n",
      "Epoch 3 Training:   1%|▎                        | 2/150 [00:01<01:54,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:   2%|▌                        | 3/150 [00:02<01:53,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:   3%|▋                        | 4/150 [00:03<01:53,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:   3%|▊                        | 5/150 [00:03<01:52,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:   4%|█                        | 6/150 [00:04<01:52,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:   5%|█▏                       | 7/150 [00:05<01:53,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:   5%|█▎                       | 8/150 [00:06<01:52,  1.26it/s]\u001b[A\n",
      "Epoch 3 Training:   6%|█▌                       | 9/150 [00:07<01:53,  1.24it/s]\u001b[A\n",
      "Epoch 3 Training:   7%|█▌                      | 10/150 [00:07<01:52,  1.24it/s]\u001b[A\n",
      "Epoch 3 Training:   7%|█▊                      | 11/150 [00:08<01:51,  1.25it/s]\u001b[A\n",
      "Epoch 3 Training:   8%|█▉                      | 12/150 [00:09<01:49,  1.26it/s]\u001b[A\n",
      "Epoch 3 Training:   9%|██                      | 13/150 [00:10<01:49,  1.25it/s]\u001b[A\n",
      "Epoch 3 Training:   9%|██▏                     | 14/150 [00:11<01:48,  1.25it/s]\u001b[A\n",
      "Epoch 3 Training:  10%|██▍                     | 15/150 [00:11<01:47,  1.26it/s]\u001b[A\n",
      "Epoch 3 Training:  11%|██▌                     | 16/150 [00:12<01:45,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  11%|██▋                     | 17/150 [00:13<01:43,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  12%|██▉                     | 18/150 [00:14<01:42,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  13%|███                     | 19/150 [00:14<01:41,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  13%|███▏                    | 20/150 [00:15<01:40,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  14%|███▎                    | 21/150 [00:16<01:40,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  15%|███▌                    | 22/150 [00:17<01:39,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  15%|███▋                    | 23/150 [00:18<01:38,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  16%|███▊                    | 24/150 [00:18<01:38,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  17%|████                    | 25/150 [00:19<01:37,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  17%|████▏                   | 26/150 [00:20<01:36,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  18%|████▎                   | 27/150 [00:21<01:35,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  19%|████▍                   | 28/150 [00:21<01:34,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  19%|████▋                   | 29/150 [00:22<01:35,  1.26it/s]\u001b[A\n",
      "Epoch 3 Training:  20%|████▊                   | 30/150 [00:23<01:35,  1.25it/s]\u001b[A\n",
      "Epoch 3 Training:  21%|████▉                   | 31/150 [00:24<01:35,  1.24it/s]\u001b[A\n",
      "Epoch 3 Training:  21%|█████                   | 32/150 [00:25<01:36,  1.22it/s]\u001b[A\n",
      "Epoch 3 Training:  22%|█████▎                  | 33/150 [00:26<01:35,  1.23it/s]\u001b[A\n",
      "Epoch 3 Training:  23%|█████▍                  | 34/150 [00:26<01:34,  1.23it/s]\u001b[A\n",
      "Epoch 3 Training:  23%|█████▌                  | 35/150 [00:27<01:33,  1.23it/s]\u001b[A\n",
      "Epoch 3 Training:  24%|█████▊                  | 36/150 [00:28<01:33,  1.22it/s]\u001b[A\n",
      "Epoch 3 Training:  25%|█████▉                  | 37/150 [00:29<01:32,  1.23it/s]\u001b[A\n",
      "Epoch 3 Training:  25%|██████                  | 38/150 [00:30<01:31,  1.22it/s]\u001b[A\n",
      "Epoch 3 Training:  26%|██████▏                 | 39/150 [00:30<01:30,  1.23it/s]\u001b[A\n",
      "Epoch 3 Training:  27%|██████▍                 | 40/150 [00:31<01:28,  1.24it/s]\u001b[A\n",
      "Epoch 3 Training:  27%|██████▌                 | 41/150 [00:32<01:28,  1.23it/s]\u001b[A\n",
      "Epoch 3 Training:  28%|██████▋                 | 42/150 [00:33<01:27,  1.23it/s]\u001b[A\n",
      "Epoch 3 Training:  29%|██████▉                 | 43/150 [00:34<01:26,  1.24it/s]\u001b[A\n",
      "Epoch 3 Training:  29%|███████                 | 44/150 [00:34<01:24,  1.25it/s]\u001b[A\n",
      "Epoch 3 Training:  30%|███████▏                | 45/150 [00:35<01:23,  1.25it/s]\u001b[A\n",
      "Epoch 3 Training:  31%|███████▎                | 46/150 [00:36<01:22,  1.26it/s]\u001b[A\n",
      "Epoch 3 Training:  31%|███████▌                | 47/150 [00:37<01:21,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  32%|███████▋                | 48/150 [00:38<01:20,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  33%|███████▊                | 49/150 [00:38<01:19,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  33%|████████                | 50/150 [00:39<01:18,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  34%|████████▏               | 51/150 [00:40<01:17,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  35%|████████▎               | 52/150 [00:41<01:16,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  35%|████████▍               | 53/150 [00:42<01:15,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  36%|████████▋               | 54/150 [00:42<01:14,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  37%|████████▊               | 55/150 [00:43<01:14,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  37%|████████▉               | 56/150 [00:44<01:13,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  38%|█████████               | 57/150 [00:45<01:12,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  39%|█████████▎              | 58/150 [00:45<01:12,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  39%|█████████▍              | 59/150 [00:46<01:11,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  40%|█████████▌              | 60/150 [00:47<01:10,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  41%|█████████▊              | 61/150 [00:48<01:09,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  41%|█████████▉              | 62/150 [00:49<01:08,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  42%|██████████              | 63/150 [00:49<01:07,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  43%|██████████▏             | 64/150 [00:50<01:07,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  43%|██████████▍             | 65/150 [00:51<01:06,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  44%|██████████▌             | 66/150 [00:52<01:05,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  45%|██████████▋             | 67/150 [00:52<01:04,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  45%|██████████▉             | 68/150 [00:53<01:04,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  46%|███████████             | 69/150 [00:54<01:04,  1.26it/s]\u001b[A\n",
      "Epoch 3 Training:  47%|███████████▏            | 70/150 [00:55<01:03,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  47%|███████████▎            | 71/150 [00:56<01:02,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  48%|███████████▌            | 72/150 [00:56<01:00,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  49%|███████████▋            | 73/150 [00:57<01:00,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  49%|███████████▊            | 74/150 [00:58<00:59,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  50%|████████████            | 75/150 [00:59<00:58,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  51%|████████████▏           | 76/150 [01:00<00:57,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  51%|████████████▎           | 77/150 [01:00<00:56,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  52%|████████████▍           | 78/150 [01:01<00:56,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  53%|████████████▋           | 79/150 [01:02<00:55,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  53%|████████████▊           | 80/150 [01:03<00:54,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  54%|████████████▉           | 81/150 [01:03<00:53,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  55%|█████████████           | 82/150 [01:04<00:53,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  55%|█████████████▎          | 83/150 [01:05<00:52,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  56%|█████████████▍          | 84/150 [01:06<00:51,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  57%|█████████████▌          | 85/150 [01:07<00:51,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  57%|█████████████▊          | 86/150 [01:07<00:50,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  58%|█████████████▉          | 87/150 [01:08<00:49,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  59%|██████████████          | 88/150 [01:09<00:48,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  59%|██████████████▏         | 89/150 [01:10<00:47,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  60%|██████████████▍         | 90/150 [01:10<00:47,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  61%|██████████████▌         | 91/150 [01:11<00:46,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  61%|██████████████▋         | 92/150 [01:12<00:46,  1.24it/s]\u001b[A\n",
      "Epoch 3 Training:  62%|██████████████▉         | 93/150 [01:13<00:45,  1.24it/s]\u001b[A\n",
      "Epoch 3 Training:  63%|███████████████         | 94/150 [01:14<00:44,  1.25it/s]\u001b[A\n",
      "Epoch 3 Training:  63%|███████████████▏        | 95/150 [01:14<00:43,  1.26it/s]\u001b[A\n",
      "Epoch 3 Training:  64%|███████████████▎        | 96/150 [01:15<00:42,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  65%|███████████████▌        | 97/150 [01:16<00:41,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  65%|███████████████▋        | 98/150 [01:17<00:41,  1.26it/s]\u001b[A\n",
      "Epoch 3 Training:  66%|███████████████▊        | 99/150 [01:18<00:40,  1.25it/s]\u001b[A\n",
      "Epoch 3 Training:  67%|███████████████▎       | 100/150 [01:18<00:40,  1.25it/s]\u001b[A\n",
      "Epoch 3 Training:  67%|███████████████▍       | 101/150 [01:19<00:39,  1.24it/s]\u001b[A\n",
      "Epoch 3 Training:  68%|███████████████▋       | 102/150 [01:20<00:38,  1.24it/s]\u001b[A\n",
      "Epoch 3 Training:  69%|███████████████▊       | 103/150 [01:21<00:38,  1.23it/s]\u001b[A\n",
      "Epoch 3 Training:  69%|███████████████▉       | 104/150 [01:22<00:37,  1.24it/s]\u001b[A\n",
      "Epoch 3 Training:  70%|████████████████       | 105/150 [01:23<00:36,  1.24it/s]\u001b[A\n",
      "Epoch 3 Training:  71%|████████████████▎      | 106/150 [01:23<00:35,  1.24it/s]\u001b[A\n",
      "Epoch 3 Training:  71%|████████████████▍      | 107/150 [01:24<00:34,  1.24it/s]\u001b[A\n",
      "Epoch 3 Training:  72%|████████████████▌      | 108/150 [01:25<00:33,  1.25it/s]\u001b[A\n",
      "Epoch 3 Training:  73%|████████████████▋      | 109/150 [01:26<00:32,  1.26it/s]\u001b[A\n",
      "Epoch 3 Training:  73%|████████████████▊      | 110/150 [01:26<00:31,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  74%|█████████████████      | 111/150 [01:27<00:30,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  75%|█████████████████▏     | 112/150 [01:28<00:29,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  75%|█████████████████▎     | 113/150 [01:29<00:28,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  76%|█████████████████▍     | 114/150 [01:30<00:28,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  77%|█████████████████▋     | 115/150 [01:30<00:27,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  77%|█████████████████▊     | 116/150 [01:31<00:26,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  78%|█████████████████▉     | 117/150 [01:32<00:25,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  79%|██████████████████     | 118/150 [01:33<00:24,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  79%|██████████████████▏    | 119/150 [01:33<00:23,  1.30it/s]\u001b[A\n",
      "Epoch 3 Training:  80%|██████████████████▍    | 120/150 [01:34<00:23,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  81%|██████████████████▌    | 121/150 [01:35<00:22,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  81%|██████████████████▋    | 122/150 [01:36<00:21,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  82%|██████████████████▊    | 123/150 [01:37<00:20,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  83%|███████████████████    | 124/150 [01:37<00:20,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  83%|███████████████████▏   | 125/150 [01:38<00:19,  1.30it/s]\u001b[A\n",
      "Epoch 3 Training:  84%|███████████████████▎   | 126/150 [01:39<00:18,  1.30it/s]\u001b[A\n",
      "Epoch 3 Training:  85%|███████████████████▍   | 127/150 [01:40<00:17,  1.30it/s]\u001b[A\n",
      "Epoch 3 Training:  85%|███████████████████▋   | 128/150 [01:40<00:16,  1.30it/s]\u001b[A\n",
      "Epoch 3 Training:  86%|███████████████████▊   | 129/150 [01:41<00:16,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  87%|███████████████████▉   | 130/150 [01:42<00:15,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  87%|████████████████████   | 131/150 [01:43<00:14,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  88%|████████████████████▏  | 132/150 [01:44<00:14,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  89%|████████████████████▍  | 133/150 [01:44<00:13,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  89%|████████████████████▌  | 134/150 [01:45<00:12,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  90%|████████████████████▋  | 135/150 [01:46<00:11,  1.26it/s]\u001b[A\n",
      "Epoch 3 Training:  91%|████████████████████▊  | 136/150 [01:47<00:11,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  91%|█████████████████████  | 137/150 [01:47<00:10,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  92%|█████████████████████▏ | 138/150 [01:48<00:09,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  93%|█████████████████████▎ | 139/150 [01:49<00:08,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  93%|█████████████████████▍ | 140/150 [01:50<00:07,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  94%|█████████████████████▌ | 141/150 [01:51<00:07,  1.27it/s]\u001b[A\n",
      "Epoch 3 Training:  95%|█████████████████████▊ | 142/150 [01:51<00:06,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  95%|█████████████████████▉ | 143/150 [01:52<00:05,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  96%|██████████████████████ | 144/150 [01:53<00:04,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training:  97%|██████████████████████▏| 145/150 [01:54<00:03,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  97%|██████████████████████▍| 146/150 [01:54<00:03,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  98%|██████████████████████▌| 147/150 [01:55<00:02,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  99%|██████████████████████▋| 148/150 [01:56<00:01,  1.29it/s]\u001b[A\n",
      "Epoch 3 Training:  99%|██████████████████████▊| 149/150 [01:57<00:00,  1.28it/s]\u001b[A\n",
      "Epoch 3 Training: 100%|███████████████████████| 150/150 [01:58<00:00,  1.27it/s]\u001b[A\n",
      "\n",
      "Epoch 3 Validation:   0%|                                | 0/38 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3 Validation:   3%|▋                       | 1/38 [00:00<00:06,  5.51it/s]\u001b[A\n",
      "Epoch 3 Validation:   5%|█▎                      | 2/38 [00:00<00:06,  5.50it/s]\u001b[A\n",
      "Epoch 3 Validation:   8%|█▉                      | 3/38 [00:00<00:06,  5.53it/s]\u001b[A\n",
      "Epoch 3 Validation:  11%|██▌                     | 4/38 [00:00<00:06,  5.50it/s]\u001b[A\n",
      "Epoch 3 Validation:  13%|███▏                    | 5/38 [00:00<00:06,  5.50it/s]\u001b[A\n",
      "Epoch 3 Validation:  16%|███▊                    | 6/38 [00:01<00:05,  5.51it/s]\u001b[A\n",
      "Epoch 3 Validation:  18%|████▍                   | 7/38 [00:01<00:05,  5.51it/s]\u001b[A\n",
      "Epoch 3 Validation:  21%|█████                   | 8/38 [00:01<00:05,  5.51it/s]\u001b[A\n",
      "Epoch 3 Validation:  24%|█████▋                  | 9/38 [00:01<00:05,  5.52it/s]\u001b[A\n",
      "Epoch 3 Validation:  26%|██████                 | 10/38 [00:01<00:05,  5.48it/s]\u001b[A\n",
      "Epoch 3 Validation:  29%|██████▋                | 11/38 [00:01<00:04,  5.50it/s]\u001b[A\n",
      "Epoch 3 Validation:  32%|███████▎               | 12/38 [00:02<00:04,  5.48it/s]\u001b[A\n",
      "Epoch 3 Validation:  34%|███████▊               | 13/38 [00:02<00:04,  5.50it/s]\u001b[A\n",
      "Epoch 3 Validation:  37%|████████▍              | 14/38 [00:02<00:04,  5.49it/s]\u001b[A\n",
      "Epoch 3 Validation:  39%|█████████              | 15/38 [00:02<00:04,  5.51it/s]\u001b[A\n",
      "Epoch 3 Validation:  42%|█████████▋             | 16/38 [00:02<00:03,  5.51it/s]\u001b[A\n",
      "Epoch 3 Validation:  45%|██████████▎            | 17/38 [00:03<00:03,  5.51it/s]\u001b[A\n",
      "Epoch 3 Validation:  47%|██████████▉            | 18/38 [00:03<00:03,  5.50it/s]\u001b[A\n",
      "Epoch 3 Validation:  50%|███████████▌           | 19/38 [00:03<00:03,  5.50it/s]\u001b[A\n",
      "Epoch 3 Validation:  53%|████████████           | 20/38 [00:03<00:03,  5.50it/s]\u001b[A\n",
      "Epoch 3 Validation:  55%|████████████▋          | 21/38 [00:03<00:03,  5.50it/s]\u001b[A\n",
      "Epoch 3 Validation:  58%|█████████████▎         | 22/38 [00:03<00:02,  5.51it/s]\u001b[A\n",
      "Epoch 3 Validation:  61%|█████████████▉         | 23/38 [00:04<00:02,  5.52it/s]\u001b[A\n",
      "Epoch 3 Validation:  63%|██████████████▌        | 24/38 [00:04<00:02,  5.51it/s]\u001b[A\n",
      "Epoch 3 Validation:  66%|███████████████▏       | 25/38 [00:04<00:02,  5.51it/s]\u001b[A\n",
      "Epoch 3 Validation:  68%|███████████████▋       | 26/38 [00:04<00:02,  5.51it/s]\u001b[A\n",
      "Epoch 3 Validation:  71%|████████████████▎      | 27/38 [00:04<00:01,  5.52it/s]\u001b[A\n",
      "Epoch 3 Validation:  74%|████████████████▉      | 28/38 [00:05<00:01,  5.53it/s]\u001b[A\n",
      "Epoch 3 Validation:  76%|█████████████████▌     | 29/38 [00:05<00:01,  5.50it/s]\u001b[A\n",
      "Epoch 3 Validation:  79%|██████████████████▏    | 30/38 [00:05<00:01,  5.51it/s]\u001b[A\n",
      "Epoch 3 Validation:  82%|██████████████████▊    | 31/38 [00:05<00:01,  5.52it/s]\u001b[A\n",
      "Epoch 3 Validation:  84%|███████████████████▎   | 32/38 [00:05<00:01,  5.50it/s]\u001b[A\n",
      "Epoch 3 Validation:  87%|███████████████████▉   | 33/38 [00:05<00:00,  5.48it/s]\u001b[A\n",
      "Epoch 3 Validation:  89%|████████████████████▌  | 34/38 [00:06<00:00,  5.48it/s]\u001b[A\n",
      "Epoch 3 Validation:  92%|█████████████████████▏ | 35/38 [00:06<00:00,  5.46it/s]\u001b[A\n",
      "Epoch 3 Validation:  95%|█████████████████████▊ | 36/38 [00:06<00:00,  5.48it/s]\u001b[A\n",
      "Epoch 3 Validation:  97%|██████████████████████▍| 37/38 [00:06<00:00,  5.48it/s]\u001b[A\n",
      "Epoch 3 Validation: 100%|███████████████████████| 38/38 [00:06<00:00,  5.56it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training Loss: 0.6545, Training Macro F1: 0.5703, Validation Loss: 0.6911, Validation Macro F1: 0.3506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Training:   0%|                                 | 0/150 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4 Training:   1%|▏                        | 1/150 [00:00<01:54,  1.30it/s]\u001b[A\n",
      "Epoch 4 Training:   1%|▎                        | 2/150 [00:01<01:54,  1.29it/s]\u001b[A\n",
      "Epoch 4 Training:   2%|▌                        | 3/150 [00:02<01:54,  1.29it/s]\u001b[A\n",
      "Epoch 4 Training:   3%|▋                        | 4/150 [00:03<01:53,  1.29it/s]\u001b[A\n",
      "Epoch 4 Training:   3%|▊                        | 5/150 [00:03<01:53,  1.28it/s]\u001b[A\n",
      "Epoch 4 Training:   4%|█                        | 6/150 [00:04<01:53,  1.27it/s]\u001b[A\n",
      "Epoch 4 Training:   5%|█▏                       | 7/150 [00:05<01:52,  1.28it/s]\u001b[A\n",
      "Epoch 4 Training:   5%|█▎                       | 8/150 [00:06<01:50,  1.28it/s]\u001b[A\n",
      "Epoch 4 Training:   6%|█▌                       | 9/150 [00:07<01:49,  1.28it/s]\u001b[A\n",
      "Epoch 4 Training:   7%|█▌                      | 10/150 [00:07<01:48,  1.29it/s]\u001b[A\n",
      "Epoch 4 Training:   7%|█▊                      | 11/150 [00:08<01:48,  1.29it/s]\u001b[A\n",
      "Epoch 4 Training:   8%|█▉                      | 12/150 [00:09<01:47,  1.28it/s]\u001b[A\n",
      "Epoch 4 Training:   9%|██                      | 13/150 [00:10<01:47,  1.28it/s]\u001b[A\n",
      "Epoch 4 Training:   9%|██▏                     | 14/150 [00:10<01:45,  1.28it/s]\u001b[A\n",
      "Epoch 4 Training:  10%|██▍                     | 15/150 [00:11<01:44,  1.29it/s]\u001b[A\n",
      "Epoch 4 Training:  11%|██▌                     | 16/150 [00:12<01:43,  1.29it/s]\u001b[A\n",
      "Epoch 4 Training:  11%|██▋                     | 17/150 [00:13<01:43,  1.29it/s]\u001b[A\n",
      "Epoch 4 Training:  12%|██▉                     | 18/150 [00:14<01:42,  1.29it/s]\u001b[A\n",
      "Epoch 4 Training:  13%|███                     | 19/150 [00:14<01:41,  1.29it/s]\u001b[A\n",
      "Epoch 4 Training:  13%|███▏                    | 20/150 [00:15<01:40,  1.29it/s]\u001b[A\n",
      "Epoch 4 Training:  14%|███▎                    | 21/150 [00:16<01:39,  1.29it/s]\u001b[A\n",
      "Epoch 4 Training:  15%|███▌                    | 22/150 [00:17<01:38,  1.30it/s]\u001b[A\n",
      "Epoch 4 Training:  15%|███▋                    | 23/150 [00:17<01:37,  1.30it/s]\u001b[A\n",
      "Epoch 4 Training:  16%|███▊                    | 24/150 [00:18<01:37,  1.30it/s]\u001b[A\n",
      "Epoch 4 Training:  17%|████                    | 25/150 [00:19<01:36,  1.29it/s]\u001b[A\n",
      "Epoch 4 Training:  17%|████▏                   | 26/150 [00:20<01:35,  1.30it/s]\u001b[A\n",
      "Epoch 4 Training:  18%|████▎                   | 27/150 [00:20<01:34,  1.30it/s]\u001b[A\n",
      "Epoch 4 Training:  19%|████▍                   | 28/150 [00:21<01:34,  1.29it/s]\u001b[A\n",
      "Epoch 4 Training:  19%|████▋                   | 29/150 [00:22<01:34,  1.28it/s]\u001b[A\n",
      "Epoch 4 Training:  20%|████▊                   | 30/150 [00:23<01:33,  1.28it/s]\u001b[A\n",
      "Epoch 4 Training:  21%|████▉                   | 31/150 [00:24<01:32,  1.28it/s]\u001b[A\n",
      "Epoch 4 Training:  21%|█████                   | 32/150 [00:24<01:31,  1.29it/s]\u001b[A\n",
      "Epoch 4 Training:  22%|█████▎                  | 33/150 [00:25<01:30,  1.29it/s]\u001b[A\n",
      "Epoch 4 Training:  23%|█████▍                  | 34/150 [00:26<01:29,  1.29it/s]\u001b[A\n",
      "Epoch 4 Training:  23%|█████▌                  | 35/150 [00:27<01:29,  1.29it/s]\u001b[A\n",
      "Epoch 4 Training:  24%|█████▊                  | 36/150 [00:27<01:28,  1.29it/s]\u001b[A\n",
      "Epoch 4 Training:  25%|█████▉                  | 37/150 [00:28<01:27,  1.29it/s]\u001b[A\n",
      "Epoch 4 Training:  25%|██████                  | 38/150 [00:29<01:27,  1.29it/s]\u001b[A\n",
      "Epoch 4 Training:  26%|██████▏                 | 39/150 [00:30<01:26,  1.29it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[337], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 38\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     40\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1564\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1578\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:436\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    428\u001b[0m         hidden_states,\n\u001b[1;32m    429\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    434\u001b[0m         output_attentions,\n\u001b[1;32m    435\u001b[0m     )\n\u001b[0;32m--> 436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:387\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    386\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 387\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/torch/nn/functional.py:1266\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "# Hyper-parameters\n",
    "lr = 1e-4\n",
    "weight_decay = 0.001\n",
    "batch_size = 16\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# To device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "best_val_f1 = 0.0\n",
    "best_model_state_dict = None\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_f1_scores = []\n",
    "val_f1_scores = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1} Training'):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        train_preds.extend(predictions)\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_f1_scores.append(train_f1)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, desc=f'Epoch {epoch + 1} Validation'):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            val_preds.extend(predictions)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "    # Save the model with the best validation F1-score\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_model_state_dict = model.state_dict()\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f'Epoch {epoch + 1}, '\n",
    "          f'Training Loss: {avg_train_loss:.4f}, Training Macro F1: {train_f1:.4f}, '\n",
    "          f'Validation Loss: {avg_val_loss:.4f}, Validation Macro F1: {val_f1:.4f}')\n",
    "\n",
    "# Load the best model's weights\n",
    "if best_model_state_dict:\n",
    "    model.load_state_dict(best_model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae1571b-0bbe-4569-9076-45175248ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trained_weights_path = f'{results_dir}{model_id}/{task_name}/model_{lr}_{weight_decay}.pth'\n",
    "torch.save(model.state_dict(), trained_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df195d1c-c1f7-4aca-980b-c0d89443e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(1, len(train_losses) + 1))\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plotting BCE losses\n",
    "ax1 = plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot = Losses\n",
    "ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax1.plot(epochs, train_losses, label='Training', marker='o')\n",
    "ax1.plot(epochs, val_losses, label='Validation', marker='o')\n",
    "ax1.set_title('BCE losses')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Plotting average F1-scores\n",
    "ax2 = plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot = F1 Scores\n",
    "ax2.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax2.plot(epochs, train_f1_scores, label='Trainining', marker='o')\n",
    "ax2.plot(epochs, val_f1_scores, label='Validation', marker='o')\n",
    "ax2.set_title('Average F1-scores')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('average F1')\n",
    "ax2.legend()\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_dir}{model_id}/{task_name}/history_{lr}_{weight_decay}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7510a89b-58cf-47c9-8f68-cbe3636cda36",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce9e138-51b5-4b37-955a-0666c8118b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "tmp_model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "tmp_model.load_state_dict(torch.load(trained_weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4680d2df-a80e-4e49-98cc-5b9f09a1cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_preds = []\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc='Testing', disable=True):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        test_preds.extend(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf42da0-ced6-42d6-b6f2-133854ed3e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_data['label']\n",
    "\n",
    "report = classification_report(y_test, test_preds, digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc809859-0de0-4712-bf8e-f00985b93764",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_f1_result = max(val_f1_scores)\n",
    "print(best_val_f1_result)\n",
    "final_result = f'{best_val_f1_result}\\n {report}'\n",
    "with open(f'{results_dir}{model_id}/{task_name}/test_eval_{lr}_{weight_decay}.txt', 'w') as outf:\n",
    "    outf.write(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5a72db1f-79c2-4c82-86df-e06a889627eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = 'dbmdz/bert-base-italian-uncased'\n",
    "model_name = 'DBMDZ'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7421281b-cf34-4b7d-ad5c-ceaa09e9bd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 60\n",
    "X_train_e = tokenizer(X_train, truncation=True, padding=True, max_length = max_length)\n",
    "X_val_e = tokenizer(X_val, truncation=True, padding=True, max_length = max_length)\n",
    "X_test_e = tokenizer(X_test, truncation=True, padding=True, max_length = max_length)\n",
    "\n",
    "train_dataset = HateSpeechDataset(X_train_e, y_train)\n",
    "val_dataset = HateSpeechDataset(X_val_e, y_val)\n",
    "test_dataset = HateSpeechDataset(X_test_e, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e354c81-1cec-4519-98ed-be7952adcfc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4031b90e-8de6-4ae0-9c3c-8c070676384c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e8628271-0499-4896-9dbe-7f26fd7f244b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-italian-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8146bb82-5a50-4287-ac13-43cccb8acfad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 150/150 [02:35<00:00,  1.04s/it]\n",
      " 33%|█████████████▋                           | 150/450 [01:57<03:59,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t loss: 1.7624383548895517 \t val_loss: 0.697160973360664 \t f1_macro: 0.4330575082303377 \t val_f1_macro: 0.3355028381083775\n",
      "Validation Loss Decreased(inf--->0.697161) \t Saving The Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|███████████████████████████▎             | 300/450 [04:03<01:57,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 \t loss: 1.5545215155680974 \t val_loss: 1.0469242632389069 \t f1_macro: 0.4402722599458411 \t val_f1_macro: 0.3201413115323751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [06:07<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 \t loss: 1.3478586278359095 \t val_loss: 1.8788661721505617 \t f1_macro: 0.4353648342157207 \t val_f1_macro: 0.3201413115323751\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "lr=0.01\n",
    "wd=0.01\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "\n",
    "\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "min_val_f1 = -np.inf\n",
    "history = {\"loss\": [],\n",
    "           \"val_loss\": [],\n",
    "           \"f1_macro\": [],\n",
    "           \"val_f1_macro\": []\n",
    "          }\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_f1_macro = 0.0\n",
    "    val_loss = 0.0\n",
    "    val_f1_macro = 0.0\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_loader, 0):\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        # print statistics\n",
    "        train_loss += loss.item()\n",
    "        prediction = np.argmax(m(outputs[\"logits\"]).cpu().detach().numpy(), axis = 1).tolist()\n",
    "        train_f1_macro += f1_score(batch['labels'], prediction, average=\"macro\")\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    model.eval()     # Optional when not using Model Specific layer\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        val_loss += loss.item()\n",
    "        prediction = np.argmax(m(outputs[\"logits\"]).cpu().detach().numpy(), axis = 1).tolist()\n",
    "        val_f1_macro += f1_score(batch['labels'], prediction, average=\"macro\")\n",
    "    \n",
    "    history[\"loss\"].append(train_loss / len(train_loader))\n",
    "    history[\"val_loss\"].append(val_loss / len(val_loader))\n",
    "    history[\"f1_macro\"].append(train_f1_macro / len(train_loader))\n",
    "    history[\"val_f1_macro\"].append(val_f1_macro / len(val_loader))\n",
    "    \n",
    "    print(f'Epoch {epoch+1} \\t loss: {train_loss / len(train_loader)} \\t val_loss: {val_loss / len(val_loader)} \\t f1_macro: {train_f1_macro/len(train_loader)} \\t val_f1_macro: {val_f1_macro/len(val_loader)}')\n",
    "    if min_val_f1 < (val_f1_macro /len(val_loader)):\n",
    "        print(f'Validation F1 Increased({min_val_f1:.6f}--->{(val_f1_macro /len(val_loader)):.6f}) \\t Saving The Model')\n",
    "        min_val_f1 = (val_f1_macro / len(val_loader))\n",
    "        # Saving State Dict\n",
    "        torch.save(model.state_dict(), f'{results_dir}{model_name}/{task_name}test_eval_{str(lr)}_{str(wd)}.txt')\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b7e90ba4-d6f3-477e-b9fc-a6af5e442d96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    m = torch.nn.Softmax(dim=1)\n",
    "    tot_loss = []\n",
    "    prediction = []\n",
    "    true_value = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            prediction += np.argmax(m(outputs[\"logits\"]).cpu().numpy(), axis = 1).tolist()\n",
    "            true_value += labels.cpu().numpy().tolist()\n",
    "            tot_loss.append(outputs[0].cpu().numpy())\n",
    "    return np.mean(tot_loss), f1_score(true_value, prediction, average=\"macro\"), classification_report(true_value, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "198bec67-bcf1-4165-8bc3-0fbe3b0ce6af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nennomp/anaconda3/envs/dl/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/nennomp/anaconda3/envs/dl/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/nennomp/anaconda3/envs/dl/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "loss, f1, report = test_evaluate(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1fe126b0-7182-4cc0-9ba8-7aaf82bd450b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      1.00      0.49       323\n",
      "           1       0.00      0.00      0.00       677\n",
      "\n",
      "    accuracy                           0.32      1000\n",
      "   macro avg       0.16      0.50      0.24      1000\n",
      "weighted avg       0.10      0.32      0.16      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "e90b1ee1-0c77-4527-9ec0-d16cc55b303a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[344], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmo\u001b[49m\n\u001b[1;32m      2\u001b[0m test_result_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mtest_eval.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(test_result_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outf:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mo' is not defined"
     ]
    }
   ],
   "source": [
    "test_result_path = f'{results_dir}{model_name}/{task_name}test_eval_{str(lr)}_{str(wd)}.txt'\n",
    "with open(test_result_path, 'w') as outf:\n",
    "    outf.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "id": "81b72746-077a-4d88-b2d2-d47c55ddcd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:28:49,125] A new study created in memory with name: no-name-4f19483f-3ac0-4d74-8d82-7adc7b6da508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #0 [of 49]:\n",
      " {'learning_rate': 0.00022220111084116662, 'n_filters': 100, 'h_dim': 64, 'dropout': 0.4473105822161531, 'reg': 0.0017761299266422542, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 2s 34ms/step - loss: 0.8363 - avg_f1: 0.4927 - val_loss: 0.5992 - val_avg_f1: 0.4235\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.7007 - avg_f1: 0.5110 - val_loss: 0.5411 - val_avg_f1: 0.5988\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.5865 - avg_f1: 0.6177 - val_loss: 0.4996 - val_avg_f1: 0.7698\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.5224 - avg_f1: 0.6927 - val_loss: 0.4866 - val_avg_f1: 0.6500\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.5056 - avg_f1: 0.7141 - val_loss: 0.4693 - val_avg_f1: 0.7043\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.4845 - avg_f1: 0.7216 - val_loss: 0.4659 - val_avg_f1: 0.7136\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.4630 - avg_f1: 0.7477 - val_loss: 0.4487 - val_avg_f1: 0.7176\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.4516 - avg_f1: 0.7589 - val_loss: 0.4464 - val_avg_f1: 0.7957\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.4380 - avg_f1: 0.7663 - val_loss: 0.4303 - val_avg_f1: 0.7914\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.4155 - avg_f1: 0.7842 - val_loss: 0.4249 - val_avg_f1: 0.7485\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.4044 - avg_f1: 0.7973 - val_loss: 0.4235 - val_avg_f1: 0.7975\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.3999 - avg_f1: 0.7941 - val_loss: 0.4261 - val_avg_f1: 0.7560\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.3918 - avg_f1: 0.8040 - val_loss: 0.4519 - val_avg_f1: 0.6762\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.3784 - avg_f1: 0.8139 - val_loss: 0.4335 - val_avg_f1: 0.7156\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.3646 - avg_f1: 0.8201 - val_loss: 0.4168 - val_avg_f1: 0.7877\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.3504 - avg_f1: 0.8240 - val_loss: 0.4149 - val_avg_f1: 0.7856\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.3382 - avg_f1: 0.8439 - val_loss: 0.4167 - val_avg_f1: 0.7645\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.3308 - avg_f1: 0.8402 - val_loss: 0.4129 - val_avg_f1: 0.7983\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.3208 - avg_f1: 0.8551 - val_loss: 0.4267 - val_avg_f1: 0.7690\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.3011 - avg_f1: 0.8581 - val_loss: 0.4171 - val_avg_f1: 0.7986\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.2857 - avg_f1: 0.8723 - val_loss: 0.4220 - val_avg_f1: 0.7726\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.2783 - avg_f1: 0.8732 - val_loss: 0.4193 - val_avg_f1: 0.7859\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.2629 - avg_f1: 0.8888 - val_loss: 0.4525 - val_avg_f1: 0.7479\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.2530 - avg_f1: 0.9072 - val_loss: 0.4207 - val_avg_f1: 0.7983\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.2455 - avg_f1: 0.8995 - val_loss: 0.4274 - val_avg_f1: 0.7952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:29:13,747] Trial 0 finished with value: 0.798591673374176 and parameters: {'learning_rate': 0.00022220111084116662, 'n_filters': 100, 'h_dim': 64, 'dropout': 0.4473105822161531, 'reg': 0.0017761299266422542, 'batch_size': 64}. Best is trial 0 with value: 0.798591673374176.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #1 [of 49]:\n",
      " {'learning_rate': 0.01045452765138434, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.23438797565742436, 'reg': 0.00026212952970930447, 'batch_size': 32}\n",
      "Epoch 1/25\n",
      "75/75 [==============================] - 2s 18ms/step - loss: 0.8571 - avg_f1: 0.5759 - val_loss: 0.4656 - val_avg_f1: 0.7792\n",
      "Epoch 2/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.4897 - avg_f1: 0.6867 - val_loss: 0.4110 - val_avg_f1: 0.7026\n",
      "Epoch 3/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.4351 - avg_f1: 0.7481 - val_loss: 0.3992 - val_avg_f1: 0.7479\n",
      "Epoch 4/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.3867 - avg_f1: 0.7784 - val_loss: 0.4526 - val_avg_f1: 0.7756\n",
      "Epoch 5/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.3451 - avg_f1: 0.8190 - val_loss: 0.5431 - val_avg_f1: 0.7113\n",
      "Epoch 6/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.2639 - avg_f1: 0.8632 - val_loss: 0.4602 - val_avg_f1: 0.7692\n",
      "Epoch 7/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.2484 - avg_f1: 0.8822 - val_loss: 0.5274 - val_avg_f1: 0.7907\n",
      "Epoch 8/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.2216 - avg_f1: 0.8940 - val_loss: 0.6537 - val_avg_f1: 0.6738\n",
      "Epoch 9/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.1897 - avg_f1: 0.9074 - val_loss: 0.5240 - val_avg_f1: 0.7535\n",
      "Epoch 10/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.1580 - avg_f1: 0.9295 - val_loss: 0.6467 - val_avg_f1: 0.7555\n",
      "Epoch 11/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.1329 - avg_f1: 0.9389 - val_loss: 0.8955 - val_avg_f1: 0.7862\n",
      "Epoch 12/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.1651 - avg_f1: 0.9316 - val_loss: 0.7620 - val_avg_f1: 0.7737\n",
      "Epoch 13/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.1827 - avg_f1: 0.9243 - val_loss: 0.8457 - val_avg_f1: 0.7916\n",
      "Epoch 14/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.1192 - avg_f1: 0.9500 - val_loss: 1.0901 - val_avg_f1: 0.7892\n",
      "Epoch 15/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.1228 - avg_f1: 0.9551 - val_loss: 1.0919 - val_avg_f1: 0.7614\n",
      "Epoch 16/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.0818 - avg_f1: 0.9663 - val_loss: 1.2306 - val_avg_f1: 0.7667\n",
      "Epoch 17/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.1556 - avg_f1: 0.9466 - val_loss: 1.1260 - val_avg_f1: 0.7240\n",
      "Epoch 18/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.1930 - avg_f1: 0.9302 - val_loss: 0.8906 - val_avg_f1: 0.7935\n",
      "Epoch 19/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.1128 - avg_f1: 0.9583 - val_loss: 1.2046 - val_avg_f1: 0.7594\n",
      "Epoch 20/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.1166 - avg_f1: 0.9675 - val_loss: 1.3909 - val_avg_f1: 0.7885\n",
      "Epoch 21/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.0703 - avg_f1: 0.9723 - val_loss: 1.0569 - val_avg_f1: 0.7892\n",
      "Epoch 22/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.0940 - avg_f1: 0.9672 - val_loss: 1.5088 - val_avg_f1: 0.7912\n",
      "Epoch 23/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.2023 - avg_f1: 0.9530 - val_loss: 2.0814 - val_avg_f1: 0.7768\n",
      "Epoch 24/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.5076 - avg_f1: 0.9175 - val_loss: 1.5746 - val_avg_f1: 0.7544\n",
      "Epoch 25/25\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 0.2374 - avg_f1: 0.9426 - val_loss: 2.4228 - val_avg_f1: 0.7680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:29:41,327] Trial 1 finished with value: 0.7934536933898926 and parameters: {'learning_rate': 0.01045452765138434, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.23438797565742436, 'reg': 0.00026212952970930447, 'batch_size': 32}. Best is trial 0 with value: 0.798591673374176.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #2 [of 49]:\n",
      " {'learning_rate': 0.08158233472932548, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.10075565502887657, 'reg': 0.06363151193512945, 'batch_size': 128}\n",
      "Epoch 1/25\n",
      "19/19 [==============================] - 2s 35ms/step - loss: 14.7578 - avg_f1: 0.3895 - val_loss: 1.2896 - val_avg_f1: 0.5855\n",
      "Epoch 2/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 2.2784 - avg_f1: 0.5073 - val_loss: 0.9261 - val_avg_f1: 0.6700\n",
      "Epoch 3/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 1.1465 - avg_f1: 0.6248 - val_loss: 0.8242 - val_avg_f1: 0.4900\n",
      "Epoch 4/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.9803 - avg_f1: 0.5957 - val_loss: 0.5962 - val_avg_f1: 0.7187\n",
      "Epoch 5/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.6936 - avg_f1: 0.6655 - val_loss: 0.5454 - val_avg_f1: 0.7715\n",
      "Epoch 6/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.6802 - avg_f1: 0.6649 - val_loss: 0.5808 - val_avg_f1: 0.7521\n",
      "Epoch 7/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5491 - avg_f1: 0.7241 - val_loss: 0.5450 - val_avg_f1: 0.7420\n",
      "Epoch 8/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4612 - avg_f1: 0.7714 - val_loss: 0.4626 - val_avg_f1: 0.7653\n",
      "Epoch 9/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4962 - avg_f1: 0.7451 - val_loss: 0.4516 - val_avg_f1: 0.7794\n",
      "Epoch 10/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4566 - avg_f1: 0.7661 - val_loss: 0.5020 - val_avg_f1: 0.6911\n",
      "Epoch 11/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4113 - avg_f1: 0.7848 - val_loss: 0.4713 - val_avg_f1: 0.7718\n",
      "Epoch 12/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3921 - avg_f1: 0.7948 - val_loss: 0.4861 - val_avg_f1: 0.7152\n",
      "Epoch 13/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3669 - avg_f1: 0.8243 - val_loss: 0.5230 - val_avg_f1: 0.7237\n",
      "Epoch 14/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3650 - avg_f1: 0.8216 - val_loss: 0.4914 - val_avg_f1: 0.7586\n",
      "Epoch 15/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3113 - avg_f1: 0.8580 - val_loss: 0.5464 - val_avg_f1: 0.7703\n",
      "Epoch 16/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2895 - avg_f1: 0.8750 - val_loss: 0.5023 - val_avg_f1: 0.7687\n",
      "Epoch 17/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2933 - avg_f1: 0.8737 - val_loss: 0.5967 - val_avg_f1: 0.7533\n",
      "Epoch 18/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2867 - avg_f1: 0.8771 - val_loss: 0.9046 - val_avg_f1: 0.7209\n",
      "Epoch 19/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2557 - avg_f1: 0.8837 - val_loss: 0.7682 - val_avg_f1: 0.7113\n",
      "Epoch 20/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2754 - avg_f1: 0.8906 - val_loss: 0.7554 - val_avg_f1: 0.7770\n",
      "Epoch 21/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2631 - avg_f1: 0.8996 - val_loss: 0.6179 - val_avg_f1: 0.7691\n",
      "Epoch 22/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2237 - avg_f1: 0.9243 - val_loss: 0.6773 - val_avg_f1: 0.7637\n",
      "Epoch 23/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3339 - avg_f1: 0.8682 - val_loss: 0.6290 - val_avg_f1: 0.7533\n",
      "Epoch 24/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2688 - avg_f1: 0.8949 - val_loss: 0.6951 - val_avg_f1: 0.7491\n",
      "Epoch 25/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.1871 - avg_f1: 0.9369 - val_loss: 0.8017 - val_avg_f1: 0.7435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:29:50,842] Trial 2 finished with value: 0.779352068901062 and parameters: {'learning_rate': 0.08158233472932548, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.10075565502887657, 'reg': 0.06363151193512945, 'batch_size': 128}. Best is trial 0 with value: 0.798591673374176.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #3 [of 49]:\n",
      " {'learning_rate': 0.07576724444334809, 'n_filters': 50, 'h_dim': 16, 'dropout': 0.00837291556719938, 'reg': 2.988611976677508e-05, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 2s 23ms/step - loss: 2.9167 - avg_f1: 0.4625 - val_loss: 0.4447 - val_avg_f1: 0.7582\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.6324 - avg_f1: 0.6171 - val_loss: 0.4481 - val_avg_f1: 0.7562\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.4846 - avg_f1: 0.7217 - val_loss: 0.4387 - val_avg_f1: 0.7681\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.4799 - avg_f1: 0.7241 - val_loss: 0.5352 - val_avg_f1: 0.7151\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.4273 - avg_f1: 0.7552 - val_loss: 0.3936 - val_avg_f1: 0.8038\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.3913 - avg_f1: 0.7869 - val_loss: 0.4229 - val_avg_f1: 0.7665\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.3937 - avg_f1: 0.7855 - val_loss: 0.4849 - val_avg_f1: 0.7687\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.2717 - avg_f1: 0.8556 - val_loss: 0.5460 - val_avg_f1: 0.7722\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.2675 - avg_f1: 0.8682 - val_loss: 0.5592 - val_avg_f1: 0.7659\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.2138 - avg_f1: 0.8882 - val_loss: 0.7691 - val_avg_f1: 0.7474\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.2078 - avg_f1: 0.9085 - val_loss: 0.7217 - val_avg_f1: 0.7796\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.1601 - avg_f1: 0.9321 - val_loss: 0.8199 - val_avg_f1: 0.6899\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.1189 - avg_f1: 0.9428 - val_loss: 1.3720 - val_avg_f1: 0.7418\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0933 - avg_f1: 0.9590 - val_loss: 1.1960 - val_avg_f1: 0.7942\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.1358 - avg_f1: 0.9538 - val_loss: 1.1773 - val_avg_f1: 0.7543\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.1037 - avg_f1: 0.9522 - val_loss: 1.1809 - val_avg_f1: 0.7373\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0742 - avg_f1: 0.9689 - val_loss: 1.7406 - val_avg_f1: 0.7748\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0410 - avg_f1: 0.9811 - val_loss: 1.7116 - val_avg_f1: 0.7504\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0325 - avg_f1: 0.9878 - val_loss: 2.2280 - val_avg_f1: 0.7706\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0616 - avg_f1: 0.9786 - val_loss: 1.7895 - val_avg_f1: 0.7655\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0439 - avg_f1: 0.9848 - val_loss: 2.1408 - val_avg_f1: 0.7653\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0424 - avg_f1: 0.9883 - val_loss: 1.8198 - val_avg_f1: 0.7397\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.1830 - avg_f1: 0.9417 - val_loss: 3.6561 - val_avg_f1: 0.7478\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.2738 - avg_f1: 0.9219 - val_loss: 2.4632 - val_avg_f1: 0.7519\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 1.3179 - avg_f1: 0.8415 - val_loss: 4.2849 - val_avg_f1: 0.7357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:30:05,888] Trial 3 finished with value: 0.803813636302948 and parameters: {'learning_rate': 0.07576724444334809, 'n_filters': 50, 'h_dim': 16, 'dropout': 0.00837291556719938, 'reg': 2.988611976677508e-05, 'batch_size': 64}. Best is trial 3 with value: 0.803813636302948.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #4 [of 49]:\n",
      " {'learning_rate': 0.009466003224234973, 'n_filters': 25, 'h_dim': 32, 'dropout': 0.23172201276377197, 'reg': 0.001066130708521262, 'batch_size': 16}\n",
      "Epoch 1/25\n",
      "150/150 [==============================] - 3s 6ms/step - loss: 0.6928 - avg_f1: 0.6026 - val_loss: 0.4384 - val_avg_f1: 0.6858\n",
      "Epoch 2/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.4868 - avg_f1: 0.6906 - val_loss: 0.4135 - val_avg_f1: 0.7369\n",
      "Epoch 3/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.4249 - avg_f1: 0.7332 - val_loss: 0.4038 - val_avg_f1: 0.7821\n",
      "Epoch 4/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.4019 - avg_f1: 0.7666 - val_loss: 0.4160 - val_avg_f1: 0.7705\n",
      "Epoch 5/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.3792 - avg_f1: 0.7736 - val_loss: 0.4148 - val_avg_f1: 0.7633\n",
      "Epoch 6/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.3223 - avg_f1: 0.8297 - val_loss: 0.4969 - val_avg_f1: 0.7747\n",
      "Epoch 7/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.2829 - avg_f1: 0.8414 - val_loss: 0.4460 - val_avg_f1: 0.7720\n",
      "Epoch 8/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.2468 - avg_f1: 0.8723 - val_loss: 0.5195 - val_avg_f1: 0.7871\n",
      "Epoch 9/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.2095 - avg_f1: 0.8977 - val_loss: 0.5353 - val_avg_f1: 0.7851\n",
      "Epoch 10/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.2065 - avg_f1: 0.8917 - val_loss: 0.5179 - val_avg_f1: 0.7442\n",
      "Epoch 11/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.1576 - avg_f1: 0.9226 - val_loss: 0.6982 - val_avg_f1: 0.7736\n",
      "Epoch 12/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.1564 - avg_f1: 0.9245 - val_loss: 0.6612 - val_avg_f1: 0.7805\n",
      "Epoch 13/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.1324 - avg_f1: 0.9390 - val_loss: 0.8698 - val_avg_f1: 0.7437\n",
      "Epoch 14/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.1342 - avg_f1: 0.9324 - val_loss: 0.7301 - val_avg_f1: 0.7784\n",
      "Epoch 15/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.1215 - avg_f1: 0.9478 - val_loss: 0.5901 - val_avg_f1: 0.7901\n",
      "Epoch 16/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.1353 - avg_f1: 0.9362 - val_loss: 0.9353 - val_avg_f1: 0.7736\n",
      "Epoch 17/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0851 - avg_f1: 0.9620 - val_loss: 1.1790 - val_avg_f1: 0.7507\n",
      "Epoch 18/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0810 - avg_f1: 0.9713 - val_loss: 1.0206 - val_avg_f1: 0.7670\n",
      "Epoch 19/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0867 - avg_f1: 0.9593 - val_loss: 0.8179 - val_avg_f1: 0.7809\n",
      "Epoch 20/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0994 - avg_f1: 0.9570 - val_loss: 0.9684 - val_avg_f1: 0.7682\n",
      "Epoch 21/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0880 - avg_f1: 0.9637 - val_loss: 1.3259 - val_avg_f1: 0.7590\n",
      "Epoch 22/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0910 - avg_f1: 0.9643 - val_loss: 1.0950 - val_avg_f1: 0.7684\n",
      "Epoch 23/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0574 - avg_f1: 0.9748 - val_loss: 1.1290 - val_avg_f1: 0.7565\n",
      "Epoch 24/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0681 - avg_f1: 0.9668 - val_loss: 1.1846 - val_avg_f1: 0.7583\n",
      "Epoch 25/25\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0753 - avg_f1: 0.9657 - val_loss: 1.0439 - val_avg_f1: 0.7506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:30:23,860] Trial 4 finished with value: 0.7901469469070435 and parameters: {'learning_rate': 0.009466003224234973, 'n_filters': 25, 'h_dim': 32, 'dropout': 0.23172201276377197, 'reg': 0.001066130708521262, 'batch_size': 16}. Best is trial 3 with value: 0.803813636302948.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #5 [of 49]:\n",
      " {'learning_rate': 0.00010905567903732225, 'n_filters': 50, 'h_dim': 64, 'dropout': 0.49904470708099463, 'reg': 0.00222455560000609, 'batch_size': 16}\n",
      "Epoch 1/25\n",
      "150/150 [==============================] - 2s 7ms/step - loss: 1.5522 - avg_f1: 0.4785 - val_loss: 0.9531 - val_avg_f1: 0.4011\n",
      "Epoch 2/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 1.1781 - avg_f1: 0.4741 - val_loss: 0.7061 - val_avg_f1: 0.4011\n",
      "Epoch 3/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.8894 - avg_f1: 0.5224 - val_loss: 0.7390 - val_avg_f1: 0.4011\n",
      "Epoch 4/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.7055 - avg_f1: 0.5819 - val_loss: 0.6589 - val_avg_f1: 0.4011\n",
      "Epoch 5/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.6227 - avg_f1: 0.6396 - val_loss: 0.5983 - val_avg_f1: 0.4011\n",
      "Epoch 6/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.5708 - avg_f1: 0.6472 - val_loss: 0.5354 - val_avg_f1: 0.5342\n",
      "Epoch 7/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.5582 - avg_f1: 0.6580 - val_loss: 0.5335 - val_avg_f1: 0.5278\n",
      "Epoch 8/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.5327 - avg_f1: 0.6700 - val_loss: 0.5187 - val_avg_f1: 0.5662\n",
      "Epoch 9/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.5144 - avg_f1: 0.6969 - val_loss: 0.5033 - val_avg_f1: 0.6280\n",
      "Epoch 10/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.5115 - avg_f1: 0.6934 - val_loss: 0.4810 - val_avg_f1: 0.6870\n",
      "Epoch 11/25\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.4921 - avg_f1: 0.7267 - val_loss: 0.4825 - val_avg_f1: 0.6612\n",
      "Epoch 12/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4930 - avg_f1: 0.7239 - val_loss: 0.4853 - val_avg_f1: 0.6532\n",
      "Epoch 13/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4781 - avg_f1: 0.7160 - val_loss: 0.4947 - val_avg_f1: 0.6098\n",
      "Epoch 14/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4782 - avg_f1: 0.7304 - val_loss: 0.4707 - val_avg_f1: 0.6615\n",
      "Epoch 15/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4642 - avg_f1: 0.7271 - val_loss: 0.4654 - val_avg_f1: 0.6674\n",
      "Epoch 16/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4572 - avg_f1: 0.7494 - val_loss: 0.4718 - val_avg_f1: 0.6428\n",
      "Epoch 17/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4590 - avg_f1: 0.7425 - val_loss: 0.4515 - val_avg_f1: 0.6983\n",
      "Epoch 18/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4456 - avg_f1: 0.7468 - val_loss: 0.4471 - val_avg_f1: 0.7063\n",
      "Epoch 19/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4399 - avg_f1: 0.7527 - val_loss: 0.4443 - val_avg_f1: 0.7165\n",
      "Epoch 20/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4315 - avg_f1: 0.7772 - val_loss: 0.4390 - val_avg_f1: 0.7451\n",
      "Epoch 21/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4368 - avg_f1: 0.7667 - val_loss: 0.4440 - val_avg_f1: 0.7062\n",
      "Epoch 22/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4264 - avg_f1: 0.7730 - val_loss: 0.4522 - val_avg_f1: 0.6871\n",
      "Epoch 23/25\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.4288 - avg_f1: 0.7691 - val_loss: 0.4389 - val_avg_f1: 0.7316\n",
      "Epoch 24/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4157 - avg_f1: 0.7755 - val_loss: 0.4540 - val_avg_f1: 0.6712\n",
      "Epoch 25/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4040 - avg_f1: 0.7802 - val_loss: 0.4255 - val_avg_f1: 0.7836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:30:45,533] Trial 5 finished with value: 0.7836350798606873 and parameters: {'learning_rate': 0.00010905567903732225, 'n_filters': 50, 'h_dim': 64, 'dropout': 0.49904470708099463, 'reg': 0.00222455560000609, 'batch_size': 16}. Best is trial 3 with value: 0.803813636302948.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #6 [of 49]:\n",
      " {'learning_rate': 5.5703898109439335e-05, 'n_filters': 50, 'h_dim': 16, 'dropout': 0.417602014319278, 'reg': 2.6076654305993296e-05, 'batch_size': 16}\n",
      "Epoch 1/25\n",
      "150/150 [==============================] - 2s 7ms/step - loss: 1.0135 - avg_f1: 0.4780 - val_loss: 0.6672 - val_avg_f1: 0.4059\n",
      "Epoch 2/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.7415 - avg_f1: 0.4952 - val_loss: 0.6479 - val_avg_f1: 0.4219\n",
      "Epoch 3/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.6781 - avg_f1: 0.4751 - val_loss: 0.6270 - val_avg_f1: 0.4521\n",
      "Epoch 4/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.6270 - avg_f1: 0.4844 - val_loss: 0.6164 - val_avg_f1: 0.6070\n",
      "Epoch 5/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.6067 - avg_f1: 0.5058 - val_loss: 0.5940 - val_avg_f1: 0.6539\n",
      "Epoch 6/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.5778 - avg_f1: 0.5790 - val_loss: 0.5658 - val_avg_f1: 0.6182\n",
      "Epoch 7/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.5587 - avg_f1: 0.5944 - val_loss: 0.5545 - val_avg_f1: 0.7383\n",
      "Epoch 8/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.5462 - avg_f1: 0.6356 - val_loss: 0.5216 - val_avg_f1: 0.7331\n",
      "Epoch 9/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.5322 - avg_f1: 0.6235 - val_loss: 0.5065 - val_avg_f1: 0.7776\n",
      "Epoch 10/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.5195 - avg_f1: 0.6758 - val_loss: 0.4645 - val_avg_f1: 0.7615\n",
      "Epoch 11/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.5031 - avg_f1: 0.6732 - val_loss: 0.4491 - val_avg_f1: 0.7860\n",
      "Epoch 12/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4956 - avg_f1: 0.6745 - val_loss: 0.4517 - val_avg_f1: 0.7840\n",
      "Epoch 13/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4818 - avg_f1: 0.6963 - val_loss: 0.4338 - val_avg_f1: 0.7544\n",
      "Epoch 14/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4767 - avg_f1: 0.6984 - val_loss: 0.4279 - val_avg_f1: 0.7950\n",
      "Epoch 15/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4741 - avg_f1: 0.7016 - val_loss: 0.4247 - val_avg_f1: 0.7234\n",
      "Epoch 16/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4625 - avg_f1: 0.7301 - val_loss: 0.4171 - val_avg_f1: 0.7834\n",
      "Epoch 17/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4628 - avg_f1: 0.7264 - val_loss: 0.4176 - val_avg_f1: 0.7831\n",
      "Epoch 18/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4596 - avg_f1: 0.7238 - val_loss: 0.4187 - val_avg_f1: 0.7977\n",
      "Epoch 19/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4530 - avg_f1: 0.7337 - val_loss: 0.4107 - val_avg_f1: 0.7638\n",
      "Epoch 20/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4460 - avg_f1: 0.7378 - val_loss: 0.4111 - val_avg_f1: 0.7491\n",
      "Epoch 21/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4386 - avg_f1: 0.7349 - val_loss: 0.4059 - val_avg_f1: 0.7539\n",
      "Epoch 22/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4330 - avg_f1: 0.7357 - val_loss: 0.4063 - val_avg_f1: 0.7499\n",
      "Epoch 23/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4317 - avg_f1: 0.7428 - val_loss: 0.4053 - val_avg_f1: 0.7267\n",
      "Epoch 24/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4251 - avg_f1: 0.7502 - val_loss: 0.3989 - val_avg_f1: 0.7597\n",
      "Epoch 25/25\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4189 - avg_f1: 0.7587 - val_loss: 0.4008 - val_avg_f1: 0.7629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:31:06,349] Trial 6 finished with value: 0.7977449297904968 and parameters: {'learning_rate': 5.5703898109439335e-05, 'n_filters': 50, 'h_dim': 16, 'dropout': 0.417602014319278, 'reg': 2.6076654305993296e-05, 'batch_size': 16}. Best is trial 3 with value: 0.803813636302948.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #7 [of 49]:\n",
      " {'learning_rate': 1.9185196597736192e-05, 'n_filters': 100, 'h_dim': 32, 'dropout': 0.16268008599777567, 'reg': 0.00014916756528490278, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 3s 38ms/step - loss: 1.7115 - avg_f1: 0.3594 - val_loss: 1.3128 - val_avg_f1: 0.2442\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 1.2899 - avg_f1: 0.3900 - val_loss: 0.9612 - val_avg_f1: 0.2747\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.9771 - avg_f1: 0.4668 - val_loss: 0.7392 - val_avg_f1: 0.4672\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.8379 - avg_f1: 0.5016 - val_loss: 0.6308 - val_avg_f1: 0.6614\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7539 - avg_f1: 0.5354 - val_loss: 0.5980 - val_avg_f1: 0.5305\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.7041 - avg_f1: 0.5446 - val_loss: 0.5936 - val_avg_f1: 0.4275\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6791 - avg_f1: 0.5324 - val_loss: 0.5920 - val_avg_f1: 0.4080\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6642 - avg_f1: 0.5431 - val_loss: 0.5894 - val_avg_f1: 0.4094\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.6491 - avg_f1: 0.5465 - val_loss: 0.5852 - val_avg_f1: 0.4094\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6520 - avg_f1: 0.5412 - val_loss: 0.5822 - val_avg_f1: 0.4094\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6336 - avg_f1: 0.5371 - val_loss: 0.5754 - val_avg_f1: 0.4094\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6402 - avg_f1: 0.5456 - val_loss: 0.5708 - val_avg_f1: 0.4094\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.6303 - avg_f1: 0.5365 - val_loss: 0.5660 - val_avg_f1: 0.4094\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6246 - avg_f1: 0.5456 - val_loss: 0.5610 - val_avg_f1: 0.4089\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6151 - avg_f1: 0.5482 - val_loss: 0.5564 - val_avg_f1: 0.4123\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6054 - avg_f1: 0.5532 - val_loss: 0.5528 - val_avg_f1: 0.4123\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.6097 - avg_f1: 0.5542 - val_loss: 0.5446 - val_avg_f1: 0.4495\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.5992 - avg_f1: 0.5561 - val_loss: 0.5388 - val_avg_f1: 0.4511\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.5766 - avg_f1: 0.5980 - val_loss: 0.5365 - val_avg_f1: 0.4543\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.5896 - avg_f1: 0.5527 - val_loss: 0.5290 - val_avg_f1: 0.4744\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.5839 - avg_f1: 0.5797 - val_loss: 0.5250 - val_avg_f1: 0.4734\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.5770 - avg_f1: 0.5787 - val_loss: 0.5202 - val_avg_f1: 0.4962\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.5682 - avg_f1: 0.5698 - val_loss: 0.5161 - val_avg_f1: 0.5122\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.5646 - avg_f1: 0.5812 - val_loss: 0.5081 - val_avg_f1: 0.5527\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.5571 - avg_f1: 0.6026 - val_loss: 0.5060 - val_avg_f1: 0.5476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:31:30,517] Trial 7 finished with value: 0.6614468693733215 and parameters: {'learning_rate': 1.9185196597736192e-05, 'n_filters': 100, 'h_dim': 32, 'dropout': 0.16268008599777567, 'reg': 0.00014916756528490278, 'batch_size': 64}. Best is trial 3 with value: 0.803813636302948.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #8 [of 49]:\n",
      " {'learning_rate': 0.000175489216642772, 'n_filters': 50, 'h_dim': 64, 'dropout': 0.3759453841504423, 'reg': 0.0007232169481753781, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 2s 24ms/step - loss: 0.9792 - avg_f1: 0.5140 - val_loss: 0.6201 - val_avg_f1: 0.4094\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.8552 - avg_f1: 0.5149 - val_loss: 0.5702 - val_avg_f1: 0.4094\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.7396 - avg_f1: 0.5259 - val_loss: 0.5027 - val_avg_f1: 0.5759\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.6159 - avg_f1: 0.6133 - val_loss: 0.4586 - val_avg_f1: 0.7353\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.5489 - avg_f1: 0.6828 - val_loss: 0.4620 - val_avg_f1: 0.6985\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.5198 - avg_f1: 0.6961 - val_loss: 0.4670 - val_avg_f1: 0.6927\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.5023 - avg_f1: 0.7008 - val_loss: 0.4588 - val_avg_f1: 0.7073\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.4783 - avg_f1: 0.7220 - val_loss: 0.4480 - val_avg_f1: 0.7428\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.4819 - avg_f1: 0.7245 - val_loss: 0.4442 - val_avg_f1: 0.7218\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.4602 - avg_f1: 0.7347 - val_loss: 0.4361 - val_avg_f1: 0.7423\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.4511 - avg_f1: 0.7552 - val_loss: 0.4347 - val_avg_f1: 0.7296\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.4476 - avg_f1: 0.7525 - val_loss: 0.4294 - val_avg_f1: 0.7952\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.4335 - avg_f1: 0.7622 - val_loss: 0.4181 - val_avg_f1: 0.7775\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.4321 - avg_f1: 0.7665 - val_loss: 0.4221 - val_avg_f1: 0.7641\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.4285 - avg_f1: 0.7589 - val_loss: 0.4190 - val_avg_f1: 0.7684\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.4147 - avg_f1: 0.7756 - val_loss: 0.4162 - val_avg_f1: 0.7812\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.4101 - avg_f1: 0.7737 - val_loss: 0.4234 - val_avg_f1: 0.7282\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.3999 - avg_f1: 0.7943 - val_loss: 0.4110 - val_avg_f1: 0.7745\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.3937 - avg_f1: 0.7882 - val_loss: 0.4150 - val_avg_f1: 0.7616\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.3990 - avg_f1: 0.7814 - val_loss: 0.4097 - val_avg_f1: 0.7993\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.3702 - avg_f1: 0.8046 - val_loss: 0.4112 - val_avg_f1: 0.7658\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.3732 - avg_f1: 0.8121 - val_loss: 0.4117 - val_avg_f1: 0.7603\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.3667 - avg_f1: 0.8085 - val_loss: 0.4221 - val_avg_f1: 0.7245\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.3569 - avg_f1: 0.8114 - val_loss: 0.4128 - val_avg_f1: 0.7668\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.3575 - avg_f1: 0.8154 - val_loss: 0.4178 - val_avg_f1: 0.7599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:31:46,145] Trial 8 finished with value: 0.7992768287658691 and parameters: {'learning_rate': 0.000175489216642772, 'n_filters': 50, 'h_dim': 64, 'dropout': 0.3759453841504423, 'reg': 0.0007232169481753781, 'batch_size': 64}. Best is trial 3 with value: 0.803813636302948.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #9 [of 49]:\n",
      " {'learning_rate': 2.7295839458329498e-05, 'n_filters': 25, 'h_dim': 16, 'dropout': 0.055771344558668334, 'reg': 0.02463141816898002, 'batch_size': 128}\n",
      "Epoch 1/25\n",
      "19/19 [==============================] - 2s 34ms/step - loss: 2.4451 - avg_f1: 0.4221 - val_loss: 2.4832 - val_avg_f1: 0.4036\n",
      "Epoch 2/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 2.3518 - avg_f1: 0.4204 - val_loss: 2.3778 - val_avg_f1: 0.4036\n",
      "Epoch 3/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 2.2982 - avg_f1: 0.4207 - val_loss: 2.2734 - val_avg_f1: 0.4036\n",
      "Epoch 4/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 2.1777 - avg_f1: 0.4200 - val_loss: 2.1696 - val_avg_f1: 0.4036\n",
      "Epoch 5/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 2.0124 - avg_f1: 0.4379 - val_loss: 2.0686 - val_avg_f1: 0.4036\n",
      "Epoch 6/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 1.9848 - avg_f1: 0.4322 - val_loss: 1.9651 - val_avg_f1: 0.4036\n",
      "Epoch 7/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 1.8626 - avg_f1: 0.4363 - val_loss: 1.8606 - val_avg_f1: 0.4036\n",
      "Epoch 8/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 1.8022 - avg_f1: 0.4338 - val_loss: 1.7538 - val_avg_f1: 0.4036\n",
      "Epoch 9/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 1.6734 - avg_f1: 0.4435 - val_loss: 1.6491 - val_avg_f1: 0.4036\n",
      "Epoch 10/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 1.5949 - avg_f1: 0.4364 - val_loss: 1.5421 - val_avg_f1: 0.4036\n",
      "Epoch 11/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 1.4821 - avg_f1: 0.4360 - val_loss: 1.4357 - val_avg_f1: 0.4036\n",
      "Epoch 12/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 1.3807 - avg_f1: 0.4442 - val_loss: 1.3317 - val_avg_f1: 0.4031\n",
      "Epoch 13/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 1.2859 - avg_f1: 0.4565 - val_loss: 1.2368 - val_avg_f1: 0.4025\n",
      "Epoch 14/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 1.2126 - avg_f1: 0.4625 - val_loss: 1.1545 - val_avg_f1: 0.4053\n",
      "Epoch 15/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 1.1715 - avg_f1: 0.4404 - val_loss: 1.0814 - val_avg_f1: 0.4111\n",
      "Epoch 16/25\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 1.0859 - avg_f1: 0.4661 - val_loss: 1.0221 - val_avg_f1: 0.4124\n",
      "Epoch 17/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 1.0409 - avg_f1: 0.4701 - val_loss: 0.9847 - val_avg_f1: 0.4310\n",
      "Epoch 18/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 1.0272 - avg_f1: 0.4650 - val_loss: 0.9547 - val_avg_f1: 0.4383\n",
      "Epoch 19/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 1.0171 - avg_f1: 0.4551 - val_loss: 0.9356 - val_avg_f1: 0.4520\n",
      "Epoch 20/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.9883 - avg_f1: 0.4778 - val_loss: 0.9218 - val_avg_f1: 0.4533\n",
      "Epoch 21/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.9472 - avg_f1: 0.4848 - val_loss: 0.9112 - val_avg_f1: 0.4560\n",
      "Epoch 22/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.9513 - avg_f1: 0.4825 - val_loss: 0.9013 - val_avg_f1: 0.4601\n",
      "Epoch 23/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.9302 - avg_f1: 0.4910 - val_loss: 0.8925 - val_avg_f1: 0.4796\n",
      "Epoch 24/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.9175 - avg_f1: 0.5002 - val_loss: 0.8838 - val_avg_f1: 0.4821\n",
      "Epoch 25/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.9203 - avg_f1: 0.4910 - val_loss: 0.8747 - val_avg_f1: 0.4944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:31:55,577] Trial 9 finished with value: 0.49442729353904724 and parameters: {'learning_rate': 2.7295839458329498e-05, 'n_filters': 25, 'h_dim': 16, 'dropout': 0.055771344558668334, 'reg': 0.02463141816898002, 'batch_size': 128}. Best is trial 3 with value: 0.803813636302948.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #10 [of 49]:\n",
      " {'learning_rate': 0.0010029796658165312, 'n_filters': 50, 'h_dim': 16, 'dropout': 0.003434011458570604, 'reg': 1.7437921931754015e-05, 'batch_size': 32}\n",
      "Epoch 1/25\n",
      "75/75 [==============================] - 2s 13ms/step - loss: 0.5935 - avg_f1: 0.5669 - val_loss: 0.4343 - val_avg_f1: 0.7115\n",
      "Epoch 2/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.4464 - avg_f1: 0.7401 - val_loss: 0.3946 - val_avg_f1: 0.7681\n",
      "Epoch 3/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.3960 - avg_f1: 0.7830 - val_loss: 0.3859 - val_avg_f1: 0.8023\n",
      "Epoch 4/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.3559 - avg_f1: 0.8072 - val_loss: 0.4146 - val_avg_f1: 0.7842\n",
      "Epoch 5/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.2994 - avg_f1: 0.8474 - val_loss: 0.4031 - val_avg_f1: 0.7644\n",
      "Epoch 6/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.2442 - avg_f1: 0.8879 - val_loss: 0.4178 - val_avg_f1: 0.7599\n",
      "Epoch 7/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.1529 - avg_f1: 0.9412 - val_loss: 0.4776 - val_avg_f1: 0.7322\n",
      "Epoch 8/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0933 - avg_f1: 0.9778 - val_loss: 0.5670 - val_avg_f1: 0.7132\n",
      "Epoch 9/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0511 - avg_f1: 0.9931 - val_loss: 0.5214 - val_avg_f1: 0.7479\n",
      "Epoch 10/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0281 - avg_f1: 0.9980 - val_loss: 0.5744 - val_avg_f1: 0.7520\n",
      "Epoch 11/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0191 - avg_f1: 0.9982 - val_loss: 0.6554 - val_avg_f1: 0.7529\n",
      "Epoch 12/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0160 - avg_f1: 0.9985 - val_loss: 0.6386 - val_avg_f1: 0.7524\n",
      "Epoch 13/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0081 - avg_f1: 0.9995 - val_loss: 0.7007 - val_avg_f1: 0.7585\n",
      "Epoch 14/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0097 - avg_f1: 0.9996 - val_loss: 0.6973 - val_avg_f1: 0.7651\n",
      "Epoch 15/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0114 - avg_f1: 0.9989 - val_loss: 0.7447 - val_avg_f1: 0.7483\n",
      "Epoch 16/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0118 - avg_f1: 0.9991 - val_loss: 0.7422 - val_avg_f1: 0.7562\n",
      "Epoch 17/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0071 - avg_f1: 0.9995 - val_loss: 0.7372 - val_avg_f1: 0.7587\n",
      "Epoch 18/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0062 - avg_f1: 0.9996 - val_loss: 0.7695 - val_avg_f1: 0.7573\n",
      "Epoch 19/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0075 - avg_f1: 0.9989 - val_loss: 0.7574 - val_avg_f1: 0.7530\n",
      "Epoch 20/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0043 - avg_f1: 0.9994 - val_loss: 0.7877 - val_avg_f1: 0.7471\n",
      "Epoch 21/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0053 - avg_f1: 0.9996 - val_loss: 0.7842 - val_avg_f1: 0.7508\n",
      "Epoch 22/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0057 - avg_f1: 0.9981 - val_loss: 0.7886 - val_avg_f1: 0.7684\n",
      "Epoch 23/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0050 - avg_f1: 0.9990 - val_loss: 0.8152 - val_avg_f1: 0.7487\n",
      "Epoch 24/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0023 - avg_f1: 0.9994 - val_loss: 0.8104 - val_avg_f1: 0.7669\n",
      "Epoch 25/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0018 - avg_f1: 0.9989 - val_loss: 0.8507 - val_avg_f1: 0.7517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:32:12,471] Trial 10 finished with value: 0.8023478984832764 and parameters: {'learning_rate': 0.0010029796658165312, 'n_filters': 50, 'h_dim': 16, 'dropout': 0.003434011458570604, 'reg': 1.7437921931754015e-05, 'batch_size': 32}. Best is trial 3 with value: 0.803813636302948.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #11 [of 49]:\n",
      " {'learning_rate': 0.0010472050349489524, 'n_filters': 50, 'h_dim': 16, 'dropout': 0.036964179551472104, 'reg': 1.006449262286283e-05, 'batch_size': 32}\n",
      "Epoch 1/25\n",
      "75/75 [==============================] - 2s 13ms/step - loss: 0.5684 - avg_f1: 0.6258 - val_loss: 0.4700 - val_avg_f1: 0.6409\n",
      "Epoch 2/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.4386 - avg_f1: 0.7452 - val_loss: 0.4860 - val_avg_f1: 0.7587\n",
      "Epoch 3/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.3951 - avg_f1: 0.7813 - val_loss: 0.3818 - val_avg_f1: 0.7968\n",
      "Epoch 4/25\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3195 - avg_f1: 0.8374 - val_loss: 0.3971 - val_avg_f1: 0.7551\n",
      "Epoch 5/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.2447 - avg_f1: 0.8851 - val_loss: 0.4398 - val_avg_f1: 0.7369\n",
      "Epoch 6/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.1719 - avg_f1: 0.9289 - val_loss: 0.4587 - val_avg_f1: 0.7767\n",
      "Epoch 7/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.1039 - avg_f1: 0.9708 - val_loss: 0.5078 - val_avg_f1: 0.7361\n",
      "Epoch 8/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0776 - avg_f1: 0.9791 - val_loss: 0.5200 - val_avg_f1: 0.7483\n",
      "Epoch 9/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0426 - avg_f1: 0.9936 - val_loss: 0.5503 - val_avg_f1: 0.7592\n",
      "Epoch 10/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0262 - avg_f1: 0.9981 - val_loss: 0.5952 - val_avg_f1: 0.7589\n",
      "Epoch 11/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0186 - avg_f1: 0.9995 - val_loss: 0.6092 - val_avg_f1: 0.7498\n",
      "Epoch 12/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0128 - avg_f1: 0.9995 - val_loss: 0.6626 - val_avg_f1: 0.7537\n",
      "Epoch 13/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0162 - avg_f1: 0.9974 - val_loss: 0.6771 - val_avg_f1: 0.7513\n",
      "Epoch 14/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0097 - avg_f1: 0.9996 - val_loss: 0.7141 - val_avg_f1: 0.7488\n",
      "Epoch 15/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0106 - avg_f1: 0.9991 - val_loss: 0.7281 - val_avg_f1: 0.7486\n",
      "Epoch 16/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0084 - avg_f1: 0.9990 - val_loss: 0.7181 - val_avg_f1: 0.7587\n",
      "Epoch 17/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0058 - avg_f1: 0.9995 - val_loss: 0.7387 - val_avg_f1: 0.7506\n",
      "Epoch 18/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0041 - avg_f1: 0.9995 - val_loss: 0.7850 - val_avg_f1: 0.7516\n",
      "Epoch 19/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0048 - avg_f1: 0.9996 - val_loss: 0.8136 - val_avg_f1: 0.7492\n",
      "Epoch 20/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0066 - avg_f1: 0.9991 - val_loss: 0.7869 - val_avg_f1: 0.7584\n",
      "Epoch 21/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0035 - avg_f1: 0.9995 - val_loss: 0.7737 - val_avg_f1: 0.7635\n",
      "Epoch 22/25\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.0028 - avg_f1: 0.9996 - val_loss: 0.8000 - val_avg_f1: 0.7674\n",
      "Epoch 23/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0027 - avg_f1: 0.9995 - val_loss: 0.8017 - val_avg_f1: 0.7776\n",
      "Epoch 24/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0029 - avg_f1: 0.9995 - val_loss: 0.8362 - val_avg_f1: 0.7524\n",
      "Epoch 25/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0046 - avg_f1: 0.9990 - val_loss: 0.8315 - val_avg_f1: 0.7570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:32:29,474] Trial 11 finished with value: 0.7967844009399414 and parameters: {'learning_rate': 0.0010472050349489524, 'n_filters': 50, 'h_dim': 16, 'dropout': 0.036964179551472104, 'reg': 1.006449262286283e-05, 'batch_size': 32}. Best is trial 3 with value: 0.803813636302948.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #12 [of 49]:\n",
      " {'learning_rate': 0.0013058827793693808, 'n_filters': 50, 'h_dim': 16, 'dropout': 0.0009054998261541526, 'reg': 5.259138954991778e-05, 'batch_size': 32}\n",
      "Epoch 1/25\n",
      "75/75 [==============================] - 2s 13ms/step - loss: 0.5643 - avg_f1: 0.6180 - val_loss: 0.4483 - val_avg_f1: 0.6628\n",
      "Epoch 2/25\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.4462 - avg_f1: 0.7277 - val_loss: 0.4198 - val_avg_f1: 0.6975\n",
      "Epoch 3/25\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3571 - avg_f1: 0.8040 - val_loss: 0.3926 - val_avg_f1: 0.7559\n",
      "Epoch 4/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.2699 - avg_f1: 0.8651 - val_loss: 0.4283 - val_avg_f1: 0.7507\n",
      "Epoch 5/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.1918 - avg_f1: 0.9249 - val_loss: 0.4628 - val_avg_f1: 0.7410\n",
      "Epoch 6/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.1156 - avg_f1: 0.9656 - val_loss: 0.5275 - val_avg_f1: 0.7442\n",
      "Epoch 7/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0662 - avg_f1: 0.9875 - val_loss: 0.5241 - val_avg_f1: 0.7603\n",
      "Epoch 8/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0369 - avg_f1: 0.9967 - val_loss: 0.5876 - val_avg_f1: 0.7628\n",
      "Epoch 9/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0229 - avg_f1: 0.9980 - val_loss: 0.6109 - val_avg_f1: 0.7615\n",
      "Epoch 10/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0116 - avg_f1: 0.9996 - val_loss: 0.6425 - val_avg_f1: 0.7712\n",
      "Epoch 11/25\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.0115 - avg_f1: 0.9994 - val_loss: 0.7593 - val_avg_f1: 0.7306\n",
      "Epoch 12/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0071 - avg_f1: 0.9994 - val_loss: 0.7840 - val_avg_f1: 0.7357\n",
      "Epoch 13/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0107 - avg_f1: 0.9995 - val_loss: 0.7866 - val_avg_f1: 0.7325\n",
      "Epoch 14/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0064 - avg_f1: 0.9996 - val_loss: 0.8830 - val_avg_f1: 0.7237\n",
      "Epoch 15/25\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.0095 - avg_f1: 0.9994 - val_loss: 0.7846 - val_avg_f1: 0.7467\n",
      "Epoch 16/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0043 - avg_f1: 0.9995 - val_loss: 0.8153 - val_avg_f1: 0.7575\n",
      "Epoch 17/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0086 - avg_f1: 0.9994 - val_loss: 0.8429 - val_avg_f1: 0.7608\n",
      "Epoch 18/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0102 - avg_f1: 0.9991 - val_loss: 0.8932 - val_avg_f1: 0.7600\n",
      "Epoch 19/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0032 - avg_f1: 0.9995 - val_loss: 0.9248 - val_avg_f1: 0.7601\n",
      "Epoch 20/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0061 - avg_f1: 0.9995 - val_loss: 0.9270 - val_avg_f1: 0.7515\n",
      "Epoch 21/25\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.0072 - avg_f1: 0.9990 - val_loss: 0.9158 - val_avg_f1: 0.7627\n",
      "Epoch 22/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0042 - avg_f1: 0.9995 - val_loss: 0.8748 - val_avg_f1: 0.7659\n",
      "Epoch 23/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0054 - avg_f1: 0.9990 - val_loss: 0.9324 - val_avg_f1: 0.7655\n",
      "Epoch 24/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0048 - avg_f1: 0.9990 - val_loss: 0.8903 - val_avg_f1: 0.7645\n",
      "Epoch 25/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0037 - avg_f1: 0.9991 - val_loss: 0.9421 - val_avg_f1: 0.7715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:32:46,670] Trial 12 finished with value: 0.7715234160423279 and parameters: {'learning_rate': 0.0013058827793693808, 'n_filters': 50, 'h_dim': 16, 'dropout': 0.0009054998261541526, 'reg': 5.259138954991778e-05, 'batch_size': 32}. Best is trial 3 with value: 0.803813636302948.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #13 [of 49]:\n",
      " {'learning_rate': 0.06388907618909956, 'n_filters': 50, 'h_dim': 16, 'dropout': 0.1168436539308615, 'reg': 1.2963940646737457e-05, 'batch_size': 32}\n",
      "Epoch 1/25\n",
      "75/75 [==============================] - 2s 12ms/step - loss: 1.7064 - avg_f1: 0.4732 - val_loss: 0.5535 - val_avg_f1: 0.7174\n",
      "Epoch 2/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.8485 - avg_f1: 0.6027 - val_loss: 0.5151 - val_avg_f1: 0.7259\n",
      "Epoch 3/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.5784 - avg_f1: 0.6488 - val_loss: 0.5219 - val_avg_f1: 0.5113\n",
      "Epoch 4/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.4491 - avg_f1: 0.7496 - val_loss: 0.4372 - val_avg_f1: 0.7868\n",
      "Epoch 5/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.4266 - avg_f1: 0.7414 - val_loss: 0.4136 - val_avg_f1: 0.7650\n",
      "Epoch 6/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.4223 - avg_f1: 0.7708 - val_loss: 0.4185 - val_avg_f1: 0.7803\n",
      "Epoch 7/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.3969 - avg_f1: 0.7844 - val_loss: 0.4476 - val_avg_f1: 0.7469\n",
      "Epoch 8/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.3821 - avg_f1: 0.8081 - val_loss: 0.5789 - val_avg_f1: 0.7338\n",
      "Epoch 9/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.5508 - avg_f1: 0.7717 - val_loss: 0.6339 - val_avg_f1: 0.7693\n",
      "Epoch 10/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 1.2045 - avg_f1: 0.7217 - val_loss: 1.2209 - val_avg_f1: 0.7265\n",
      "Epoch 11/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 45.9951 - avg_f1: 0.6243 - val_loss: 61.7945 - val_avg_f1: 0.7442\n",
      "Epoch 12/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 49.5869 - avg_f1: 0.6346 - val_loss: 6.4557 - val_avg_f1: 0.7211\n",
      "Epoch 13/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 3.2883 - avg_f1: 0.7280 - val_loss: 39.4594 - val_avg_f1: 0.6039\n",
      "Epoch 14/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 35.3237 - avg_f1: 0.7148 - val_loss: 8.4189 - val_avg_f1: 0.6906\n",
      "Epoch 15/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 2.1323 - avg_f1: 0.7853 - val_loss: 0.6477 - val_avg_f1: 0.7237\n",
      "Epoch 16/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.6123 - avg_f1: 0.8240 - val_loss: 0.9458 - val_avg_f1: 0.7599\n",
      "Epoch 17/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.4268 - avg_f1: 0.8392 - val_loss: 1.7688 - val_avg_f1: 0.6547\n",
      "Epoch 18/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 1.6315 - avg_f1: 0.8472 - val_loss: 9.5917 - val_avg_f1: 0.6982\n",
      "Epoch 19/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 3.4154 - avg_f1: 0.8401 - val_loss: 5.7915 - val_avg_f1: 0.7008\n",
      "Epoch 20/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.7745 - avg_f1: 0.8909 - val_loss: 2.7043 - val_avg_f1: 0.7802\n",
      "Epoch 21/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.2482 - avg_f1: 0.9087 - val_loss: 1.9191 - val_avg_f1: 0.7826\n",
      "Epoch 22/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.2096 - avg_f1: 0.9087 - val_loss: 1.7489 - val_avg_f1: 0.7609\n",
      "Epoch 23/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.1746 - avg_f1: 0.9266 - val_loss: 1.5265 - val_avg_f1: 0.7700\n",
      "Epoch 24/25\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.1575 - avg_f1: 0.9291 - val_loss: 1.6352 - val_avg_f1: 0.7667\n",
      "Epoch 25/25\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.1573 - avg_f1: 0.9383 - val_loss: 3.0330 - val_avg_f1: 0.7386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:33:03,526] Trial 13 finished with value: 0.7868366241455078 and parameters: {'learning_rate': 0.06388907618909956, 'n_filters': 50, 'h_dim': 16, 'dropout': 0.1168436539308615, 'reg': 1.2963940646737457e-05, 'batch_size': 32}. Best is trial 3 with value: 0.803813636302948.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #14 [of 49]:\n",
      " {'learning_rate': 0.004183519119923477, 'n_filters': 50, 'h_dim': 128, 'dropout': 0.02260579569610703, 'reg': 3.0393816318572463e-05, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 2s 24ms/step - loss: 0.7390 - avg_f1: 0.5396 - val_loss: 0.4299 - val_avg_f1: 0.7571\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.5182 - avg_f1: 0.6765 - val_loss: 0.4101 - val_avg_f1: 0.8145\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.4109 - avg_f1: 0.7704 - val_loss: 0.4366 - val_avg_f1: 0.7727\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.3583 - avg_f1: 0.8074 - val_loss: 0.3990 - val_avg_f1: 0.8029\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.2773 - avg_f1: 0.8594 - val_loss: 0.4229 - val_avg_f1: 0.8014\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.1825 - avg_f1: 0.9206 - val_loss: 0.5053 - val_avg_f1: 0.7622\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.1031 - avg_f1: 0.9616 - val_loss: 0.6290 - val_avg_f1: 0.7401\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0955 - avg_f1: 0.9567 - val_loss: 0.6467 - val_avg_f1: 0.7924\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0504 - avg_f1: 0.9806 - val_loss: 0.7228 - val_avg_f1: 0.7817\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0241 - avg_f1: 0.9912 - val_loss: 0.8312 - val_avg_f1: 0.7543\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0177 - avg_f1: 0.9963 - val_loss: 0.9401 - val_avg_f1: 0.7549\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0126 - avg_f1: 0.9971 - val_loss: 1.0866 - val_avg_f1: 0.7460\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0312 - avg_f1: 0.9902 - val_loss: 0.9694 - val_avg_f1: 0.7580\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0322 - avg_f1: 0.9912 - val_loss: 1.0150 - val_avg_f1: 0.6946\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0233 - avg_f1: 0.9930 - val_loss: 1.1341 - val_avg_f1: 0.7564\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 16ms/step - loss: 0.0251 - avg_f1: 0.9904 - val_loss: 1.0373 - val_avg_f1: 0.7359\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0161 - avg_f1: 0.9947 - val_loss: 1.1743 - val_avg_f1: 0.7389\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0171 - avg_f1: 0.9954 - val_loss: 1.4107 - val_avg_f1: 0.7253\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0324 - avg_f1: 0.9851 - val_loss: 1.2935 - val_avg_f1: 0.7116\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0369 - avg_f1: 0.9876 - val_loss: 1.1966 - val_avg_f1: 0.7536\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0218 - avg_f1: 0.9932 - val_loss: 1.1349 - val_avg_f1: 0.7748\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 16ms/step - loss: 0.0190 - avg_f1: 0.9929 - val_loss: 1.1572 - val_avg_f1: 0.7416\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0093 - avg_f1: 0.9977 - val_loss: 1.1339 - val_avg_f1: 0.7758\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0109 - avg_f1: 0.9967 - val_loss: 1.3436 - val_avg_f1: 0.7409\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0085 - avg_f1: 0.9964 - val_loss: 1.3734 - val_avg_f1: 0.7321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:33:19,755] Trial 14 finished with value: 0.8144979476928711 and parameters: {'learning_rate': 0.004183519119923477, 'n_filters': 50, 'h_dim': 128, 'dropout': 0.02260579569610703, 'reg': 3.0393816318572463e-05, 'batch_size': 64}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #15 [of 49]:\n",
      " {'learning_rate': 0.019684478782214684, 'n_filters': 50, 'h_dim': 128, 'dropout': 0.09127631656768934, 'reg': 6.21729990665164e-05, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 2s 24ms/step - loss: 1.0849 - avg_f1: 0.5460 - val_loss: 0.5262 - val_avg_f1: 0.5007\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.4954 - avg_f1: 0.7002 - val_loss: 0.4515 - val_avg_f1: 0.6967\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.4585 - avg_f1: 0.7192 - val_loss: 0.4378 - val_avg_f1: 0.6715\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.4049 - avg_f1: 0.7598 - val_loss: 0.4210 - val_avg_f1: 0.7669\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.3455 - avg_f1: 0.8235 - val_loss: 0.4233 - val_avg_f1: 0.7914\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.2925 - avg_f1: 0.8474 - val_loss: 0.5225 - val_avg_f1: 0.7524\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 16ms/step - loss: 0.2412 - avg_f1: 0.8882 - val_loss: 0.6573 - val_avg_f1: 0.7845\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.2322 - avg_f1: 0.8948 - val_loss: 0.6352 - val_avg_f1: 0.7208\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.2340 - avg_f1: 0.8846 - val_loss: 0.6035 - val_avg_f1: 0.7400\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.1562 - avg_f1: 0.9289 - val_loss: 0.8675 - val_avg_f1: 0.7712\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.1083 - avg_f1: 0.9564 - val_loss: 0.9332 - val_avg_f1: 0.7683\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0826 - avg_f1: 0.9637 - val_loss: 1.3959 - val_avg_f1: 0.6825\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.1333 - avg_f1: 0.9434 - val_loss: 1.1664 - val_avg_f1: 0.7457\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.1002 - avg_f1: 0.9576 - val_loss: 0.9985 - val_avg_f1: 0.7659\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0859 - avg_f1: 0.9669 - val_loss: 1.2017 - val_avg_f1: 0.7514\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0556 - avg_f1: 0.9838 - val_loss: 1.4002 - val_avg_f1: 0.7733\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0818 - avg_f1: 0.9726 - val_loss: 1.4514 - val_avg_f1: 0.7573\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0721 - avg_f1: 0.9721 - val_loss: 1.2927 - val_avg_f1: 0.7530\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0452 - avg_f1: 0.9771 - val_loss: 1.3307 - val_avg_f1: 0.7489\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0395 - avg_f1: 0.9841 - val_loss: 1.4335 - val_avg_f1: 0.7604\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0460 - avg_f1: 0.9810 - val_loss: 1.5689 - val_avg_f1: 0.7532\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0534 - avg_f1: 0.9756 - val_loss: 1.9459 - val_avg_f1: 0.7646\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 16ms/step - loss: 0.1399 - avg_f1: 0.9675 - val_loss: 3.2409 - val_avg_f1: 0.7389\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 15.6159 - avg_f1: 0.7075 - val_loss: 5.0405 - val_avg_f1: 0.7491\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 1.1539 - avg_f1: 0.8769 - val_loss: 2.7185 - val_avg_f1: 0.7560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:33:35,906] Trial 15 finished with value: 0.7914456725120544 and parameters: {'learning_rate': 0.019684478782214684, 'n_filters': 50, 'h_dim': 128, 'dropout': 0.09127631656768934, 'reg': 6.21729990665164e-05, 'batch_size': 64}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #16 [of 49]:\n",
      " {'learning_rate': 0.004040133254767517, 'n_filters': 50, 'h_dim': 128, 'dropout': 0.1531033161509487, 'reg': 7.84087811266926e-05, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 2s 24ms/step - loss: 0.8334 - avg_f1: 0.5648 - val_loss: 0.4681 - val_avg_f1: 0.6682\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.5035 - avg_f1: 0.7224 - val_loss: 0.4179 - val_avg_f1: 0.7967\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.4223 - avg_f1: 0.7663 - val_loss: 0.3882 - val_avg_f1: 0.7855\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.3328 - avg_f1: 0.8287 - val_loss: 0.4416 - val_avg_f1: 0.7398\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.2611 - avg_f1: 0.8706 - val_loss: 0.6381 - val_avg_f1: 0.6417\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.1990 - avg_f1: 0.9082 - val_loss: 0.5176 - val_avg_f1: 0.7861\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.1493 - avg_f1: 0.9359 - val_loss: 0.6114 - val_avg_f1: 0.7623\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.1150 - avg_f1: 0.9459 - val_loss: 0.6469 - val_avg_f1: 0.7487\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0995 - avg_f1: 0.9583 - val_loss: 0.5980 - val_avg_f1: 0.7656\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0533 - avg_f1: 0.9766 - val_loss: 0.7000 - val_avg_f1: 0.7612\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0430 - avg_f1: 0.9833 - val_loss: 0.7431 - val_avg_f1: 0.7692\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0393 - avg_f1: 0.9853 - val_loss: 0.8292 - val_avg_f1: 0.7520\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0422 - avg_f1: 0.9823 - val_loss: 0.8894 - val_avg_f1: 0.7652\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0394 - avg_f1: 0.9833 - val_loss: 0.9355 - val_avg_f1: 0.7571\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 16ms/step - loss: 0.0490 - avg_f1: 0.9782 - val_loss: 0.7740 - val_avg_f1: 0.7789\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0291 - avg_f1: 0.9878 - val_loss: 1.4763 - val_avg_f1: 0.6746\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0438 - avg_f1: 0.9827 - val_loss: 1.1388 - val_avg_f1: 0.7160\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0332 - avg_f1: 0.9859 - val_loss: 0.9601 - val_avg_f1: 0.7564\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0198 - avg_f1: 0.9908 - val_loss: 1.1056 - val_avg_f1: 0.7484\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0201 - avg_f1: 0.9921 - val_loss: 1.2245 - val_avg_f1: 0.7483\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0202 - avg_f1: 0.9936 - val_loss: 1.1236 - val_avg_f1: 0.7404\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0198 - avg_f1: 0.9923 - val_loss: 1.0514 - val_avg_f1: 0.7655\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0094 - avg_f1: 0.9963 - val_loss: 1.2223 - val_avg_f1: 0.7437\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0374 - avg_f1: 0.9868 - val_loss: 1.0048 - val_avg_f1: 0.7413\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0276 - avg_f1: 0.9886 - val_loss: 1.4679 - val_avg_f1: 0.6917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:33:52,093] Trial 16 finished with value: 0.7966998815536499 and parameters: {'learning_rate': 0.004040133254767517, 'n_filters': 50, 'h_dim': 128, 'dropout': 0.1531033161509487, 'reg': 7.84087811266926e-05, 'batch_size': 64}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #17 [of 49]:\n",
      " {'learning_rate': 0.044287410205308714, 'n_filters': 50, 'h_dim': 128, 'dropout': 0.058655174695049384, 'reg': 3.7038318445005364e-05, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 2s 24ms/step - loss: 2.9798 - avg_f1: 0.4502 - val_loss: 0.6707 - val_avg_f1: 0.7273\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.6450 - avg_f1: 0.6752 - val_loss: 0.4489 - val_avg_f1: 0.6950\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.4618 - avg_f1: 0.7338 - val_loss: 0.5232 - val_avg_f1: 0.6363\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.3916 - avg_f1: 0.7812 - val_loss: 0.4557 - val_avg_f1: 0.7531\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 16ms/step - loss: 0.3292 - avg_f1: 0.8366 - val_loss: 0.8219 - val_avg_f1: 0.7166\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.2874 - avg_f1: 0.8542 - val_loss: 0.6316 - val_avg_f1: 0.7319\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.3118 - avg_f1: 0.8539 - val_loss: 0.7996 - val_avg_f1: 0.6749\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.2394 - avg_f1: 0.8866 - val_loss: 0.9518 - val_avg_f1: 0.6712\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.1971 - avg_f1: 0.9073 - val_loss: 0.8171 - val_avg_f1: 0.7302\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.2105 - avg_f1: 0.9143 - val_loss: 1.2852 - val_avg_f1: 0.7397\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.1836 - avg_f1: 0.9222 - val_loss: 1.0513 - val_avg_f1: 0.7113\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.1543 - avg_f1: 0.9383 - val_loss: 1.1809 - val_avg_f1: 0.7161\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.1520 - avg_f1: 0.9373 - val_loss: 1.2542 - val_avg_f1: 0.7424\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0795 - avg_f1: 0.9624 - val_loss: 1.7294 - val_avg_f1: 0.6910\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0944 - avg_f1: 0.9608 - val_loss: 1.6287 - val_avg_f1: 0.7280\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.0594 - avg_f1: 0.9771 - val_loss: 2.0749 - val_avg_f1: 0.7457\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.1969 - avg_f1: 0.9323 - val_loss: 1.7861 - val_avg_f1: 0.6992\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.2382 - avg_f1: 0.9184 - val_loss: 2.1137 - val_avg_f1: 0.7359\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.1973 - avg_f1: 0.9365 - val_loss: 2.2790 - val_avg_f1: 0.7383\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.3071 - avg_f1: 0.9300 - val_loss: 2.8348 - val_avg_f1: 0.6917\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.3050 - avg_f1: 0.9272 - val_loss: 3.0097 - val_avg_f1: 0.6923\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 0.5469 - avg_f1: 0.9066 - val_loss: 2.3097 - val_avg_f1: 0.6825\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 1.1915 - avg_f1: 0.8824 - val_loss: 9.5035 - val_avg_f1: 0.7104\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 179.8671 - avg_f1: 0.6891 - val_loss: 518.8696 - val_avg_f1: 0.5046\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 105.0279 - avg_f1: 0.7094 - val_loss: 86.5432 - val_avg_f1: 0.5851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:34:08,227] Trial 17 finished with value: 0.753142237663269 and parameters: {'learning_rate': 0.044287410205308714, 'n_filters': 50, 'h_dim': 128, 'dropout': 0.058655174695049384, 'reg': 3.7038318445005364e-05, 'batch_size': 64}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #18 [of 49]:\n",
      " {'learning_rate': 0.0032571227249209036, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.1721313811586473, 'reg': 0.00016252930217279182, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 2s 34ms/step - loss: 0.7045 - avg_f1: 0.5861 - val_loss: 0.5182 - val_avg_f1: 0.6008\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.4765 - avg_f1: 0.7174 - val_loss: 0.4000 - val_avg_f1: 0.8040\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.3907 - avg_f1: 0.7792 - val_loss: 0.4241 - val_avg_f1: 0.7860\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.3512 - avg_f1: 0.8129 - val_loss: 0.4153 - val_avg_f1: 0.7653\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.2393 - avg_f1: 0.8812 - val_loss: 0.4463 - val_avg_f1: 0.7912\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.1629 - avg_f1: 0.9291 - val_loss: 0.6721 - val_avg_f1: 0.6666\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.1645 - avg_f1: 0.9200 - val_loss: 0.5497 - val_avg_f1: 0.7729\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0899 - avg_f1: 0.9641 - val_loss: 0.6233 - val_avg_f1: 0.7937\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0690 - avg_f1: 0.9703 - val_loss: 0.7333 - val_avg_f1: 0.7557\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0428 - avg_f1: 0.9867 - val_loss: 0.7810 - val_avg_f1: 0.7787\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0281 - avg_f1: 0.9904 - val_loss: 0.9308 - val_avg_f1: 0.7520\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0284 - avg_f1: 0.9907 - val_loss: 0.9695 - val_avg_f1: 0.7400\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0235 - avg_f1: 0.9924 - val_loss: 0.9446 - val_avg_f1: 0.7680\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0168 - avg_f1: 0.9944 - val_loss: 1.0046 - val_avg_f1: 0.7673\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0290 - avg_f1: 0.9908 - val_loss: 0.9401 - val_avg_f1: 0.7819\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0438 - avg_f1: 0.9842 - val_loss: 1.1865 - val_avg_f1: 0.7036\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0602 - avg_f1: 0.9775 - val_loss: 0.8179 - val_avg_f1: 0.7549\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0386 - avg_f1: 0.9837 - val_loss: 0.8104 - val_avg_f1: 0.7879\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0272 - avg_f1: 0.9883 - val_loss: 0.9522 - val_avg_f1: 0.7463\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0151 - avg_f1: 0.9952 - val_loss: 1.2205 - val_avg_f1: 0.7486\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0142 - avg_f1: 0.9945 - val_loss: 1.2690 - val_avg_f1: 0.7495\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0108 - avg_f1: 0.9962 - val_loss: 1.1860 - val_avg_f1: 0.7712\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0328 - avg_f1: 0.9880 - val_loss: 1.0061 - val_avg_f1: 0.7878\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 26ms/step - loss: 0.0494 - avg_f1: 0.9808 - val_loss: 1.0029 - val_avg_f1: 0.7661\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0269 - avg_f1: 0.9882 - val_loss: 1.0803 - val_avg_f1: 0.7483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:34:33,406] Trial 18 finished with value: 0.8039519190788269 and parameters: {'learning_rate': 0.0032571227249209036, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.1721313811586473, 'reg': 0.00016252930217279182, 'batch_size': 64}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #19 [of 49]:\n",
      " {'learning_rate': 0.004494177795385324, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.3069930272519521, 'reg': 0.00023538228377932436, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 4s 34ms/step - loss: 0.7571 - avg_f1: 0.5997 - val_loss: 0.4284 - val_avg_f1: 0.7334\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.4469 - avg_f1: 0.7379 - val_loss: 0.3932 - val_avg_f1: 0.7990\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.3845 - avg_f1: 0.7882 - val_loss: 0.4490 - val_avg_f1: 0.7696\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.3569 - avg_f1: 0.8100 - val_loss: 0.4262 - val_avg_f1: 0.7986\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.2683 - avg_f1: 0.8678 - val_loss: 0.4390 - val_avg_f1: 0.7866\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.2035 - avg_f1: 0.9050 - val_loss: 0.5416 - val_avg_f1: 0.7317\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1754 - avg_f1: 0.9171 - val_loss: 0.5156 - val_avg_f1: 0.7978\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1395 - avg_f1: 0.9425 - val_loss: 0.6646 - val_avg_f1: 0.7281\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1259 - avg_f1: 0.9433 - val_loss: 0.6822 - val_avg_f1: 0.7794\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1118 - avg_f1: 0.9452 - val_loss: 0.8976 - val_avg_f1: 0.7048\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0746 - avg_f1: 0.9664 - val_loss: 0.7193 - val_avg_f1: 0.7609\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0601 - avg_f1: 0.9767 - val_loss: 0.7759 - val_avg_f1: 0.7761\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0930 - avg_f1: 0.9620 - val_loss: 0.7070 - val_avg_f1: 0.7595\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0510 - avg_f1: 0.9834 - val_loss: 0.9991 - val_avg_f1: 0.7292\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0425 - avg_f1: 0.9850 - val_loss: 0.9382 - val_avg_f1: 0.7629\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0410 - avg_f1: 0.9837 - val_loss: 0.8925 - val_avg_f1: 0.7784\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0300 - avg_f1: 0.9898 - val_loss: 0.9462 - val_avg_f1: 0.7667\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0357 - avg_f1: 0.9854 - val_loss: 1.0080 - val_avg_f1: 0.7492\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0301 - avg_f1: 0.9858 - val_loss: 0.9526 - val_avg_f1: 0.7883\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0303 - avg_f1: 0.9866 - val_loss: 1.0415 - val_avg_f1: 0.7708\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0405 - avg_f1: 0.9862 - val_loss: 1.0882 - val_avg_f1: 0.7674\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0424 - avg_f1: 0.9840 - val_loss: 0.9573 - val_avg_f1: 0.7607\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0302 - avg_f1: 0.9899 - val_loss: 0.9722 - val_avg_f1: 0.7736\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0230 - avg_f1: 0.9917 - val_loss: 0.9835 - val_avg_f1: 0.7522\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0141 - avg_f1: 0.9955 - val_loss: 1.0930 - val_avg_f1: 0.7806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:35:00,088] Trial 19 finished with value: 0.7989702224731445 and parameters: {'learning_rate': 0.004494177795385324, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.3069930272519521, 'reg': 0.00023538228377932436, 'batch_size': 64}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #20 [of 49]:\n",
      " {'learning_rate': 0.0023734637218114215, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.16966683511518332, 'reg': 0.0001524589695991926, 'batch_size': 128}\n",
      "Epoch 1/25\n",
      "19/19 [==============================] - 2s 64ms/step - loss: 0.7717 - avg_f1: 0.5206 - val_loss: 0.4678 - val_avg_f1: 0.6979\n",
      "Epoch 2/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.5328 - avg_f1: 0.6696 - val_loss: 0.4305 - val_avg_f1: 0.7764\n",
      "Epoch 3/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.4548 - avg_f1: 0.7396 - val_loss: 0.4157 - val_avg_f1: 0.7749\n",
      "Epoch 4/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.4098 - avg_f1: 0.7652 - val_loss: 0.4106 - val_avg_f1: 0.7828\n",
      "Epoch 5/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.3409 - avg_f1: 0.8114 - val_loss: 0.4376 - val_avg_f1: 0.7409\n",
      "Epoch 6/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.2989 - avg_f1: 0.8489 - val_loss: 0.4612 - val_avg_f1: 0.7198\n",
      "Epoch 7/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.2213 - avg_f1: 0.9013 - val_loss: 0.5021 - val_avg_f1: 0.7305\n",
      "Epoch 8/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.1459 - avg_f1: 0.9476 - val_loss: 0.5020 - val_avg_f1: 0.7515\n",
      "Epoch 9/25\n",
      "19/19 [==============================] - 1s 46ms/step - loss: 0.1035 - avg_f1: 0.9569 - val_loss: 0.5767 - val_avg_f1: 0.7636\n",
      "Epoch 10/25\n",
      "19/19 [==============================] - 1s 46ms/step - loss: 0.0819 - avg_f1: 0.9698 - val_loss: 0.5896 - val_avg_f1: 0.7681\n",
      "Epoch 11/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0576 - avg_f1: 0.9817 - val_loss: 0.6954 - val_avg_f1: 0.7336\n",
      "Epoch 12/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0353 - avg_f1: 0.9890 - val_loss: 0.7768 - val_avg_f1: 0.7071\n",
      "Epoch 13/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0266 - avg_f1: 0.9942 - val_loss: 0.7129 - val_avg_f1: 0.7660\n",
      "Epoch 14/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0198 - avg_f1: 0.9973 - val_loss: 0.7627 - val_avg_f1: 0.7736\n",
      "Epoch 15/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0201 - avg_f1: 0.9947 - val_loss: 0.7885 - val_avg_f1: 0.7726\n",
      "Epoch 16/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0178 - avg_f1: 0.9946 - val_loss: 0.8649 - val_avg_f1: 0.7720\n",
      "Epoch 17/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0155 - avg_f1: 0.9966 - val_loss: 0.8700 - val_avg_f1: 0.7398\n",
      "Epoch 18/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0187 - avg_f1: 0.9947 - val_loss: 0.8627 - val_avg_f1: 0.7569\n",
      "Epoch 19/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0122 - avg_f1: 0.9976 - val_loss: 0.8656 - val_avg_f1: 0.7777\n",
      "Epoch 20/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0152 - avg_f1: 0.9963 - val_loss: 0.8389 - val_avg_f1: 0.7814\n",
      "Epoch 21/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0126 - avg_f1: 0.9968 - val_loss: 0.9026 - val_avg_f1: 0.7899\n",
      "Epoch 22/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0109 - avg_f1: 0.9981 - val_loss: 0.9629 - val_avg_f1: 0.7489\n",
      "Epoch 23/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0126 - avg_f1: 0.9971 - val_loss: 0.9636 - val_avg_f1: 0.7555\n",
      "Epoch 24/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0164 - avg_f1: 0.9953 - val_loss: 0.9721 - val_avg_f1: 0.7575\n",
      "Epoch 25/25\n",
      "19/19 [==============================] - 1s 46ms/step - loss: 0.0138 - avg_f1: 0.9963 - val_loss: 0.9448 - val_avg_f1: 0.7740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:35:23,361] Trial 20 finished with value: 0.7899302244186401 and parameters: {'learning_rate': 0.0023734637218114215, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.16966683511518332, 'reg': 0.0001524589695991926, 'batch_size': 128}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #21 [of 49]:\n",
      " {'learning_rate': 0.019871882209050656, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.0004496656357643649, 'reg': 3.0193290265299346e-05, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 2s 33ms/step - loss: 1.3727 - avg_f1: 0.5414 - val_loss: 0.5108 - val_avg_f1: 0.4886\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.4909 - avg_f1: 0.6787 - val_loss: 0.4157 - val_avg_f1: 0.7758\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.4232 - avg_f1: 0.7553 - val_loss: 0.4123 - val_avg_f1: 0.7789\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.3808 - avg_f1: 0.7844 - val_loss: 0.4342 - val_avg_f1: 0.7928\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.3380 - avg_f1: 0.8286 - val_loss: 0.5285 - val_avg_f1: 0.7430\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.2662 - avg_f1: 0.8723 - val_loss: 0.4865 - val_avg_f1: 0.7588\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.2331 - avg_f1: 0.8815 - val_loss: 0.5495 - val_avg_f1: 0.7659\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.1869 - avg_f1: 0.9161 - val_loss: 0.6142 - val_avg_f1: 0.7606\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1114 - avg_f1: 0.9525 - val_loss: 1.2987 - val_avg_f1: 0.7009\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.1567 - avg_f1: 0.9328 - val_loss: 0.8984 - val_avg_f1: 0.7338\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1179 - avg_f1: 0.9483 - val_loss: 0.7566 - val_avg_f1: 0.7559\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0870 - avg_f1: 0.9584 - val_loss: 0.9936 - val_avg_f1: 0.7593\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0843 - avg_f1: 0.9645 - val_loss: 0.9235 - val_avg_f1: 0.7738\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0335 - avg_f1: 0.9882 - val_loss: 1.2358 - val_avg_f1: 0.7592\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0451 - avg_f1: 0.9860 - val_loss: 1.3203 - val_avg_f1: 0.7515\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0484 - avg_f1: 0.9819 - val_loss: 1.1988 - val_avg_f1: 0.7524\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0209 - avg_f1: 0.9934 - val_loss: 1.5851 - val_avg_f1: 0.7492\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0270 - avg_f1: 0.9916 - val_loss: 1.4851 - val_avg_f1: 0.7657\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0192 - avg_f1: 0.9939 - val_loss: 1.5930 - val_avg_f1: 0.7633\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0127 - avg_f1: 0.9951 - val_loss: 1.6841 - val_avg_f1: 0.7460\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0167 - avg_f1: 0.9941 - val_loss: 1.7260 - val_avg_f1: 0.7530\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0152 - avg_f1: 0.9944 - val_loss: 1.6987 - val_avg_f1: 0.7457\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0444 - avg_f1: 0.9843 - val_loss: 1.7364 - val_avg_f1: 0.7563\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0551 - avg_f1: 0.9767 - val_loss: 2.0166 - val_avg_f1: 0.7278\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0602 - avg_f1: 0.9793 - val_loss: 1.7531 - val_avg_f1: 0.7123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:35:48,444] Trial 21 finished with value: 0.7927913069725037 and parameters: {'learning_rate': 0.019871882209050656, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.0004496656357643649, 'reg': 3.0193290265299346e-05, 'batch_size': 64}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #22 [of 49]:\n",
      " {'learning_rate': 0.0004991552186943149, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.057911680119111186, 'reg': 7.284077756198881e-05, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 2s 33ms/step - loss: 0.6006 - avg_f1: 0.5774 - val_loss: 0.4363 - val_avg_f1: 0.7871\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.4653 - avg_f1: 0.7352 - val_loss: 0.3988 - val_avg_f1: 0.7433\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.4082 - avg_f1: 0.7605 - val_loss: 0.4041 - val_avg_f1: 0.7638\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.3504 - avg_f1: 0.8155 - val_loss: 0.3961 - val_avg_f1: 0.7775\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.3107 - avg_f1: 0.8432 - val_loss: 0.4834 - val_avg_f1: 0.6756\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.2514 - avg_f1: 0.8890 - val_loss: 0.4190 - val_avg_f1: 0.7915\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1930 - avg_f1: 0.9270 - val_loss: 0.4359 - val_avg_f1: 0.7617\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1547 - avg_f1: 0.9446 - val_loss: 0.4328 - val_avg_f1: 0.7841\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1028 - avg_f1: 0.9758 - val_loss: 0.4827 - val_avg_f1: 0.7549\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0782 - avg_f1: 0.9869 - val_loss: 0.4772 - val_avg_f1: 0.7604\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0546 - avg_f1: 0.9946 - val_loss: 0.5146 - val_avg_f1: 0.7817\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0377 - avg_f1: 0.9976 - val_loss: 0.5334 - val_avg_f1: 0.7825\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0271 - avg_f1: 0.9990 - val_loss: 0.6096 - val_avg_f1: 0.7653\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0209 - avg_f1: 0.9991 - val_loss: 0.6018 - val_avg_f1: 0.7768\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0187 - avg_f1: 0.9989 - val_loss: 0.5949 - val_avg_f1: 0.7867\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0136 - avg_f1: 0.9995 - val_loss: 0.5963 - val_avg_f1: 0.7858\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0133 - avg_f1: 0.9996 - val_loss: 0.6072 - val_avg_f1: 0.7987\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0124 - avg_f1: 0.9989 - val_loss: 0.6159 - val_avg_f1: 0.8077\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0197 - avg_f1: 0.9986 - val_loss: 0.6480 - val_avg_f1: 0.7853\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0132 - avg_f1: 0.9995 - val_loss: 0.6439 - val_avg_f1: 0.7880\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0120 - avg_f1: 0.9990 - val_loss: 0.6582 - val_avg_f1: 0.7869\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0080 - avg_f1: 0.9995 - val_loss: 0.6685 - val_avg_f1: 0.7967\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0083 - avg_f1: 0.9995 - val_loss: 0.6715 - val_avg_f1: 0.7902\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0067 - avg_f1: 0.9995 - val_loss: 0.7931 - val_avg_f1: 0.7435\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0076 - avg_f1: 0.9995 - val_loss: 0.7034 - val_avg_f1: 0.7758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:36:13,650] Trial 22 finished with value: 0.8077002763748169 and parameters: {'learning_rate': 0.0004991552186943149, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.057911680119111186, 'reg': 7.284077756198881e-05, 'batch_size': 64}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #23 [of 49]:\n",
      " {'learning_rate': 0.0006865718525273194, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.08860876865637328, 'reg': 6.983529488461392e-05, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 2s 33ms/step - loss: 0.5886 - avg_f1: 0.5934 - val_loss: 0.4215 - val_avg_f1: 0.7841\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.4606 - avg_f1: 0.7283 - val_loss: 0.4360 - val_avg_f1: 0.6934\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.4087 - avg_f1: 0.7634 - val_loss: 0.4192 - val_avg_f1: 0.7788\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.3413 - avg_f1: 0.8247 - val_loss: 0.4031 - val_avg_f1: 0.8060\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.2749 - avg_f1: 0.8597 - val_loss: 0.4539 - val_avg_f1: 0.7378\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.2057 - avg_f1: 0.9123 - val_loss: 0.4363 - val_avg_f1: 0.7760\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1318 - avg_f1: 0.9559 - val_loss: 0.4467 - val_avg_f1: 0.7761\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1085 - avg_f1: 0.9666 - val_loss: 0.4740 - val_avg_f1: 0.7836\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0744 - avg_f1: 0.9793 - val_loss: 0.5624 - val_avg_f1: 0.7516\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0422 - avg_f1: 0.9934 - val_loss: 0.6010 - val_avg_f1: 0.7416\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0269 - avg_f1: 0.9985 - val_loss: 0.5690 - val_avg_f1: 0.7629\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0234 - avg_f1: 0.9971 - val_loss: 0.6008 - val_avg_f1: 0.7769\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0145 - avg_f1: 0.9995 - val_loss: 0.6251 - val_avg_f1: 0.7551\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0133 - avg_f1: 0.9996 - val_loss: 0.6776 - val_avg_f1: 0.7446\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0138 - avg_f1: 0.9991 - val_loss: 0.6977 - val_avg_f1: 0.7503\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0097 - avg_f1: 0.9995 - val_loss: 0.6694 - val_avg_f1: 0.7731\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0130 - avg_f1: 0.9990 - val_loss: 0.7114 - val_avg_f1: 0.7521\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0112 - avg_f1: 0.9984 - val_loss: 0.8288 - val_avg_f1: 0.7354\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0085 - avg_f1: 0.9995 - val_loss: 0.7415 - val_avg_f1: 0.7553\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0079 - avg_f1: 0.9991 - val_loss: 0.7932 - val_avg_f1: 0.7554\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0068 - avg_f1: 0.9991 - val_loss: 0.7270 - val_avg_f1: 0.7655\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0084 - avg_f1: 0.9991 - val_loss: 0.7902 - val_avg_f1: 0.7613\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0050 - avg_f1: 0.9991 - val_loss: 0.7430 - val_avg_f1: 0.7718\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0089 - avg_f1: 0.9990 - val_loss: 0.7439 - val_avg_f1: 0.7694\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0069 - avg_f1: 0.9995 - val_loss: 0.7481 - val_avg_f1: 0.7839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:36:38,899] Trial 23 finished with value: 0.805972695350647 and parameters: {'learning_rate': 0.0006865718525273194, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.08860876865637328, 'reg': 6.983529488461392e-05, 'batch_size': 64}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #24 [of 49]:\n",
      " {'learning_rate': 0.0003863428048228457, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.06936768663461566, 'reg': 6.27543124486556e-05, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 2s 34ms/step - loss: 0.7097 - avg_f1: 0.5100 - val_loss: 0.4955 - val_avg_f1: 0.5822\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.5016 - avg_f1: 0.6850 - val_loss: 0.4057 - val_avg_f1: 0.7373\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.4377 - avg_f1: 0.7502 - val_loss: 0.4027 - val_avg_f1: 0.8014\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.3962 - avg_f1: 0.7776 - val_loss: 0.4083 - val_avg_f1: 0.7948\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.3537 - avg_f1: 0.8025 - val_loss: 0.3870 - val_avg_f1: 0.7943\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.3057 - avg_f1: 0.8404 - val_loss: 0.4002 - val_avg_f1: 0.7872\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.2527 - avg_f1: 0.8852 - val_loss: 0.4080 - val_avg_f1: 0.7864\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.2125 - avg_f1: 0.9133 - val_loss: 0.4111 - val_avg_f1: 0.7869\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1698 - avg_f1: 0.9392 - val_loss: 0.4240 - val_avg_f1: 0.7805\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1310 - avg_f1: 0.9592 - val_loss: 0.4392 - val_avg_f1: 0.7656\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0976 - avg_f1: 0.9829 - val_loss: 0.4536 - val_avg_f1: 0.7648\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0708 - avg_f1: 0.9871 - val_loss: 0.4514 - val_avg_f1: 0.7751\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0587 - avg_f1: 0.9910 - val_loss: 0.4720 - val_avg_f1: 0.7802\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 26ms/step - loss: 0.0434 - avg_f1: 0.9976 - val_loss: 0.5094 - val_avg_f1: 0.7702\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 26ms/step - loss: 0.0339 - avg_f1: 0.9982 - val_loss: 0.5238 - val_avg_f1: 0.7665\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 26ms/step - loss: 0.0267 - avg_f1: 0.9991 - val_loss: 0.5269 - val_avg_f1: 0.7741\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0205 - avg_f1: 0.9995 - val_loss: 0.5367 - val_avg_f1: 0.7748\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 26ms/step - loss: 0.0186 - avg_f1: 0.9995 - val_loss: 0.5616 - val_avg_f1: 0.7751\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0162 - avg_f1: 0.9995 - val_loss: 0.5449 - val_avg_f1: 0.7979\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0159 - avg_f1: 0.9994 - val_loss: 0.6259 - val_avg_f1: 0.7666\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0111 - avg_f1: 1.0000 - val_loss: 0.5888 - val_avg_f1: 0.7756\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0149 - avg_f1: 0.9991 - val_loss: 0.6074 - val_avg_f1: 0.7923\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0083 - avg_f1: 0.9990 - val_loss: 0.6176 - val_avg_f1: 0.7857\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0077 - avg_f1: 0.9995 - val_loss: 0.6911 - val_avg_f1: 0.7781\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0119 - avg_f1: 0.9990 - val_loss: 0.6610 - val_avg_f1: 0.7770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:37:04,419] Trial 24 finished with value: 0.8014312982559204 and parameters: {'learning_rate': 0.0003863428048228457, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.06936768663461566, 'reg': 6.27543124486556e-05, 'batch_size': 64}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #25 [of 49]:\n",
      " {'learning_rate': 0.0004241123305338018, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.11469221170450312, 'reg': 1.6495798297705616e-05, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 3s 34ms/step - loss: 0.6359 - avg_f1: 0.5570 - val_loss: 0.4572 - val_avg_f1: 0.7875\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.5102 - avg_f1: 0.6839 - val_loss: 0.4207 - val_avg_f1: 0.7035\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.4366 - avg_f1: 0.7538 - val_loss: 0.4452 - val_avg_f1: 0.6765\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.3929 - avg_f1: 0.7697 - val_loss: 0.3975 - val_avg_f1: 0.7689\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.3445 - avg_f1: 0.8172 - val_loss: 0.4116 - val_avg_f1: 0.7597\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.2973 - avg_f1: 0.8507 - val_loss: 0.4127 - val_avg_f1: 0.7563\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.2457 - avg_f1: 0.8859 - val_loss: 0.4126 - val_avg_f1: 0.7581\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.2074 - avg_f1: 0.9085 - val_loss: 0.4433 - val_avg_f1: 0.7555\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1642 - avg_f1: 0.9334 - val_loss: 0.4920 - val_avg_f1: 0.7191\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1232 - avg_f1: 0.9651 - val_loss: 0.4555 - val_avg_f1: 0.7831\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0887 - avg_f1: 0.9801 - val_loss: 0.4796 - val_avg_f1: 0.7813\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0673 - avg_f1: 0.9876 - val_loss: 0.4917 - val_avg_f1: 0.7824\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0537 - avg_f1: 0.9921 - val_loss: 0.5346 - val_avg_f1: 0.7666\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0396 - avg_f1: 0.9950 - val_loss: 0.5270 - val_avg_f1: 0.7879\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0298 - avg_f1: 0.9972 - val_loss: 0.5262 - val_avg_f1: 0.7811\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0247 - avg_f1: 0.9982 - val_loss: 0.5633 - val_avg_f1: 0.7647\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0202 - avg_f1: 0.9990 - val_loss: 0.5740 - val_avg_f1: 0.7671\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0197 - avg_f1: 0.9980 - val_loss: 0.5852 - val_avg_f1: 0.7893\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0158 - avg_f1: 0.9995 - val_loss: 0.6213 - val_avg_f1: 0.7618\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0141 - avg_f1: 0.9995 - val_loss: 0.6062 - val_avg_f1: 0.7849\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0157 - avg_f1: 0.9985 - val_loss: 0.6321 - val_avg_f1: 0.7821\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0104 - avg_f1: 0.9990 - val_loss: 0.6873 - val_avg_f1: 0.7527\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0157 - avg_f1: 0.9995 - val_loss: 0.6703 - val_avg_f1: 0.7688\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0101 - avg_f1: 0.9995 - val_loss: 0.6840 - val_avg_f1: 0.7716\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0081 - avg_f1: 0.9996 - val_loss: 0.7004 - val_avg_f1: 0.7661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:37:29,694] Trial 25 finished with value: 0.7893048524856567 and parameters: {'learning_rate': 0.0004241123305338018, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.11469221170450312, 'reg': 1.6495798297705616e-05, 'batch_size': 64}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #26 [of 49]:\n",
      " {'learning_rate': 0.0004970558549896878, 'n_filters': 100, 'h_dim': 32, 'dropout': 0.0414289194424037, 'reg': 9.731063762178639e-05, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 2s 32ms/step - loss: 0.6203 - avg_f1: 0.5016 - val_loss: 0.4763 - val_avg_f1: 0.6512\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.4817 - avg_f1: 0.7028 - val_loss: 0.4219 - val_avg_f1: 0.7774\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.4242 - avg_f1: 0.7570 - val_loss: 0.3959 - val_avg_f1: 0.7970\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.3827 - avg_f1: 0.7869 - val_loss: 0.3921 - val_avg_f1: 0.7951\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.3401 - avg_f1: 0.8294 - val_loss: 0.4168 - val_avg_f1: 0.7987\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.3101 - avg_f1: 0.8455 - val_loss: 0.3971 - val_avg_f1: 0.7941\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.2599 - avg_f1: 0.8815 - val_loss: 0.4005 - val_avg_f1: 0.7844\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.2191 - avg_f1: 0.9051 - val_loss: 0.4214 - val_avg_f1: 0.7735\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.1745 - avg_f1: 0.9390 - val_loss: 0.4187 - val_avg_f1: 0.7837\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.1332 - avg_f1: 0.9691 - val_loss: 0.4353 - val_avg_f1: 0.7926\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0967 - avg_f1: 0.9843 - val_loss: 0.4892 - val_avg_f1: 0.7613\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0670 - avg_f1: 0.9904 - val_loss: 0.5107 - val_avg_f1: 0.7614\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0531 - avg_f1: 0.9956 - val_loss: 0.5087 - val_avg_f1: 0.7866\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0434 - avg_f1: 0.9959 - val_loss: 0.5260 - val_avg_f1: 0.7723\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0286 - avg_f1: 0.9982 - val_loss: 0.5621 - val_avg_f1: 0.7504\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0239 - avg_f1: 0.9981 - val_loss: 0.5771 - val_avg_f1: 0.7547\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0187 - avg_f1: 0.9990 - val_loss: 0.6256 - val_avg_f1: 0.7750\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0156 - avg_f1: 0.9995 - val_loss: 0.6066 - val_avg_f1: 0.7741\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0145 - avg_f1: 0.9986 - val_loss: 0.6138 - val_avg_f1: 0.7670\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0131 - avg_f1: 0.9995 - val_loss: 0.6399 - val_avg_f1: 0.7740\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0103 - avg_f1: 0.9995 - val_loss: 0.6374 - val_avg_f1: 0.7714\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0103 - avg_f1: 0.9996 - val_loss: 0.7023 - val_avg_f1: 0.7563\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0119 - avg_f1: 0.9996 - val_loss: 0.7775 - val_avg_f1: 0.7530\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0089 - avg_f1: 0.9990 - val_loss: 0.6665 - val_avg_f1: 0.7694\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0090 - avg_f1: 0.9996 - val_loss: 0.6811 - val_avg_f1: 0.7682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:37:53,373] Trial 26 finished with value: 0.798729658126831 and parameters: {'learning_rate': 0.0004970558549896878, 'n_filters': 100, 'h_dim': 32, 'dropout': 0.0414289194424037, 'reg': 9.731063762178639e-05, 'batch_size': 64}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #27 [of 49]:\n",
      " {'learning_rate': 0.0014767724631739782, 'n_filters': 25, 'h_dim': 128, 'dropout': 0.07187980758156683, 'reg': 0.0003728674923608384, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 2s 19ms/step - loss: 0.6537 - avg_f1: 0.6013 - val_loss: 0.4895 - val_avg_f1: 0.6522\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.5293 - avg_f1: 0.7009 - val_loss: 0.4456 - val_avg_f1: 0.7121\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.4459 - avg_f1: 0.7449 - val_loss: 0.4415 - val_avg_f1: 0.7105\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.3826 - avg_f1: 0.7942 - val_loss: 0.4407 - val_avg_f1: 0.7782\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.3312 - avg_f1: 0.8233 - val_loss: 0.4515 - val_avg_f1: 0.7732\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.3074 - avg_f1: 0.8457 - val_loss: 0.4851 - val_avg_f1: 0.7435\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.2501 - avg_f1: 0.8770 - val_loss: 0.4724 - val_avg_f1: 0.7518\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.1752 - avg_f1: 0.9244 - val_loss: 0.5086 - val_avg_f1: 0.7907\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.1442 - avg_f1: 0.9369 - val_loss: 0.5468 - val_avg_f1: 0.7757\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.1134 - avg_f1: 0.9597 - val_loss: 0.7219 - val_avg_f1: 0.7008\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.1024 - avg_f1: 0.9598 - val_loss: 0.7159 - val_avg_f1: 0.7186\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0748 - avg_f1: 0.9708 - val_loss: 0.6770 - val_avg_f1: 0.7528\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0523 - avg_f1: 0.9894 - val_loss: 0.6721 - val_avg_f1: 0.7572\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 0s 11ms/step - loss: 0.0437 - avg_f1: 0.9903 - val_loss: 0.7184 - val_avg_f1: 0.7500\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0341 - avg_f1: 0.9947 - val_loss: 0.7584 - val_avg_f1: 0.7458\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0309 - avg_f1: 0.9957 - val_loss: 0.8256 - val_avg_f1: 0.7329\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0272 - avg_f1: 0.9970 - val_loss: 0.7868 - val_avg_f1: 0.7543\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0305 - avg_f1: 0.9934 - val_loss: 0.8038 - val_avg_f1: 0.7731\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0257 - avg_f1: 0.9953 - val_loss: 1.0743 - val_avg_f1: 0.6989\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0299 - avg_f1: 0.9943 - val_loss: 0.8402 - val_avg_f1: 0.7521\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0310 - avg_f1: 0.9916 - val_loss: 0.8584 - val_avg_f1: 0.7675\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0272 - avg_f1: 0.9942 - val_loss: 0.7942 - val_avg_f1: 0.7701\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0295 - avg_f1: 0.9921 - val_loss: 0.8382 - val_avg_f1: 0.7773\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0228 - avg_f1: 0.9959 - val_loss: 0.9074 - val_avg_f1: 0.7604\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0208 - avg_f1: 0.9962 - val_loss: 1.0089 - val_avg_f1: 0.7353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:38:04,980] Trial 27 finished with value: 0.7906815409660339 and parameters: {'learning_rate': 0.0014767724631739782, 'n_filters': 25, 'h_dim': 128, 'dropout': 0.07187980758156683, 'reg': 0.0003728674923608384, 'batch_size': 64}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #28 [of 49]:\n",
      " {'learning_rate': 0.0005258697509054757, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.1300067312371324, 'reg': 3.6795333002596644e-05, 'batch_size': 16}\n",
      "Epoch 1/25\n",
      "150/150 [==============================] - 3s 11ms/step - loss: 0.5630 - avg_f1: 0.6315 - val_loss: 0.4064 - val_avg_f1: 0.7750\n",
      "Epoch 2/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.4390 - avg_f1: 0.7531 - val_loss: 0.5221 - val_avg_f1: 0.5973\n",
      "Epoch 3/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.3714 - avg_f1: 0.7879 - val_loss: 0.4210 - val_avg_f1: 0.7177\n",
      "Epoch 4/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.3044 - avg_f1: 0.8282 - val_loss: 0.4120 - val_avg_f1: 0.7674\n",
      "Epoch 5/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1943 - avg_f1: 0.9135 - val_loss: 0.5287 - val_avg_f1: 0.6761\n",
      "Epoch 6/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1269 - avg_f1: 0.9480 - val_loss: 0.4621 - val_avg_f1: 0.7848\n",
      "Epoch 7/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0853 - avg_f1: 0.9672 - val_loss: 0.5193 - val_avg_f1: 0.7548\n",
      "Epoch 8/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0434 - avg_f1: 0.9873 - val_loss: 0.5672 - val_avg_f1: 0.7705\n",
      "Epoch 9/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0351 - avg_f1: 0.9938 - val_loss: 0.5918 - val_avg_f1: 0.7566\n",
      "Epoch 10/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0211 - avg_f1: 0.9965 - val_loss: 0.6609 - val_avg_f1: 0.7543\n",
      "Epoch 11/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0176 - avg_f1: 0.9964 - val_loss: 0.6967 - val_avg_f1: 0.7471\n",
      "Epoch 12/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0135 - avg_f1: 0.9944 - val_loss: 0.7047 - val_avg_f1: 0.7448\n",
      "Epoch 13/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0143 - avg_f1: 0.9990 - val_loss: 0.8022 - val_avg_f1: 0.7153\n",
      "Epoch 14/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0050 - avg_f1: 1.0000 - val_loss: 0.7510 - val_avg_f1: 0.7452\n",
      "Epoch 15/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0115 - avg_f1: 0.9983 - val_loss: 0.7796 - val_avg_f1: 0.7604\n",
      "Epoch 16/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0053 - avg_f1: 0.9996 - val_loss: 0.8188 - val_avg_f1: 0.7222\n",
      "Epoch 17/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0244 - avg_f1: 0.9907 - val_loss: 0.8702 - val_avg_f1: 0.6919\n",
      "Epoch 18/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0474 - avg_f1: 0.9799 - val_loss: 0.8633 - val_avg_f1: 0.7013\n",
      "Epoch 19/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0264 - avg_f1: 0.9895 - val_loss: 0.7823 - val_avg_f1: 0.7682\n",
      "Epoch 20/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0209 - avg_f1: 0.9928 - val_loss: 0.8358 - val_avg_f1: 0.7614\n",
      "Epoch 21/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0137 - avg_f1: 0.9943 - val_loss: 0.9314 - val_avg_f1: 0.7554\n",
      "Epoch 22/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0150 - avg_f1: 0.9936 - val_loss: 0.9992 - val_avg_f1: 0.7291\n",
      "Epoch 23/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0190 - avg_f1: 0.9899 - val_loss: 0.9259 - val_avg_f1: 0.7392\n",
      "Epoch 24/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0199 - avg_f1: 0.9943 - val_loss: 1.3329 - val_avg_f1: 0.6570\n",
      "Epoch 25/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0327 - avg_f1: 0.9824 - val_loss: 1.0615 - val_avg_f1: 0.7330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:38:39,018] Trial 28 finished with value: 0.7847700119018555 and parameters: {'learning_rate': 0.0005258697509054757, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.1300067312371324, 'reg': 3.6795333002596644e-05, 'batch_size': 16}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #29 [of 49]:\n",
      " {'learning_rate': 0.00027442785027802257, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.08554542716170126, 'reg': 9.565081797137966e-05, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 2s 34ms/step - loss: 0.6522 - avg_f1: 0.5207 - val_loss: 0.4903 - val_avg_f1: 0.7243\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.5218 - avg_f1: 0.6623 - val_loss: 0.4267 - val_avg_f1: 0.7084\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.4618 - avg_f1: 0.7255 - val_loss: 0.4038 - val_avg_f1: 0.7552\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.4349 - avg_f1: 0.7513 - val_loss: 0.4008 - val_avg_f1: 0.7555\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.3962 - avg_f1: 0.7793 - val_loss: 0.4138 - val_avg_f1: 0.7326\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.3505 - avg_f1: 0.8197 - val_loss: 0.3947 - val_avg_f1: 0.7854\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.3260 - avg_f1: 0.8277 - val_loss: 0.3903 - val_avg_f1: 0.8002\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.2840 - avg_f1: 0.8508 - val_loss: 0.4024 - val_avg_f1: 0.7858\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.2503 - avg_f1: 0.8878 - val_loss: 0.4053 - val_avg_f1: 0.7953\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.2175 - avg_f1: 0.9068 - val_loss: 0.4182 - val_avg_f1: 0.7810\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1782 - avg_f1: 0.9391 - val_loss: 0.4124 - val_avg_f1: 0.7899\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.1439 - avg_f1: 0.9578 - val_loss: 0.4224 - val_avg_f1: 0.7920\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1262 - avg_f1: 0.9635 - val_loss: 0.4398 - val_avg_f1: 0.7932\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1017 - avg_f1: 0.9773 - val_loss: 0.4533 - val_avg_f1: 0.7787\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0819 - avg_f1: 0.9849 - val_loss: 0.4537 - val_avg_f1: 0.7895\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0668 - avg_f1: 0.9924 - val_loss: 0.4852 - val_avg_f1: 0.7663\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0522 - avg_f1: 0.9946 - val_loss: 0.4852 - val_avg_f1: 0.7685\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0418 - avg_f1: 0.9957 - val_loss: 0.4970 - val_avg_f1: 0.7767\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0331 - avg_f1: 0.9991 - val_loss: 0.5418 - val_avg_f1: 0.7589\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0296 - avg_f1: 0.9990 - val_loss: 0.5538 - val_avg_f1: 0.7615\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0280 - avg_f1: 0.9974 - val_loss: 0.5406 - val_avg_f1: 0.7833\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0274 - avg_f1: 0.9957 - val_loss: 0.5279 - val_avg_f1: 0.7891\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0222 - avg_f1: 0.9973 - val_loss: 0.6018 - val_avg_f1: 0.7648\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0229 - avg_f1: 0.9996 - val_loss: 0.5700 - val_avg_f1: 0.7881\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0173 - avg_f1: 0.9990 - val_loss: 0.5735 - val_avg_f1: 0.7844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:39:04,220] Trial 29 finished with value: 0.8002115488052368 and parameters: {'learning_rate': 0.00027442785027802257, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.08554542716170126, 'reg': 9.565081797137966e-05, 'batch_size': 64}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #30 [of 49]:\n",
      " {'learning_rate': 0.0007360070272579418, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.04514043368628775, 'reg': 1.7809537141711306e-05, 'batch_size': 128}\n",
      "Epoch 1/25\n",
      "19/19 [==============================] - 2s 64ms/step - loss: 0.6958 - avg_f1: 0.4879 - val_loss: 0.5544 - val_avg_f1: 0.4036\n",
      "Epoch 2/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.5314 - avg_f1: 0.6144 - val_loss: 0.4375 - val_avg_f1: 0.7405\n",
      "Epoch 3/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.4768 - avg_f1: 0.7108 - val_loss: 0.4311 - val_avg_f1: 0.7914\n",
      "Epoch 4/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.4367 - avg_f1: 0.7551 - val_loss: 0.4041 - val_avg_f1: 0.7939\n",
      "Epoch 5/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.3858 - avg_f1: 0.7813 - val_loss: 0.4199 - val_avg_f1: 0.7269\n",
      "Epoch 6/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.3587 - avg_f1: 0.7944 - val_loss: 0.3857 - val_avg_f1: 0.8035\n",
      "Epoch 7/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.3065 - avg_f1: 0.8418 - val_loss: 0.4059 - val_avg_f1: 0.7880\n",
      "Epoch 8/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.2891 - avg_f1: 0.8541 - val_loss: 0.3955 - val_avg_f1: 0.7908\n",
      "Epoch 9/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.2189 - avg_f1: 0.9017 - val_loss: 0.4209 - val_avg_f1: 0.7629\n",
      "Epoch 10/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.1649 - avg_f1: 0.9438 - val_loss: 0.4367 - val_avg_f1: 0.7926\n",
      "Epoch 11/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.1193 - avg_f1: 0.9618 - val_loss: 0.4491 - val_avg_f1: 0.7837\n",
      "Epoch 12/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0833 - avg_f1: 0.9828 - val_loss: 0.4860 - val_avg_f1: 0.7435\n",
      "Epoch 13/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0520 - avg_f1: 0.9910 - val_loss: 0.4969 - val_avg_f1: 0.7654\n",
      "Epoch 14/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0359 - avg_f1: 0.9972 - val_loss: 0.5651 - val_avg_f1: 0.7548\n",
      "Epoch 15/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0232 - avg_f1: 0.9981 - val_loss: 0.5585 - val_avg_f1: 0.7661\n",
      "Epoch 16/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0184 - avg_f1: 0.9985 - val_loss: 0.5665 - val_avg_f1: 0.7641\n",
      "Epoch 17/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0146 - avg_f1: 0.9990 - val_loss: 0.5901 - val_avg_f1: 0.7832\n",
      "Epoch 18/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0126 - avg_f1: 0.9991 - val_loss: 0.5983 - val_avg_f1: 0.7833\n",
      "Epoch 19/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0084 - avg_f1: 0.9995 - val_loss: 0.6128 - val_avg_f1: 0.7825\n",
      "Epoch 20/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0084 - avg_f1: 0.9994 - val_loss: 0.6859 - val_avg_f1: 0.7705\n",
      "Epoch 21/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0057 - avg_f1: 1.0000 - val_loss: 0.6424 - val_avg_f1: 0.7688\n",
      "Epoch 22/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0091 - avg_f1: 0.9995 - val_loss: 0.6616 - val_avg_f1: 0.7764\n",
      "Epoch 23/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0070 - avg_f1: 0.9991 - val_loss: 0.6985 - val_avg_f1: 0.7615\n",
      "Epoch 24/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0055 - avg_f1: 0.9995 - val_loss: 0.7193 - val_avg_f1: 0.7492\n",
      "Epoch 25/25\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0071 - avg_f1: 0.9995 - val_loss: 0.6814 - val_avg_f1: 0.7763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:39:27,436] Trial 30 finished with value: 0.8034520149230957 and parameters: {'learning_rate': 0.0007360070272579418, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.04514043368628775, 'reg': 1.7809537141711306e-05, 'batch_size': 128}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #31 [of 49]:\n",
      " {'learning_rate': 0.0022188661903566925, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.18077212186397912, 'reg': 4.949311690504639e-05, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 2s 34ms/step - loss: 0.7262 - avg_f1: 0.5676 - val_loss: 0.4243 - val_avg_f1: 0.7599\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.4832 - avg_f1: 0.7164 - val_loss: 0.4470 - val_avg_f1: 0.6743\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.4160 - avg_f1: 0.7706 - val_loss: 0.4565 - val_avg_f1: 0.6859\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.3606 - avg_f1: 0.8039 - val_loss: 0.4170 - val_avg_f1: 0.7836\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.2594 - avg_f1: 0.8740 - val_loss: 0.4626 - val_avg_f1: 0.7725\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1829 - avg_f1: 0.9162 - val_loss: 0.4851 - val_avg_f1: 0.7996\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1630 - avg_f1: 0.9266 - val_loss: 0.5252 - val_avg_f1: 0.7733\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0823 - avg_f1: 0.9711 - val_loss: 0.7111 - val_avg_f1: 0.7300\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0762 - avg_f1: 0.9726 - val_loss: 0.6463 - val_avg_f1: 0.7809\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0345 - avg_f1: 0.9906 - val_loss: 0.6809 - val_avg_f1: 0.7965\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0262 - avg_f1: 0.9915 - val_loss: 0.7644 - val_avg_f1: 0.7711\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0313 - avg_f1: 0.9894 - val_loss: 0.8081 - val_avg_f1: 0.7736\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0198 - avg_f1: 0.9954 - val_loss: 0.8719 - val_avg_f1: 0.7722\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0145 - avg_f1: 0.9963 - val_loss: 1.0689 - val_avg_f1: 0.7366\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0218 - avg_f1: 0.9905 - val_loss: 0.8751 - val_avg_f1: 0.7577\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0180 - avg_f1: 0.9941 - val_loss: 1.0143 - val_avg_f1: 0.7533\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0153 - avg_f1: 0.9942 - val_loss: 0.9759 - val_avg_f1: 0.7673\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0219 - avg_f1: 0.9899 - val_loss: 1.2346 - val_avg_f1: 0.7219\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0368 - avg_f1: 0.9831 - val_loss: 0.9233 - val_avg_f1: 0.7714\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0206 - avg_f1: 0.9908 - val_loss: 1.0193 - val_avg_f1: 0.7756\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0299 - avg_f1: 0.9885 - val_loss: 1.0027 - val_avg_f1: 0.7683\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0279 - avg_f1: 0.9895 - val_loss: 1.0005 - val_avg_f1: 0.7621\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0387 - avg_f1: 0.9850 - val_loss: 1.0019 - val_avg_f1: 0.7765\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0235 - avg_f1: 0.9895 - val_loss: 1.0445 - val_avg_f1: 0.7423\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0220 - avg_f1: 0.9919 - val_loss: 1.1492 - val_avg_f1: 0.7443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:39:52,668] Trial 31 finished with value: 0.7996065020561218 and parameters: {'learning_rate': 0.0022188661903566925, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.18077212186397912, 'reg': 4.949311690504639e-05, 'batch_size': 64}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #32 [of 49]:\n",
      " {'learning_rate': 0.002699816979773009, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.13271418099814497, 'reg': 0.0001408648629656462, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 2s 34ms/step - loss: 0.7567 - avg_f1: 0.5665 - val_loss: 0.4970 - val_avg_f1: 0.7334\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.4858 - avg_f1: 0.7012 - val_loss: 0.4377 - val_avg_f1: 0.6913\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.3767 - avg_f1: 0.7991 - val_loss: 0.4316 - val_avg_f1: 0.7215\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.2925 - avg_f1: 0.8464 - val_loss: 0.4403 - val_avg_f1: 0.7373\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1866 - avg_f1: 0.9113 - val_loss: 0.5219 - val_avg_f1: 0.7456\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.1311 - avg_f1: 0.9459 - val_loss: 0.6626 - val_avg_f1: 0.7072\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0849 - avg_f1: 0.9629 - val_loss: 0.6139 - val_avg_f1: 0.7516\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0562 - avg_f1: 0.9829 - val_loss: 0.9169 - val_avg_f1: 0.6595\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0425 - avg_f1: 0.9877 - val_loss: 0.6733 - val_avg_f1: 0.7707\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0221 - avg_f1: 0.9947 - val_loss: 0.8199 - val_avg_f1: 0.7385\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0158 - avg_f1: 0.9961 - val_loss: 0.8595 - val_avg_f1: 0.7517\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0176 - avg_f1: 0.9968 - val_loss: 0.8396 - val_avg_f1: 0.7490\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0185 - avg_f1: 0.9921 - val_loss: 0.8500 - val_avg_f1: 0.7872\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0677 - avg_f1: 0.9698 - val_loss: 0.9297 - val_avg_f1: 0.7131\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0544 - avg_f1: 0.9809 - val_loss: 0.7692 - val_avg_f1: 0.7682\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0229 - avg_f1: 0.9919 - val_loss: 0.9251 - val_avg_f1: 0.7710\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0145 - avg_f1: 0.9968 - val_loss: 0.8436 - val_avg_f1: 0.7823\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0146 - avg_f1: 0.9960 - val_loss: 0.8225 - val_avg_f1: 0.8015\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0121 - avg_f1: 0.9970 - val_loss: 0.9315 - val_avg_f1: 0.7809\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0081 - avg_f1: 0.9991 - val_loss: 0.9474 - val_avg_f1: 0.7804\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0326 - avg_f1: 0.9884 - val_loss: 0.9631 - val_avg_f1: 0.7650\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0139 - avg_f1: 0.9950 - val_loss: 1.0019 - val_avg_f1: 0.7681\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0091 - avg_f1: 0.9967 - val_loss: 1.0710 - val_avg_f1: 0.7949\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0084 - avg_f1: 0.9982 - val_loss: 1.0977 - val_avg_f1: 0.7724\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0087 - avg_f1: 0.9986 - val_loss: 1.0557 - val_avg_f1: 0.7713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:40:17,925] Trial 32 finished with value: 0.8015052080154419 and parameters: {'learning_rate': 0.002699816979773009, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.13271418099814497, 'reg': 0.0001408648629656462, 'batch_size': 64}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #33 [of 49]:\n",
      " {'learning_rate': 0.0006746348719218393, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.1972463841641705, 'reg': 0.0002840100524756337, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 2s 34ms/step - loss: 0.6903 - avg_f1: 0.5833 - val_loss: 0.4264 - val_avg_f1: 0.7458\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.5413 - avg_f1: 0.6867 - val_loss: 0.4204 - val_avg_f1: 0.7172\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.4475 - avg_f1: 0.7497 - val_loss: 0.4122 - val_avg_f1: 0.7338\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.3950 - avg_f1: 0.7831 - val_loss: 0.4275 - val_avg_f1: 0.7300\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.3276 - avg_f1: 0.8315 - val_loss: 0.4297 - val_avg_f1: 0.7863\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.2774 - avg_f1: 0.8673 - val_loss: 0.4331 - val_avg_f1: 0.7788\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.2082 - avg_f1: 0.9078 - val_loss: 0.5283 - val_avg_f1: 0.6870\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1646 - avg_f1: 0.9366 - val_loss: 0.4465 - val_avg_f1: 0.7616\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.1112 - avg_f1: 0.9653 - val_loss: 0.5107 - val_avg_f1: 0.7623\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0808 - avg_f1: 0.9812 - val_loss: 0.5554 - val_avg_f1: 0.7417\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0648 - avg_f1: 0.9855 - val_loss: 0.5837 - val_avg_f1: 0.7376\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0548 - avg_f1: 0.9885 - val_loss: 0.5493 - val_avg_f1: 0.7431\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0382 - avg_f1: 0.9957 - val_loss: 0.6386 - val_avg_f1: 0.7565\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0334 - avg_f1: 0.9951 - val_loss: 0.6233 - val_avg_f1: 0.7486\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0390 - avg_f1: 0.9889 - val_loss: 0.6337 - val_avg_f1: 0.7600\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0281 - avg_f1: 0.9964 - val_loss: 0.7496 - val_avg_f1: 0.7242\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0236 - avg_f1: 0.9972 - val_loss: 0.6408 - val_avg_f1: 0.7683\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0196 - avg_f1: 0.9978 - val_loss: 0.6990 - val_avg_f1: 0.7543\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0160 - avg_f1: 0.9980 - val_loss: 0.7239 - val_avg_f1: 0.7589\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0177 - avg_f1: 0.9979 - val_loss: 0.8153 - val_avg_f1: 0.7426\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0141 - avg_f1: 0.9991 - val_loss: 0.8106 - val_avg_f1: 0.7229\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0125 - avg_f1: 0.9995 - val_loss: 0.7448 - val_avg_f1: 0.7680\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0109 - avg_f1: 0.9991 - val_loss: 0.7102 - val_avg_f1: 0.7757\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0146 - avg_f1: 0.9982 - val_loss: 0.7113 - val_avg_f1: 0.7862\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 0.0146 - avg_f1: 0.9988 - val_loss: 0.7884 - val_avg_f1: 0.7511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:40:43,174] Trial 33 finished with value: 0.7862733602523804 and parameters: {'learning_rate': 0.0006746348719218393, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.1972463841641705, 'reg': 0.0002840100524756337, 'batch_size': 64}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #34 [of 49]:\n",
      " {'learning_rate': 0.004858476135739782, 'n_filters': 100, 'h_dim': 64, 'dropout': 0.10055280082754553, 'reg': 2.4241689318411457e-05, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 2s 33ms/step - loss: 0.6933 - avg_f1: 0.5562 - val_loss: 0.4152 - val_avg_f1: 0.7465\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.4648 - avg_f1: 0.7325 - val_loss: 0.4048 - val_avg_f1: 0.7742\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.3884 - avg_f1: 0.7898 - val_loss: 0.4348 - val_avg_f1: 0.6954\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.3396 - avg_f1: 0.8119 - val_loss: 0.3798 - val_avg_f1: 0.7946\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.2592 - avg_f1: 0.8644 - val_loss: 0.4868 - val_avg_f1: 0.7662\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.1827 - avg_f1: 0.9114 - val_loss: 0.5720 - val_avg_f1: 0.7470\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.1366 - avg_f1: 0.9366 - val_loss: 0.7025 - val_avg_f1: 0.7372\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0763 - avg_f1: 0.9715 - val_loss: 0.7907 - val_avg_f1: 0.7438\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0539 - avg_f1: 0.9823 - val_loss: 0.8140 - val_avg_f1: 0.7551\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0561 - avg_f1: 0.9809 - val_loss: 0.9081 - val_avg_f1: 0.7450\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0572 - avg_f1: 0.9768 - val_loss: 0.8631 - val_avg_f1: 0.7761\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0241 - avg_f1: 0.9912 - val_loss: 0.9658 - val_avg_f1: 0.7722\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0256 - avg_f1: 0.9895 - val_loss: 1.0076 - val_avg_f1: 0.7705\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0440 - avg_f1: 0.9822 - val_loss: 0.9519 - val_avg_f1: 0.7772\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0340 - avg_f1: 0.9863 - val_loss: 1.0808 - val_avg_f1: 0.7530\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0209 - avg_f1: 0.9925 - val_loss: 1.1644 - val_avg_f1: 0.7689\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0205 - avg_f1: 0.9923 - val_loss: 1.1126 - val_avg_f1: 0.7711\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0257 - avg_f1: 0.9898 - val_loss: 1.2236 - val_avg_f1: 0.7411\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0168 - avg_f1: 0.9947 - val_loss: 1.2321 - val_avg_f1: 0.7631\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0274 - avg_f1: 0.9875 - val_loss: 1.6526 - val_avg_f1: 0.7386\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0193 - avg_f1: 0.9900 - val_loss: 1.3686 - val_avg_f1: 0.7717\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0136 - avg_f1: 0.9936 - val_loss: 1.3006 - val_avg_f1: 0.7801\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0078 - avg_f1: 0.9959 - val_loss: 1.3872 - val_avg_f1: 0.7703\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0104 - avg_f1: 0.9957 - val_loss: 1.3818 - val_avg_f1: 0.7756\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0207 - avg_f1: 0.9915 - val_loss: 1.5313 - val_avg_f1: 0.7731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:41:08,858] Trial 34 finished with value: 0.7945824265480042 and parameters: {'learning_rate': 0.004858476135739782, 'n_filters': 100, 'h_dim': 64, 'dropout': 0.10055280082754553, 'reg': 2.4241689318411457e-05, 'batch_size': 64}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #35 [of 49]:\n",
      " {'learning_rate': 0.0015021398859267013, 'n_filters': 25, 'h_dim': 128, 'dropout': 0.08844323479731153, 'reg': 8.448672418428687e-05, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 2s 20ms/step - loss: 0.7563 - avg_f1: 0.5539 - val_loss: 0.4245 - val_avg_f1: 0.7416\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.5201 - avg_f1: 0.7084 - val_loss: 0.4082 - val_avg_f1: 0.7919\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.4742 - avg_f1: 0.7401 - val_loss: 0.4031 - val_avg_f1: 0.7573\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.3968 - avg_f1: 0.7810 - val_loss: 0.4021 - val_avg_f1: 0.7852\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.3306 - avg_f1: 0.8306 - val_loss: 0.4672 - val_avg_f1: 0.7089\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.3035 - avg_f1: 0.8456 - val_loss: 0.4241 - val_avg_f1: 0.7776\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.2269 - avg_f1: 0.8880 - val_loss: 0.4453 - val_avg_f1: 0.7786\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.1736 - avg_f1: 0.9228 - val_loss: 0.5111 - val_avg_f1: 0.7436\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 0s 11ms/step - loss: 0.1650 - avg_f1: 0.9255 - val_loss: 0.4865 - val_avg_f1: 0.7811\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0969 - avg_f1: 0.9667 - val_loss: 0.5628 - val_avg_f1: 0.7398\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0870 - avg_f1: 0.9673 - val_loss: 0.6012 - val_avg_f1: 0.7696\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0741 - avg_f1: 0.9734 - val_loss: 0.6169 - val_avg_f1: 0.7712\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0465 - avg_f1: 0.9860 - val_loss: 0.6975 - val_avg_f1: 0.7665\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0406 - avg_f1: 0.9890 - val_loss: 0.7003 - val_avg_f1: 0.7723\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0313 - avg_f1: 0.9916 - val_loss: 0.7541 - val_avg_f1: 0.7577\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0264 - avg_f1: 0.9944 - val_loss: 0.8015 - val_avg_f1: 0.7531\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0210 - avg_f1: 0.9956 - val_loss: 0.7974 - val_avg_f1: 0.7559\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0253 - avg_f1: 0.9941 - val_loss: 0.8717 - val_avg_f1: 0.7332\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0273 - avg_f1: 0.9928 - val_loss: 0.9264 - val_avg_f1: 0.7424\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0373 - avg_f1: 0.9867 - val_loss: 0.9857 - val_avg_f1: 0.7336\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0419 - avg_f1: 0.9816 - val_loss: 0.8999 - val_avg_f1: 0.7426\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0281 - avg_f1: 0.9938 - val_loss: 0.8578 - val_avg_f1: 0.7628\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0282 - avg_f1: 0.9903 - val_loss: 0.9804 - val_avg_f1: 0.7302\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0233 - avg_f1: 0.9925 - val_loss: 0.9297 - val_avg_f1: 0.7762\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 0.0209 - avg_f1: 0.9940 - val_loss: 0.9135 - val_avg_f1: 0.7641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:41:20,440] Trial 35 finished with value: 0.7918965220451355 and parameters: {'learning_rate': 0.0015021398859267013, 'n_filters': 25, 'h_dim': 128, 'dropout': 0.08844323479731153, 'reg': 8.448672418428687e-05, 'batch_size': 64}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #36 [of 49]:\n",
      " {'learning_rate': 0.007388319257470385, 'n_filters': 100, 'h_dim': 32, 'dropout': 0.030790378484543725, 'reg': 4.552896642024726e-05, 'batch_size': 64}\n",
      "Epoch 1/25\n",
      "38/38 [==============================] - 2s 32ms/step - loss: 0.6618 - avg_f1: 0.5996 - val_loss: 0.4380 - val_avg_f1: 0.7800\n",
      "Epoch 2/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.4532 - avg_f1: 0.7365 - val_loss: 0.4562 - val_avg_f1: 0.6397\n",
      "Epoch 3/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.3507 - avg_f1: 0.8094 - val_loss: 0.4356 - val_avg_f1: 0.7719\n",
      "Epoch 4/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.2754 - avg_f1: 0.8599 - val_loss: 0.4605 - val_avg_f1: 0.7632\n",
      "Epoch 5/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.1766 - avg_f1: 0.9102 - val_loss: 0.5296 - val_avg_f1: 0.7570\n",
      "Epoch 6/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0939 - avg_f1: 0.9622 - val_loss: 0.7005 - val_avg_f1: 0.7530\n",
      "Epoch 7/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0674 - avg_f1: 0.9722 - val_loss: 0.8650 - val_avg_f1: 0.7248\n",
      "Epoch 8/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0622 - avg_f1: 0.9741 - val_loss: 0.8081 - val_avg_f1: 0.7577\n",
      "Epoch 9/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0519 - avg_f1: 0.9780 - val_loss: 0.8976 - val_avg_f1: 0.7373\n",
      "Epoch 10/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0425 - avg_f1: 0.9838 - val_loss: 1.0267 - val_avg_f1: 0.7888\n",
      "Epoch 11/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0397 - avg_f1: 0.9828 - val_loss: 1.0185 - val_avg_f1: 0.7420\n",
      "Epoch 12/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0344 - avg_f1: 0.9844 - val_loss: 1.0575 - val_avg_f1: 0.7427\n",
      "Epoch 13/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0426 - avg_f1: 0.9802 - val_loss: 0.9732 - val_avg_f1: 0.7574\n",
      "Epoch 14/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0376 - avg_f1: 0.9837 - val_loss: 1.2434 - val_avg_f1: 0.7521\n",
      "Epoch 15/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0769 - avg_f1: 0.9672 - val_loss: 0.9459 - val_avg_f1: 0.7193\n",
      "Epoch 16/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0378 - avg_f1: 0.9851 - val_loss: 1.0484 - val_avg_f1: 0.7575\n",
      "Epoch 17/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0239 - avg_f1: 0.9911 - val_loss: 1.2923 - val_avg_f1: 0.7252\n",
      "Epoch 18/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0078 - avg_f1: 0.9986 - val_loss: 1.4600 - val_avg_f1: 0.7564\n",
      "Epoch 19/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0078 - avg_f1: 0.9973 - val_loss: 1.3946 - val_avg_f1: 0.7575\n",
      "Epoch 20/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0178 - avg_f1: 0.9908 - val_loss: 1.4327 - val_avg_f1: 0.7244\n",
      "Epoch 21/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0259 - avg_f1: 0.9944 - val_loss: 1.3607 - val_avg_f1: 0.7352\n",
      "Epoch 22/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0152 - avg_f1: 0.9931 - val_loss: 1.4188 - val_avg_f1: 0.7506\n",
      "Epoch 23/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0086 - avg_f1: 0.9961 - val_loss: 1.6081 - val_avg_f1: 0.7563\n",
      "Epoch 24/25\n",
      "38/38 [==============================] - 1s 23ms/step - loss: 0.0269 - avg_f1: 0.9880 - val_loss: 1.5326 - val_avg_f1: 0.7542\n",
      "Epoch 25/25\n",
      "38/38 [==============================] - 1s 24ms/step - loss: 0.0483 - avg_f1: 0.9787 - val_loss: 0.9492 - val_avg_f1: 0.7353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:41:44,158] Trial 36 finished with value: 0.7888464331626892 and parameters: {'learning_rate': 0.007388319257470385, 'n_filters': 100, 'h_dim': 32, 'dropout': 0.030790378484543725, 'reg': 4.552896642024726e-05, 'batch_size': 64}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #37 [of 49]:\n",
      " {'learning_rate': 0.00021316480245701957, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.1445761658671119, 'reg': 0.0004959847281163463, 'batch_size': 16}\n",
      "Epoch 1/25\n",
      "150/150 [==============================] - 3s 11ms/step - loss: 0.6255 - avg_f1: 0.5589 - val_loss: 0.4368 - val_avg_f1: 0.7347\n",
      "Epoch 2/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.5016 - avg_f1: 0.6973 - val_loss: 0.4266 - val_avg_f1: 0.6968\n",
      "Epoch 3/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.4396 - avg_f1: 0.7502 - val_loss: 0.4130 - val_avg_f1: 0.7912\n",
      "Epoch 4/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.3929 - avg_f1: 0.7773 - val_loss: 0.4235 - val_avg_f1: 0.7085\n",
      "Epoch 5/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.3416 - avg_f1: 0.8165 - val_loss: 0.4048 - val_avg_f1: 0.7799\n",
      "Epoch 6/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.3054 - avg_f1: 0.8492 - val_loss: 0.4065 - val_avg_f1: 0.7666\n",
      "Epoch 7/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.2637 - avg_f1: 0.8661 - val_loss: 0.4094 - val_avg_f1: 0.7772\n",
      "Epoch 8/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.2212 - avg_f1: 0.8996 - val_loss: 0.4118 - val_avg_f1: 0.7873\n",
      "Epoch 9/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1800 - avg_f1: 0.9260 - val_loss: 0.4376 - val_avg_f1: 0.7690\n",
      "Epoch 10/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1412 - avg_f1: 0.9544 - val_loss: 0.4571 - val_avg_f1: 0.7636\n",
      "Epoch 11/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1104 - avg_f1: 0.9735 - val_loss: 0.4737 - val_avg_f1: 0.7648\n",
      "Epoch 12/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0815 - avg_f1: 0.9847 - val_loss: 0.4989 - val_avg_f1: 0.7543\n",
      "Epoch 13/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0700 - avg_f1: 0.9829 - val_loss: 0.5667 - val_avg_f1: 0.7470\n",
      "Epoch 14/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0535 - avg_f1: 0.9926 - val_loss: 0.5580 - val_avg_f1: 0.7601\n",
      "Epoch 15/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0467 - avg_f1: 0.9954 - val_loss: 0.5577 - val_avg_f1: 0.7599\n",
      "Epoch 16/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0405 - avg_f1: 0.9935 - val_loss: 0.5851 - val_avg_f1: 0.7565\n",
      "Epoch 17/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0356 - avg_f1: 0.9962 - val_loss: 0.5940 - val_avg_f1: 0.7680\n",
      "Epoch 18/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0347 - avg_f1: 0.9917 - val_loss: 0.5944 - val_avg_f1: 0.7678\n",
      "Epoch 19/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0279 - avg_f1: 0.9989 - val_loss: 0.6007 - val_avg_f1: 0.7596\n",
      "Epoch 20/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0293 - avg_f1: 0.9974 - val_loss: 0.6381 - val_avg_f1: 0.7607\n",
      "Epoch 21/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0251 - avg_f1: 0.9975 - val_loss: 0.6379 - val_avg_f1: 0.7679\n",
      "Epoch 22/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0241 - avg_f1: 0.9950 - val_loss: 0.6526 - val_avg_f1: 0.7575\n",
      "Epoch 23/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0239 - avg_f1: 0.9984 - val_loss: 0.6522 - val_avg_f1: 0.7624\n",
      "Epoch 24/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0182 - avg_f1: 0.9995 - val_loss: 0.6799 - val_avg_f1: 0.7633\n",
      "Epoch 25/25\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0188 - avg_f1: 0.9991 - val_loss: 0.6878 - val_avg_f1: 0.7629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:42:17,971] Trial 37 finished with value: 0.791185200214386 and parameters: {'learning_rate': 0.00021316480245701957, 'n_filters': 100, 'h_dim': 128, 'dropout': 0.1445761658671119, 'reg': 0.0004959847281163463, 'batch_size': 16}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #38 [of 49]:\n",
      " {'learning_rate': 0.0007966763798716039, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.2098189762382458, 'reg': 0.00022094972983322843, 'batch_size': 128}\n",
      "Epoch 1/25\n",
      "19/19 [==============================] - 2s 34ms/step - loss: 1.1888 - avg_f1: 0.5227 - val_loss: 0.7003 - val_avg_f1: 0.4110\n",
      "Epoch 2/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.9528 - avg_f1: 0.5541 - val_loss: 0.4350 - val_avg_f1: 0.7606\n",
      "Epoch 3/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.7042 - avg_f1: 0.6564 - val_loss: 0.4218 - val_avg_f1: 0.7633\n",
      "Epoch 4/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5843 - avg_f1: 0.6840 - val_loss: 0.4443 - val_avg_f1: 0.7970\n",
      "Epoch 5/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4947 - avg_f1: 0.7109 - val_loss: 0.4340 - val_avg_f1: 0.7530\n",
      "Epoch 6/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4627 - avg_f1: 0.7410 - val_loss: 0.4309 - val_avg_f1: 0.7216\n",
      "Epoch 7/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.4321 - avg_f1: 0.7554 - val_loss: 0.4163 - val_avg_f1: 0.8080\n",
      "Epoch 8/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.4216 - avg_f1: 0.7709 - val_loss: 0.4246 - val_avg_f1: 0.7284\n",
      "Epoch 9/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.4029 - avg_f1: 0.7813 - val_loss: 0.4157 - val_avg_f1: 0.7767\n",
      "Epoch 10/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3994 - avg_f1: 0.7931 - val_loss: 0.4209 - val_avg_f1: 0.7632\n",
      "Epoch 11/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3830 - avg_f1: 0.7944 - val_loss: 0.4131 - val_avg_f1: 0.7709\n",
      "Epoch 12/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.3608 - avg_f1: 0.8171 - val_loss: 0.4036 - val_avg_f1: 0.7954\n",
      "Epoch 13/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3379 - avg_f1: 0.8296 - val_loss: 0.4488 - val_avg_f1: 0.7144\n",
      "Epoch 14/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3397 - avg_f1: 0.8285 - val_loss: 0.4094 - val_avg_f1: 0.7844\n",
      "Epoch 15/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.3200 - avg_f1: 0.8377 - val_loss: 0.4088 - val_avg_f1: 0.7898\n",
      "Epoch 16/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.3007 - avg_f1: 0.8626 - val_loss: 0.4161 - val_avg_f1: 0.7840\n",
      "Epoch 17/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2913 - avg_f1: 0.8512 - val_loss: 0.4248 - val_avg_f1: 0.7804\n",
      "Epoch 18/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.2674 - avg_f1: 0.8700 - val_loss: 0.4165 - val_avg_f1: 0.7766\n",
      "Epoch 19/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.2618 - avg_f1: 0.8815 - val_loss: 0.4145 - val_avg_f1: 0.7932\n",
      "Epoch 20/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.2594 - avg_f1: 0.8738 - val_loss: 0.4237 - val_avg_f1: 0.7970\n",
      "Epoch 21/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.2298 - avg_f1: 0.8984 - val_loss: 0.4172 - val_avg_f1: 0.7913\n",
      "Epoch 22/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.2134 - avg_f1: 0.9060 - val_loss: 0.4499 - val_avg_f1: 0.7717\n",
      "Epoch 23/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2082 - avg_f1: 0.9089 - val_loss: 0.4552 - val_avg_f1: 0.7582\n",
      "Epoch 24/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2034 - avg_f1: 0.9048 - val_loss: 0.4719 - val_avg_f1: 0.7522\n",
      "Epoch 25/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.1948 - avg_f1: 0.9160 - val_loss: 0.4543 - val_avg_f1: 0.7802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:42:27,522] Trial 38 finished with value: 0.8079918622970581 and parameters: {'learning_rate': 0.0007966763798716039, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.2098189762382458, 'reg': 0.00022094972983322843, 'batch_size': 128}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #39 [of 49]:\n",
      " {'learning_rate': 0.00013237112612661647, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.21115533862508262, 'reg': 0.00022609595956925107, 'batch_size': 128}\n",
      "Epoch 1/25\n",
      "19/19 [==============================] - 2s 35ms/step - loss: 1.8720 - avg_f1: 0.4398 - val_loss: 0.7255 - val_avg_f1: 0.5112\n",
      "Epoch 2/25\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 1.1327 - avg_f1: 0.5027 - val_loss: 0.6813 - val_avg_f1: 0.4036\n",
      "Epoch 3/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 1.0261 - avg_f1: 0.5082 - val_loss: 0.6496 - val_avg_f1: 0.4036\n",
      "Epoch 4/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.9911 - avg_f1: 0.5310 - val_loss: 0.6046 - val_avg_f1: 0.4174\n",
      "Epoch 5/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.9514 - avg_f1: 0.5314 - val_loss: 0.5968 - val_avg_f1: 0.4174\n",
      "Epoch 6/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.9381 - avg_f1: 0.5319 - val_loss: 0.5850 - val_avg_f1: 0.4277\n",
      "Epoch 7/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.9246 - avg_f1: 0.5300 - val_loss: 0.5702 - val_avg_f1: 0.4333\n",
      "Epoch 8/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.8798 - avg_f1: 0.5658 - val_loss: 0.5687 - val_avg_f1: 0.4511\n",
      "Epoch 9/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.8802 - avg_f1: 0.5446 - val_loss: 0.5225 - val_avg_f1: 0.5353\n",
      "Epoch 10/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.8198 - avg_f1: 0.5739 - val_loss: 0.5107 - val_avg_f1: 0.5463\n",
      "Epoch 11/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.8006 - avg_f1: 0.5818 - val_loss: 0.5047 - val_avg_f1: 0.5837\n",
      "Epoch 12/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.7741 - avg_f1: 0.5995 - val_loss: 0.4644 - val_avg_f1: 0.6883\n",
      "Epoch 13/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.7119 - avg_f1: 0.6297 - val_loss: 0.4643 - val_avg_f1: 0.6819\n",
      "Epoch 14/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.7049 - avg_f1: 0.6340 - val_loss: 0.4869 - val_avg_f1: 0.6276\n",
      "Epoch 15/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.6453 - avg_f1: 0.6673 - val_loss: 0.4450 - val_avg_f1: 0.7267\n",
      "Epoch 16/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.6039 - avg_f1: 0.6745 - val_loss: 0.4424 - val_avg_f1: 0.7287\n",
      "Epoch 17/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5986 - avg_f1: 0.6818 - val_loss: 0.4341 - val_avg_f1: 0.7360\n",
      "Epoch 18/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5831 - avg_f1: 0.6946 - val_loss: 0.4289 - val_avg_f1: 0.7451\n",
      "Epoch 19/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.5367 - avg_f1: 0.7062 - val_loss: 0.4457 - val_avg_f1: 0.7061\n",
      "Epoch 20/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5170 - avg_f1: 0.7088 - val_loss: 0.4464 - val_avg_f1: 0.7095\n",
      "Epoch 21/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5107 - avg_f1: 0.7081 - val_loss: 0.4265 - val_avg_f1: 0.7652\n",
      "Epoch 22/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5086 - avg_f1: 0.7129 - val_loss: 0.4391 - val_avg_f1: 0.7257\n",
      "Epoch 23/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4992 - avg_f1: 0.7195 - val_loss: 0.4556 - val_avg_f1: 0.6837\n",
      "Epoch 24/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4745 - avg_f1: 0.7240 - val_loss: 0.4380 - val_avg_f1: 0.7275\n",
      "Epoch 25/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4680 - avg_f1: 0.7196 - val_loss: 0.4309 - val_avg_f1: 0.7471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:42:37,100] Trial 39 finished with value: 0.7652032971382141 and parameters: {'learning_rate': 0.00013237112612661647, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.21115533862508262, 'reg': 0.00022609595956925107, 'batch_size': 128}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #40 [of 49]:\n",
      " {'learning_rate': 8.638671150109208e-05, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.2671686408019936, 'reg': 2.8159006682976586e-05, 'batch_size': 128}\n",
      "Epoch 1/25\n",
      "19/19 [==============================] - 2s 35ms/step - loss: 2.7744 - avg_f1: 0.4222 - val_loss: 1.7147 - val_avg_f1: 0.2423\n",
      "Epoch 2/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 1.8283 - avg_f1: 0.4539 - val_loss: 0.7815 - val_avg_f1: 0.3587\n",
      "Epoch 3/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 1.1777 - avg_f1: 0.5141 - val_loss: 0.6281 - val_avg_f1: 0.4031\n",
      "Epoch 4/25\n",
      "19/19 [==============================] - 0s 19ms/step - loss: 1.0557 - avg_f1: 0.5117 - val_loss: 0.7109 - val_avg_f1: 0.4036\n",
      "Epoch 5/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 1.0487 - avg_f1: 0.5175 - val_loss: 0.6870 - val_avg_f1: 0.4036\n",
      "Epoch 6/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 1.0425 - avg_f1: 0.5015 - val_loss: 0.6606 - val_avg_f1: 0.4036\n",
      "Epoch 7/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.9872 - avg_f1: 0.5150 - val_loss: 0.6523 - val_avg_f1: 0.4116\n",
      "Epoch 8/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.9380 - avg_f1: 0.5212 - val_loss: 0.6483 - val_avg_f1: 0.4116\n",
      "Epoch 9/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.9752 - avg_f1: 0.5136 - val_loss: 0.6277 - val_avg_f1: 0.4116\n",
      "Epoch 10/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.9414 - avg_f1: 0.5255 - val_loss: 0.6130 - val_avg_f1: 0.4116\n",
      "Epoch 11/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.8847 - avg_f1: 0.5446 - val_loss: 0.6146 - val_avg_f1: 0.4116\n",
      "Epoch 12/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.8864 - avg_f1: 0.5380 - val_loss: 0.5916 - val_avg_f1: 0.4116\n",
      "Epoch 13/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.8253 - avg_f1: 0.5624 - val_loss: 0.6112 - val_avg_f1: 0.4116\n",
      "Epoch 14/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.8150 - avg_f1: 0.5439 - val_loss: 0.5875 - val_avg_f1: 0.4116\n",
      "Epoch 15/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.8099 - avg_f1: 0.5747 - val_loss: 0.5667 - val_avg_f1: 0.4157\n",
      "Epoch 16/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.7836 - avg_f1: 0.5938 - val_loss: 0.5503 - val_avg_f1: 0.4373\n",
      "Epoch 17/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.6801 - avg_f1: 0.6241 - val_loss: 0.5627 - val_avg_f1: 0.4373\n",
      "Epoch 18/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.7232 - avg_f1: 0.6094 - val_loss: 0.5318 - val_avg_f1: 0.4963\n",
      "Epoch 19/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.6892 - avg_f1: 0.6258 - val_loss: 0.5303 - val_avg_f1: 0.5037\n",
      "Epoch 20/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.6750 - avg_f1: 0.6179 - val_loss: 0.5262 - val_avg_f1: 0.5122\n",
      "Epoch 21/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.6648 - avg_f1: 0.6233 - val_loss: 0.5225 - val_avg_f1: 0.5248\n",
      "Epoch 22/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.6395 - avg_f1: 0.6525 - val_loss: 0.4983 - val_avg_f1: 0.5673\n",
      "Epoch 23/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.6302 - avg_f1: 0.6498 - val_loss: 0.5061 - val_avg_f1: 0.5598\n",
      "Epoch 24/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.6240 - avg_f1: 0.6460 - val_loss: 0.4802 - val_avg_f1: 0.6186\n",
      "Epoch 25/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.5802 - avg_f1: 0.6766 - val_loss: 0.5130 - val_avg_f1: 0.5717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:42:46,698] Trial 40 finished with value: 0.6185795068740845 and parameters: {'learning_rate': 8.638671150109208e-05, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.2671686408019936, 'reg': 2.8159006682976586e-05, 'batch_size': 128}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #41 [of 49]:\n",
      " {'learning_rate': 0.0008670159808652207, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.10750325350369686, 'reg': 0.0001437398715833946, 'batch_size': 128}\n",
      "Epoch 1/25\n",
      "19/19 [==============================] - 2s 34ms/step - loss: 1.0940 - avg_f1: 0.4196 - val_loss: 0.5933 - val_avg_f1: 0.4036\n",
      "Epoch 2/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.6799 - avg_f1: 0.5392 - val_loss: 0.5655 - val_avg_f1: 0.4036\n",
      "Epoch 3/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.6035 - avg_f1: 0.6005 - val_loss: 0.4655 - val_avg_f1: 0.6530\n",
      "Epoch 4/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.5655 - avg_f1: 0.6481 - val_loss: 0.4292 - val_avg_f1: 0.7726\n",
      "Epoch 5/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5152 - avg_f1: 0.6768 - val_loss: 0.4227 - val_avg_f1: 0.7695\n",
      "Epoch 6/25\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.5002 - avg_f1: 0.7019 - val_loss: 0.4319 - val_avg_f1: 0.7090\n",
      "Epoch 7/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4686 - avg_f1: 0.7277 - val_loss: 0.4090 - val_avg_f1: 0.7862\n",
      "Epoch 8/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4396 - avg_f1: 0.7543 - val_loss: 0.4196 - val_avg_f1: 0.7330\n",
      "Epoch 9/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4134 - avg_f1: 0.7690 - val_loss: 0.4060 - val_avg_f1: 0.7782\n",
      "Epoch 10/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4032 - avg_f1: 0.7803 - val_loss: 0.3951 - val_avg_f1: 0.8067\n",
      "Epoch 11/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3715 - avg_f1: 0.7979 - val_loss: 0.4072 - val_avg_f1: 0.7617\n",
      "Epoch 12/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3455 - avg_f1: 0.8240 - val_loss: 0.3999 - val_avg_f1: 0.8064\n",
      "Epoch 13/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3295 - avg_f1: 0.8273 - val_loss: 0.4012 - val_avg_f1: 0.7910\n",
      "Epoch 14/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.3098 - avg_f1: 0.8460 - val_loss: 0.4267 - val_avg_f1: 0.7578\n",
      "Epoch 15/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.2856 - avg_f1: 0.8651 - val_loss: 0.4320 - val_avg_f1: 0.7543\n",
      "Epoch 16/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2660 - avg_f1: 0.8765 - val_loss: 0.4091 - val_avg_f1: 0.7881\n",
      "Epoch 17/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.2414 - avg_f1: 0.9002 - val_loss: 0.4206 - val_avg_f1: 0.7845\n",
      "Epoch 18/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.2262 - avg_f1: 0.9006 - val_loss: 0.4396 - val_avg_f1: 0.7702\n",
      "Epoch 19/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.1997 - avg_f1: 0.9162 - val_loss: 0.4368 - val_avg_f1: 0.7745\n",
      "Epoch 20/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.1737 - avg_f1: 0.9370 - val_loss: 0.4420 - val_avg_f1: 0.7789\n",
      "Epoch 21/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.1575 - avg_f1: 0.9446 - val_loss: 0.4866 - val_avg_f1: 0.7440\n",
      "Epoch 22/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.1455 - avg_f1: 0.9479 - val_loss: 0.4809 - val_avg_f1: 0.7567\n",
      "Epoch 23/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.1227 - avg_f1: 0.9611 - val_loss: 0.4988 - val_avg_f1: 0.7615\n",
      "Epoch 24/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.1148 - avg_f1: 0.9606 - val_loss: 0.4902 - val_avg_f1: 0.7655\n",
      "Epoch 25/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.0957 - avg_f1: 0.9762 - val_loss: 0.5172 - val_avg_f1: 0.7641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:42:56,288] Trial 41 finished with value: 0.8067428469657898 and parameters: {'learning_rate': 0.0008670159808652207, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.10750325350369686, 'reg': 0.0001437398715833946, 'batch_size': 128}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #42 [of 49]:\n",
      " {'learning_rate': 0.0008267480142873699, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.029614652915995188, 'reg': 0.0001228414691408756, 'batch_size': 128}\n",
      "Epoch 1/25\n",
      "19/19 [==============================] - 2s 35ms/step - loss: 0.6438 - avg_f1: 0.5208 - val_loss: 0.5575 - val_avg_f1: 0.4485\n",
      "Epoch 2/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.5365 - avg_f1: 0.6271 - val_loss: 0.4270 - val_avg_f1: 0.7602\n",
      "Epoch 3/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4785 - avg_f1: 0.7201 - val_loss: 0.4048 - val_avg_f1: 0.7726\n",
      "Epoch 4/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4434 - avg_f1: 0.7397 - val_loss: 0.3954 - val_avg_f1: 0.7840\n",
      "Epoch 5/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4077 - avg_f1: 0.7796 - val_loss: 0.3926 - val_avg_f1: 0.7970\n",
      "Epoch 6/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3871 - avg_f1: 0.7776 - val_loss: 0.3973 - val_avg_f1: 0.7846\n",
      "Epoch 7/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3684 - avg_f1: 0.8068 - val_loss: 0.3910 - val_avg_f1: 0.8035\n",
      "Epoch 8/25\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.3449 - avg_f1: 0.8255 - val_loss: 0.4259 - val_avg_f1: 0.7402\n",
      "Epoch 9/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3204 - avg_f1: 0.8323 - val_loss: 0.4082 - val_avg_f1: 0.7738\n",
      "Epoch 10/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.2929 - avg_f1: 0.8612 - val_loss: 0.4057 - val_avg_f1: 0.7866\n",
      "Epoch 11/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.2736 - avg_f1: 0.8717 - val_loss: 0.4058 - val_avg_f1: 0.7843\n",
      "Epoch 12/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2502 - avg_f1: 0.8943 - val_loss: 0.4478 - val_avg_f1: 0.7505\n",
      "Epoch 13/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2301 - avg_f1: 0.9036 - val_loss: 0.4179 - val_avg_f1: 0.7814\n",
      "Epoch 14/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2075 - avg_f1: 0.9116 - val_loss: 0.4412 - val_avg_f1: 0.7850\n",
      "Epoch 15/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.1906 - avg_f1: 0.9260 - val_loss: 0.4393 - val_avg_f1: 0.7716\n",
      "Epoch 16/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.1636 - avg_f1: 0.9470 - val_loss: 0.4429 - val_avg_f1: 0.7738\n",
      "Epoch 17/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.1421 - avg_f1: 0.9568 - val_loss: 0.4612 - val_avg_f1: 0.7723\n",
      "Epoch 18/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.1284 - avg_f1: 0.9646 - val_loss: 0.4757 - val_avg_f1: 0.7708\n",
      "Epoch 19/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.1183 - avg_f1: 0.9693 - val_loss: 0.5176 - val_avg_f1: 0.7488\n",
      "Epoch 20/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.1164 - avg_f1: 0.9582 - val_loss: 0.5937 - val_avg_f1: 0.7222\n",
      "Epoch 21/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.0999 - avg_f1: 0.9770 - val_loss: 0.5461 - val_avg_f1: 0.7419\n",
      "Epoch 22/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.0763 - avg_f1: 0.9862 - val_loss: 0.5457 - val_avg_f1: 0.7600\n",
      "Epoch 23/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.0669 - avg_f1: 0.9874 - val_loss: 0.5575 - val_avg_f1: 0.7517\n",
      "Epoch 24/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.0545 - avg_f1: 0.9938 - val_loss: 0.5658 - val_avg_f1: 0.7588\n",
      "Epoch 25/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.0489 - avg_f1: 0.9932 - val_loss: 0.5691 - val_avg_f1: 0.7675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:43:05,895] Trial 42 finished with value: 0.803507924079895 and parameters: {'learning_rate': 0.0008267480142873699, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.029614652915995188, 'reg': 0.0001228414691408756, 'batch_size': 128}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #43 [of 49]:\n",
      " {'learning_rate': 0.00030702130176919783, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.10449937257938857, 'reg': 7.483801955676974e-05, 'batch_size': 128}\n",
      "Epoch 1/25\n",
      "19/19 [==============================] - 2s 34ms/step - loss: 0.8733 - avg_f1: 0.4870 - val_loss: 0.6403 - val_avg_f1: 0.4036\n",
      "Epoch 2/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.7791 - avg_f1: 0.5224 - val_loss: 0.5857 - val_avg_f1: 0.4036\n",
      "Epoch 3/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.6969 - avg_f1: 0.5587 - val_loss: 0.5227 - val_avg_f1: 0.5747\n",
      "Epoch 4/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.6224 - avg_f1: 0.6165 - val_loss: 0.4710 - val_avg_f1: 0.7023\n",
      "Epoch 5/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5970 - avg_f1: 0.6432 - val_loss: 0.4501 - val_avg_f1: 0.6992\n",
      "Epoch 6/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5653 - avg_f1: 0.6792 - val_loss: 0.4418 - val_avg_f1: 0.7086\n",
      "Epoch 7/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5417 - avg_f1: 0.6976 - val_loss: 0.4290 - val_avg_f1: 0.7130\n",
      "Epoch 8/25\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.5094 - avg_f1: 0.7059 - val_loss: 0.4070 - val_avg_f1: 0.7777\n",
      "Epoch 9/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4960 - avg_f1: 0.7112 - val_loss: 0.4045 - val_avg_f1: 0.7907\n",
      "Epoch 10/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4696 - avg_f1: 0.7326 - val_loss: 0.4191 - val_avg_f1: 0.7295\n",
      "Epoch 11/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4486 - avg_f1: 0.7485 - val_loss: 0.4107 - val_avg_f1: 0.7556\n",
      "Epoch 12/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4433 - avg_f1: 0.7465 - val_loss: 0.4016 - val_avg_f1: 0.7713\n",
      "Epoch 13/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4137 - avg_f1: 0.7624 - val_loss: 0.3993 - val_avg_f1: 0.7886\n",
      "Epoch 14/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4000 - avg_f1: 0.7785 - val_loss: 0.4006 - val_avg_f1: 0.7976\n",
      "Epoch 15/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.4120 - avg_f1: 0.7786 - val_loss: 0.4163 - val_avg_f1: 0.7529\n",
      "Epoch 16/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3841 - avg_f1: 0.7882 - val_loss: 0.4084 - val_avg_f1: 0.7719\n",
      "Epoch 17/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3681 - avg_f1: 0.7933 - val_loss: 0.4032 - val_avg_f1: 0.7911\n",
      "Epoch 18/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3702 - avg_f1: 0.8019 - val_loss: 0.4264 - val_avg_f1: 0.7348\n",
      "Epoch 19/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.3624 - avg_f1: 0.7971 - val_loss: 0.4015 - val_avg_f1: 0.8018\n",
      "Epoch 20/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.3503 - avg_f1: 0.8168 - val_loss: 0.4024 - val_avg_f1: 0.8110\n",
      "Epoch 21/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3406 - avg_f1: 0.8231 - val_loss: 0.4125 - val_avg_f1: 0.7699\n",
      "Epoch 22/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.3309 - avg_f1: 0.8335 - val_loss: 0.4187 - val_avg_f1: 0.7599\n",
      "Epoch 23/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3262 - avg_f1: 0.8346 - val_loss: 0.4062 - val_avg_f1: 0.7936\n",
      "Epoch 24/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.3153 - avg_f1: 0.8430 - val_loss: 0.4051 - val_avg_f1: 0.7894\n",
      "Epoch 25/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.2995 - avg_f1: 0.8458 - val_loss: 0.4151 - val_avg_f1: 0.7805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:43:15,485] Trial 43 finished with value: 0.8110414743423462 and parameters: {'learning_rate': 0.00030702130176919783, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.10449937257938857, 'reg': 7.483801955676974e-05, 'batch_size': 128}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #44 [of 49]:\n",
      " {'learning_rate': 0.0002277873271306515, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.11403880745121389, 'reg': 0.00021705693377670435, 'batch_size': 128}\n",
      "Epoch 1/25\n",
      "19/19 [==============================] - 2s 35ms/step - loss: 1.3362 - avg_f1: 0.4298 - val_loss: 0.6292 - val_avg_f1: 0.4036\n",
      "Epoch 2/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.8030 - avg_f1: 0.4946 - val_loss: 0.6287 - val_avg_f1: 0.4036\n",
      "Epoch 3/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.7692 - avg_f1: 0.5230 - val_loss: 0.5721 - val_avg_f1: 0.4174\n",
      "Epoch 4/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.7261 - avg_f1: 0.5573 - val_loss: 0.5472 - val_avg_f1: 0.4299\n",
      "Epoch 5/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.6804 - avg_f1: 0.5572 - val_loss: 0.5077 - val_avg_f1: 0.5618\n",
      "Epoch 6/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.6421 - avg_f1: 0.5976 - val_loss: 0.4896 - val_avg_f1: 0.5676\n",
      "Epoch 7/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.6425 - avg_f1: 0.5983 - val_loss: 0.4678 - val_avg_f1: 0.6399\n",
      "Epoch 8/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.6210 - avg_f1: 0.6244 - val_loss: 0.4502 - val_avg_f1: 0.6931\n",
      "Epoch 9/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5869 - avg_f1: 0.6566 - val_loss: 0.4370 - val_avg_f1: 0.7102\n",
      "Epoch 10/25\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.5644 - avg_f1: 0.6801 - val_loss: 0.4286 - val_avg_f1: 0.7151\n",
      "Epoch 11/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5407 - avg_f1: 0.7001 - val_loss: 0.4178 - val_avg_f1: 0.7489\n",
      "Epoch 12/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.5343 - avg_f1: 0.6925 - val_loss: 0.4107 - val_avg_f1: 0.7570\n",
      "Epoch 13/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.5098 - avg_f1: 0.7133 - val_loss: 0.4250 - val_avg_f1: 0.7170\n",
      "Epoch 14/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5065 - avg_f1: 0.7173 - val_loss: 0.4260 - val_avg_f1: 0.7164\n",
      "Epoch 15/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4935 - avg_f1: 0.7269 - val_loss: 0.4175 - val_avg_f1: 0.7188\n",
      "Epoch 16/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4730 - avg_f1: 0.7337 - val_loss: 0.4041 - val_avg_f1: 0.7410\n",
      "Epoch 17/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4444 - avg_f1: 0.7572 - val_loss: 0.3968 - val_avg_f1: 0.7724\n",
      "Epoch 18/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4401 - avg_f1: 0.7556 - val_loss: 0.3972 - val_avg_f1: 0.7745\n",
      "Epoch 19/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.4332 - avg_f1: 0.7558 - val_loss: 0.3970 - val_avg_f1: 0.7849\n",
      "Epoch 20/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.4189 - avg_f1: 0.7682 - val_loss: 0.4023 - val_avg_f1: 0.7699\n",
      "Epoch 21/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4117 - avg_f1: 0.7761 - val_loss: 0.3982 - val_avg_f1: 0.7758\n",
      "Epoch 22/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4112 - avg_f1: 0.7724 - val_loss: 0.4248 - val_avg_f1: 0.7235\n",
      "Epoch 23/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3982 - avg_f1: 0.7806 - val_loss: 0.4260 - val_avg_f1: 0.7260\n",
      "Epoch 24/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.3867 - avg_f1: 0.7911 - val_loss: 0.4206 - val_avg_f1: 0.7337\n",
      "Epoch 25/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3765 - avg_f1: 0.7896 - val_loss: 0.4122 - val_avg_f1: 0.7558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:43:25,099] Trial 44 finished with value: 0.7849329710006714 and parameters: {'learning_rate': 0.0002277873271306515, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.11403880745121389, 'reg': 0.00021705693377670435, 'batch_size': 128}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #45 [of 49]:\n",
      " {'learning_rate': 0.0002920466765714217, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.14257775678613638, 'reg': 0.00010702432056351712, 'batch_size': 128}\n",
      "Epoch 1/25\n",
      "19/19 [==============================] - 2s 35ms/step - loss: 2.2874 - avg_f1: 0.3708 - val_loss: 0.6289 - val_avg_f1: 0.5167\n",
      "Epoch 2/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.8877 - avg_f1: 0.4781 - val_loss: 0.7268 - val_avg_f1: 0.4036\n",
      "Epoch 3/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.7512 - avg_f1: 0.5416 - val_loss: 0.5526 - val_avg_f1: 0.4845\n",
      "Epoch 4/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.7140 - avg_f1: 0.5341 - val_loss: 0.5497 - val_avg_f1: 0.4174\n",
      "Epoch 5/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.6596 - avg_f1: 0.5643 - val_loss: 0.4963 - val_avg_f1: 0.5683\n",
      "Epoch 6/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.6332 - avg_f1: 0.5984 - val_loss: 0.4867 - val_avg_f1: 0.5869\n",
      "Epoch 7/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.6031 - avg_f1: 0.6200 - val_loss: 0.4581 - val_avg_f1: 0.6921\n",
      "Epoch 8/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5829 - avg_f1: 0.6319 - val_loss: 0.4459 - val_avg_f1: 0.7215\n",
      "Epoch 9/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5655 - avg_f1: 0.6451 - val_loss: 0.4482 - val_avg_f1: 0.6934\n",
      "Epoch 10/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5620 - avg_f1: 0.6447 - val_loss: 0.4338 - val_avg_f1: 0.7318\n",
      "Epoch 11/25\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.5483 - avg_f1: 0.6812 - val_loss: 0.4304 - val_avg_f1: 0.7406\n",
      "Epoch 12/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.5195 - avg_f1: 0.6887 - val_loss: 0.4284 - val_avg_f1: 0.7325\n",
      "Epoch 13/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5069 - avg_f1: 0.7118 - val_loss: 0.4254 - val_avg_f1: 0.7395\n",
      "Epoch 14/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.4830 - avg_f1: 0.7189 - val_loss: 0.4207 - val_avg_f1: 0.7457\n",
      "Epoch 15/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.4619 - avg_f1: 0.7351 - val_loss: 0.4212 - val_avg_f1: 0.7306\n",
      "Epoch 16/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4605 - avg_f1: 0.7373 - val_loss: 0.4143 - val_avg_f1: 0.7598\n",
      "Epoch 17/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4433 - avg_f1: 0.7526 - val_loss: 0.4179 - val_avg_f1: 0.7415\n",
      "Epoch 18/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4354 - avg_f1: 0.7563 - val_loss: 0.4153 - val_avg_f1: 0.7478\n",
      "Epoch 19/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.4247 - avg_f1: 0.7665 - val_loss: 0.4036 - val_avg_f1: 0.8033\n",
      "Epoch 20/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4132 - avg_f1: 0.7704 - val_loss: 0.4091 - val_avg_f1: 0.7735\n",
      "Epoch 21/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4117 - avg_f1: 0.7723 - val_loss: 0.4076 - val_avg_f1: 0.7711\n",
      "Epoch 22/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3957 - avg_f1: 0.7739 - val_loss: 0.4032 - val_avg_f1: 0.7976\n",
      "Epoch 23/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.3856 - avg_f1: 0.7944 - val_loss: 0.4005 - val_avg_f1: 0.8010\n",
      "Epoch 24/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3803 - avg_f1: 0.7901 - val_loss: 0.4106 - val_avg_f1: 0.7607\n",
      "Epoch 25/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3776 - avg_f1: 0.7989 - val_loss: 0.4112 - val_avg_f1: 0.7629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:43:34,735] Trial 45 finished with value: 0.8033088445663452 and parameters: {'learning_rate': 0.0002920466765714217, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.14257775678613638, 'reg': 0.00010702432056351712, 'batch_size': 128}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #46 [of 49]:\n",
      " {'learning_rate': 0.0015404454091324008, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.06994223202430883, 'reg': 0.0004899547956867739, 'batch_size': 128}\n",
      "Epoch 1/25\n",
      "19/19 [==============================] - 2s 35ms/step - loss: 1.4257 - avg_f1: 0.4446 - val_loss: 0.6426 - val_avg_f1: 0.6422\n",
      "Epoch 2/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.6871 - avg_f1: 0.5610 - val_loss: 0.4873 - val_avg_f1: 0.7339\n",
      "Epoch 3/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5909 - avg_f1: 0.6568 - val_loss: 0.4381 - val_avg_f1: 0.7678\n",
      "Epoch 4/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.5344 - avg_f1: 0.6917 - val_loss: 0.4493 - val_avg_f1: 0.6954\n",
      "Epoch 5/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.5151 - avg_f1: 0.6881 - val_loss: 0.4218 - val_avg_f1: 0.7717\n",
      "Epoch 6/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4789 - avg_f1: 0.7081 - val_loss: 0.4639 - val_avg_f1: 0.6818\n",
      "Epoch 7/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4625 - avg_f1: 0.7417 - val_loss: 0.4167 - val_avg_f1: 0.7940\n",
      "Epoch 8/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.4309 - avg_f1: 0.7690 - val_loss: 0.4303 - val_avg_f1: 0.7604\n",
      "Epoch 9/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.3819 - avg_f1: 0.7990 - val_loss: 0.4199 - val_avg_f1: 0.7684\n",
      "Epoch 10/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3527 - avg_f1: 0.8134 - val_loss: 0.4275 - val_avg_f1: 0.7658\n",
      "Epoch 11/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3096 - avg_f1: 0.8496 - val_loss: 0.4450 - val_avg_f1: 0.7654\n",
      "Epoch 12/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2829 - avg_f1: 0.8663 - val_loss: 0.4493 - val_avg_f1: 0.7624\n",
      "Epoch 13/25\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.2558 - avg_f1: 0.8796 - val_loss: 0.4951 - val_avg_f1: 0.7275\n",
      "Epoch 14/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.2365 - avg_f1: 0.8831 - val_loss: 0.4803 - val_avg_f1: 0.7574\n",
      "Epoch 15/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.1954 - avg_f1: 0.9221 - val_loss: 0.4536 - val_avg_f1: 0.7915\n",
      "Epoch 16/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.1632 - avg_f1: 0.9391 - val_loss: 0.4711 - val_avg_f1: 0.7771\n",
      "Epoch 17/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.1509 - avg_f1: 0.9466 - val_loss: 0.4754 - val_avg_f1: 0.7890\n",
      "Epoch 18/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.1149 - avg_f1: 0.9672 - val_loss: 0.4992 - val_avg_f1: 0.8035\n",
      "Epoch 19/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.0986 - avg_f1: 0.9755 - val_loss: 0.5253 - val_avg_f1: 0.7670\n",
      "Epoch 20/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.0805 - avg_f1: 0.9792 - val_loss: 0.5495 - val_avg_f1: 0.7652\n",
      "Epoch 21/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.0700 - avg_f1: 0.9848 - val_loss: 0.5926 - val_avg_f1: 0.7637\n",
      "Epoch 22/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.0555 - avg_f1: 0.9918 - val_loss: 0.6082 - val_avg_f1: 0.7706\n",
      "Epoch 23/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.0524 - avg_f1: 0.9919 - val_loss: 0.5916 - val_avg_f1: 0.7923\n",
      "Epoch 24/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.0456 - avg_f1: 0.9931 - val_loss: 0.6138 - val_avg_f1: 0.7947\n",
      "Epoch 25/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.0363 - avg_f1: 0.9950 - val_loss: 0.6468 - val_avg_f1: 0.7749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:43:44,323] Trial 46 finished with value: 0.803512692451477 and parameters: {'learning_rate': 0.0015404454091324008, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.06994223202430883, 'reg': 0.0004899547956867739, 'batch_size': 128}. Best is trial 14 with value: 0.8144979476928711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #47 [of 49]:\n",
      " {'learning_rate': 0.0003811681796003376, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.02233406550456435, 'reg': 0.0011433511889232344, 'batch_size': 128}\n",
      "Epoch 1/25\n",
      "19/19 [==============================] - 2s 35ms/step - loss: 0.6736 - avg_f1: 0.4747 - val_loss: 0.5978 - val_avg_f1: 0.5818\n",
      "Epoch 2/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.6310 - avg_f1: 0.5487 - val_loss: 0.5200 - val_avg_f1: 0.7140\n",
      "Epoch 3/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.5687 - avg_f1: 0.6434 - val_loss: 0.4573 - val_avg_f1: 0.7498\n",
      "Epoch 4/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5179 - avg_f1: 0.7049 - val_loss: 0.4315 - val_avg_f1: 0.7375\n",
      "Epoch 5/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4934 - avg_f1: 0.7263 - val_loss: 0.4267 - val_avg_f1: 0.7439\n",
      "Epoch 6/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4808 - avg_f1: 0.7169 - val_loss: 0.4123 - val_avg_f1: 0.8032\n",
      "Epoch 7/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4652 - avg_f1: 0.7440 - val_loss: 0.4094 - val_avg_f1: 0.7881\n",
      "Epoch 8/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4418 - avg_f1: 0.7541 - val_loss: 0.4086 - val_avg_f1: 0.7971\n",
      "Epoch 9/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4270 - avg_f1: 0.7669 - val_loss: 0.4067 - val_avg_f1: 0.8137\n",
      "Epoch 10/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.4131 - avg_f1: 0.7795 - val_loss: 0.4075 - val_avg_f1: 0.7879\n",
      "Epoch 11/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.4000 - avg_f1: 0.7894 - val_loss: 0.4155 - val_avg_f1: 0.7772\n",
      "Epoch 12/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3860 - avg_f1: 0.7986 - val_loss: 0.4076 - val_avg_f1: 0.8209\n",
      "Epoch 13/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.3778 - avg_f1: 0.8023 - val_loss: 0.4236 - val_avg_f1: 0.7728\n",
      "Epoch 14/25\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.3568 - avg_f1: 0.8277 - val_loss: 0.4072 - val_avg_f1: 0.8148\n",
      "Epoch 15/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3494 - avg_f1: 0.8270 - val_loss: 0.4186 - val_avg_f1: 0.7872\n",
      "Epoch 16/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3339 - avg_f1: 0.8397 - val_loss: 0.4118 - val_avg_f1: 0.7999\n",
      "Epoch 17/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.3230 - avg_f1: 0.8442 - val_loss: 0.4371 - val_avg_f1: 0.7517\n",
      "Epoch 18/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3192 - avg_f1: 0.8393 - val_loss: 0.4155 - val_avg_f1: 0.7998\n",
      "Epoch 19/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2931 - avg_f1: 0.8665 - val_loss: 0.4162 - val_avg_f1: 0.7945\n",
      "Epoch 20/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2804 - avg_f1: 0.8765 - val_loss: 0.4205 - val_avg_f1: 0.7850\n",
      "Epoch 21/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2710 - avg_f1: 0.8812 - val_loss: 0.4353 - val_avg_f1: 0.7761\n",
      "Epoch 22/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2538 - avg_f1: 0.8899 - val_loss: 0.4284 - val_avg_f1: 0.7840\n",
      "Epoch 23/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2430 - avg_f1: 0.8998 - val_loss: 0.4275 - val_avg_f1: 0.7863\n",
      "Epoch 24/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2349 - avg_f1: 0.9084 - val_loss: 0.4424 - val_avg_f1: 0.7757\n",
      "Epoch 25/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.2265 - avg_f1: 0.9142 - val_loss: 0.4624 - val_avg_f1: 0.7948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:43:53,910] Trial 47 finished with value: 0.8209121823310852 and parameters: {'learning_rate': 0.0003811681796003376, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.02233406550456435, 'reg': 0.0011433511889232344, 'batch_size': 128}. Best is trial 47 with value: 0.8209121823310852.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #48 [of 49]:\n",
      " {'learning_rate': 0.00033801733594110325, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.028267067080598764, 'reg': 0.0014371341117426456, 'batch_size': 128}\n",
      "Epoch 1/25\n",
      "19/19 [==============================] - 3s 35ms/step - loss: 0.7942 - avg_f1: 0.4054 - val_loss: 0.6687 - val_avg_f1: 0.4036\n",
      "Epoch 2/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.6662 - avg_f1: 0.5040 - val_loss: 0.5974 - val_avg_f1: 0.4174\n",
      "Epoch 3/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.6180 - avg_f1: 0.5074 - val_loss: 0.5471 - val_avg_f1: 0.5619\n",
      "Epoch 4/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5801 - avg_f1: 0.5981 - val_loss: 0.4939 - val_avg_f1: 0.6293\n",
      "Epoch 5/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5429 - avg_f1: 0.6590 - val_loss: 0.4543 - val_avg_f1: 0.7389\n",
      "Epoch 6/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5158 - avg_f1: 0.6882 - val_loss: 0.4352 - val_avg_f1: 0.7699\n",
      "Epoch 7/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.4988 - avg_f1: 0.6980 - val_loss: 0.4305 - val_avg_f1: 0.8030\n",
      "Epoch 8/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.4782 - avg_f1: 0.7419 - val_loss: 0.4192 - val_avg_f1: 0.7955\n",
      "Epoch 9/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4603 - avg_f1: 0.7409 - val_loss: 0.4159 - val_avg_f1: 0.8136\n",
      "Epoch 10/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4554 - avg_f1: 0.7514 - val_loss: 0.4118 - val_avg_f1: 0.7912\n",
      "Epoch 11/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.4412 - avg_f1: 0.7563 - val_loss: 0.4112 - val_avg_f1: 0.8097\n",
      "Epoch 12/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4181 - avg_f1: 0.7759 - val_loss: 0.4091 - val_avg_f1: 0.8051\n",
      "Epoch 13/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.4146 - avg_f1: 0.7748 - val_loss: 0.4054 - val_avg_f1: 0.8001\n",
      "Epoch 14/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4004 - avg_f1: 0.7925 - val_loss: 0.4133 - val_avg_f1: 0.7790\n",
      "Epoch 15/25\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.3854 - avg_f1: 0.7971 - val_loss: 0.4117 - val_avg_f1: 0.7792\n",
      "Epoch 16/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.3759 - avg_f1: 0.8110 - val_loss: 0.4137 - val_avg_f1: 0.7843\n",
      "Epoch 17/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3661 - avg_f1: 0.8151 - val_loss: 0.4097 - val_avg_f1: 0.7908\n",
      "Epoch 18/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3571 - avg_f1: 0.8224 - val_loss: 0.4337 - val_avg_f1: 0.7958\n",
      "Epoch 19/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3418 - avg_f1: 0.8397 - val_loss: 0.4132 - val_avg_f1: 0.7818\n",
      "Epoch 20/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3257 - avg_f1: 0.8431 - val_loss: 0.4158 - val_avg_f1: 0.8012\n",
      "Epoch 21/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3211 - avg_f1: 0.8450 - val_loss: 0.4198 - val_avg_f1: 0.7824\n",
      "Epoch 22/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.3073 - avg_f1: 0.8546 - val_loss: 0.4208 - val_avg_f1: 0.7868\n",
      "Epoch 23/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.3025 - avg_f1: 0.8545 - val_loss: 0.4320 - val_avg_f1: 0.7760\n",
      "Epoch 24/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.2879 - avg_f1: 0.8767 - val_loss: 0.4241 - val_avg_f1: 0.7998\n",
      "Epoch 25/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.2850 - avg_f1: 0.8708 - val_loss: 0.4257 - val_avg_f1: 0.7862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:44:04,672] Trial 48 finished with value: 0.8136259913444519 and parameters: {'learning_rate': 0.00033801733594110325, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.028267067080598764, 'reg': 0.0014371341117426456, 'batch_size': 128}. Best is trial 47 with value: 0.8209121823310852.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #49 [of 49]:\n",
      " {'learning_rate': 0.00014563363937858173, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.02247807171241506, 'reg': 0.001770210742493246, 'batch_size': 128}\n",
      "Epoch 1/25\n",
      "19/19 [==============================] - 2s 34ms/step - loss: 0.8106 - avg_f1: 0.4441 - val_loss: 0.7058 - val_avg_f1: 0.4196\n",
      "Epoch 2/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.7134 - avg_f1: 0.4729 - val_loss: 0.6495 - val_avg_f1: 0.4116\n",
      "Epoch 3/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.6646 - avg_f1: 0.5038 - val_loss: 0.5996 - val_avg_f1: 0.4152\n",
      "Epoch 4/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.6217 - avg_f1: 0.5361 - val_loss: 0.5526 - val_avg_f1: 0.5643\n",
      "Epoch 5/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.5951 - avg_f1: 0.5943 - val_loss: 0.5095 - val_avg_f1: 0.7015\n",
      "Epoch 6/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5630 - avg_f1: 0.6484 - val_loss: 0.4823 - val_avg_f1: 0.6820\n",
      "Epoch 7/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5366 - avg_f1: 0.6724 - val_loss: 0.4615 - val_avg_f1: 0.7128\n",
      "Epoch 8/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5247 - avg_f1: 0.6901 - val_loss: 0.4463 - val_avg_f1: 0.7313\n",
      "Epoch 9/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5055 - avg_f1: 0.7158 - val_loss: 0.4370 - val_avg_f1: 0.7275\n",
      "Epoch 10/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.5079 - avg_f1: 0.7111 - val_loss: 0.4362 - val_avg_f1: 0.7302\n",
      "Epoch 11/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4867 - avg_f1: 0.7334 - val_loss: 0.4331 - val_avg_f1: 0.7360\n",
      "Epoch 12/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.4837 - avg_f1: 0.7311 - val_loss: 0.4312 - val_avg_f1: 0.7322\n",
      "Epoch 13/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4646 - avg_f1: 0.7475 - val_loss: 0.4186 - val_avg_f1: 0.7956\n",
      "Epoch 14/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.4488 - avg_f1: 0.7645 - val_loss: 0.4172 - val_avg_f1: 0.7982\n",
      "Epoch 15/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4663 - avg_f1: 0.7440 - val_loss: 0.4144 - val_avg_f1: 0.8020\n",
      "Epoch 16/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4478 - avg_f1: 0.7640 - val_loss: 0.4163 - val_avg_f1: 0.7668\n",
      "Epoch 17/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.4421 - avg_f1: 0.7705 - val_loss: 0.4176 - val_avg_f1: 0.7603\n",
      "Epoch 18/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4357 - avg_f1: 0.7701 - val_loss: 0.4144 - val_avg_f1: 0.7633\n",
      "Epoch 19/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4308 - avg_f1: 0.7664 - val_loss: 0.4170 - val_avg_f1: 0.7593\n",
      "Epoch 20/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.4164 - avg_f1: 0.7837 - val_loss: 0.4110 - val_avg_f1: 0.7829\n",
      "Epoch 21/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.4124 - avg_f1: 0.7851 - val_loss: 0.4111 - val_avg_f1: 0.7860\n",
      "Epoch 22/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.3993 - avg_f1: 0.7969 - val_loss: 0.4104 - val_avg_f1: 0.8174\n",
      "Epoch 23/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.4022 - avg_f1: 0.7898 - val_loss: 0.4120 - val_avg_f1: 0.8096\n",
      "Epoch 24/25\n",
      "19/19 [==============================] - 0s 17ms/step - loss: 0.3950 - avg_f1: 0.8038 - val_loss: 0.4105 - val_avg_f1: 0.7965\n",
      "Epoch 25/25\n",
      "19/19 [==============================] - 0s 16ms/step - loss: 0.3852 - avg_f1: 0.8159 - val_loss: 0.4266 - val_avg_f1: 0.7448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-08 23:44:14,231] Trial 49 finished with value: 0.8173797726631165 and parameters: {'learning_rate': 0.00014563363937858173, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.02247807171241506, 'reg': 0.001770210742493246, 'batch_size': 128}. Best is trial 47 with value: 0.8209121823310852.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search done. Best val_avg_f1 = 0.8209121823310852\n",
      "Best Config: {'learning_rate': 0.0003811681796003376, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.02233406550456435, 'reg': 0.0011433511889232344, 'batch_size': 128}\n"
     ]
    }
   ],
   "source": [
    "# Bayesian Optimization\n",
    "best_target, best_config = bayesian_optimization(\n",
    "    get_dcnn_model, input_train, y_train, input_val, y_val, \n",
    "    bayesian_optimization_spaces=hparams_spaces, TARGET=TARGET, N_TRIALS=50, EPOCHS=25, PATIENCE=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "id": "a4fbac69-80d3-415d-9ea4-5a5697f7bb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = best_config\n",
    "\n",
    "# set new intervals for fine-tune random search\n",
    "lr = hparams['learning_rate']\n",
    "n_filters = hparams['n_filters']\n",
    "h_dim = hparams['h_dim']\n",
    "dropout = hparams['dropout']\n",
    "reg = hparams['reg']\n",
    "batch_size = hparams['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "id": "325d2528-0979-4deb-8871-f7554ec31646",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.2\n",
    "random_search_spaces_finetune = {\n",
    "    'learning_rate': ([10 ** (np.log10(lr) - epsilon), 10 ** (np.log10(lr) + epsilon)], 'float'),\n",
    "    'n_filters': ([n_filters], 'item'),\n",
    "    'h_dim': ([h_dim], 'item'),\n",
    "    'dropout': ([10 ** (np.log10(dropout) - epsilon), 10 ** (np.log10(dropout) + epsilon)], 'float'),\n",
    "    'reg': ([10 ** (np.log10(reg) - epsilon), 10 ** (np.log10(reg) + epsilon)], 'float'),\n",
    "    'batch_size': ([batch_size], 'item'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "id": "ada58c9a-c4c0-4333-87e3-4fd7e01a7f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #1 [of 30]:\n",
      " {'learning_rate': 0.0002678869663256501, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.02450776800407293, 'reg': 0.0008920927760349287, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 0.7203 - avg_f1: 0.4662 - val_loss: 0.6460 - val_avg_f1: 0.4036\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6484 - avg_f1: 0.5140 - val_loss: 0.5667 - val_avg_f1: 0.5008\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.5876 - avg_f1: 0.5947 - val_loss: 0.5034 - val_avg_f1: 0.7423\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5438 - avg_f1: 0.6557 - val_loss: 0.4542 - val_avg_f1: 0.7548\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.5057 - avg_f1: 0.7059 - val_loss: 0.4291 - val_avg_f1: 0.7609\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.4804 - avg_f1: 0.7137 - val_loss: 0.4184 - val_avg_f1: 0.7815\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.4688 - avg_f1: 0.7222 - val_loss: 0.4127 - val_avg_f1: 0.7932\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4659 - avg_f1: 0.7371 - val_loss: 0.4107 - val_avg_f1: 0.8022\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4505 - avg_f1: 0.7419 - val_loss: 0.4158 - val_avg_f1: 0.7924\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4458 - avg_f1: 0.7554 - val_loss: 0.4056 - val_avg_f1: 0.7934\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.4248 - avg_f1: 0.7659 - val_loss: 0.4030 - val_avg_f1: 0.7883\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.4150 - avg_f1: 0.7807 - val_loss: 0.4133 - val_avg_f1: 0.7538\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.4000 - avg_f1: 0.7764 - val_loss: 0.4013 - val_avg_f1: 0.7883\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "Epoch 13: early stopping.\n",
      "Best validation macro F1-score: 0.8022390604019165\n",
      "\n",
      "Evaluating Config #2 [of 30]:\n",
      " {'learning_rate': 0.00043508070306619263, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.030445034669431575, 'reg': 0.0016306323710381162, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 0.7455 - avg_f1: 0.4813 - val_loss: 0.6229 - val_avg_f1: 0.4036\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6262 - avg_f1: 0.5804 - val_loss: 0.5230 - val_avg_f1: 0.6352\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.5634 - avg_f1: 0.6558 - val_loss: 0.4512 - val_avg_f1: 0.7455\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5340 - avg_f1: 0.6907 - val_loss: 0.4316 - val_avg_f1: 0.7756\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.4998 - avg_f1: 0.7274 - val_loss: 0.4238 - val_avg_f1: 0.7592\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.4635 - avg_f1: 0.7413 - val_loss: 0.4190 - val_avg_f1: 0.8006\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.4438 - avg_f1: 0.7658 - val_loss: 0.4284 - val_avg_f1: 0.7496\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4317 - avg_f1: 0.7731 - val_loss: 0.4180 - val_avg_f1: 0.7600\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4135 - avg_f1: 0.7849 - val_loss: 0.4212 - val_avg_f1: 0.8049\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4096 - avg_f1: 0.7807 - val_loss: 0.4145 - val_avg_f1: 0.7718\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.3786 - avg_f1: 0.8130 - val_loss: 0.4122 - val_avg_f1: 0.8052\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.3679 - avg_f1: 0.8170 - val_loss: 0.4261 - val_avg_f1: 0.7849\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.3474 - avg_f1: 0.8341 - val_loss: 0.4183 - val_avg_f1: 0.7880\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.3301 - avg_f1: 0.8332 - val_loss: 0.4243 - val_avg_f1: 0.7759\n",
      "\n",
      "\n",
      "Epoch 15/25 - loss: 0.3155 - avg_f1: 0.8520 - val_loss: 0.4243 - val_avg_f1: 0.7668\n",
      "\n",
      "\n",
      "Epoch 16/25 - loss: 0.2937 - avg_f1: 0.8719 - val_loss: 0.4296 - val_avg_f1: 0.7793\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "\n",
      "Epoch 16: early stopping.\n",
      "Best validation macro F1-score: 0.8051816821098328\n",
      "\n",
      "Evaluating Config #3 [of 30]:\n",
      " {'learning_rate': 0.0003869618428771639, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.026885566321670537, 'reg': 0.0009433315979562756, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 0.7593 - avg_f1: 0.4694 - val_loss: 0.6365 - val_avg_f1: 0.4613\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6320 - avg_f1: 0.5198 - val_loss: 0.5159 - val_avg_f1: 0.5424\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.5374 - avg_f1: 0.6424 - val_loss: 0.4572 - val_avg_f1: 0.6877\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5080 - avg_f1: 0.7053 - val_loss: 0.4315 - val_avg_f1: 0.7467\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.4940 - avg_f1: 0.7128 - val_loss: 0.4225 - val_avg_f1: 0.7668\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.4632 - avg_f1: 0.7297 - val_loss: 0.4179 - val_avg_f1: 0.7786\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.4499 - avg_f1: 0.7414 - val_loss: 0.4118 - val_avg_f1: 0.7889\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4361 - avg_f1: 0.7589 - val_loss: 0.4263 - val_avg_f1: 0.7923\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4360 - avg_f1: 0.7581 - val_loss: 0.4151 - val_avg_f1: 0.7923\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4173 - avg_f1: 0.7701 - val_loss: 0.4100 - val_avg_f1: 0.7875\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.4004 - avg_f1: 0.7824 - val_loss: 0.4228 - val_avg_f1: 0.7495\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.3874 - avg_f1: 0.7959 - val_loss: 0.4118 - val_avg_f1: 0.7838\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.3730 - avg_f1: 0.7964 - val_loss: 0.4132 - val_avg_f1: 0.7812\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.3698 - avg_f1: 0.8090 - val_loss: 0.4086 - val_avg_f1: 0.7899\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "\n",
      "Epoch 14: early stopping.\n",
      "Best validation macro F1-score: 0.7923429608345032\n",
      "\n",
      "Evaluating Config #4 [of 30]:\n",
      " {'learning_rate': 0.0005467692069492794, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.014331542150721965, 'reg': 0.0017956343796277662, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 0.6640 - avg_f1: 0.4619 - val_loss: 0.5716 - val_avg_f1: 0.5369\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.5616 - avg_f1: 0.6382 - val_loss: 0.4664 - val_avg_f1: 0.7519\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.5083 - avg_f1: 0.7082 - val_loss: 0.4309 - val_avg_f1: 0.7720\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.4839 - avg_f1: 0.7324 - val_loss: 0.4267 - val_avg_f1: 0.7786\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.4551 - avg_f1: 0.7513 - val_loss: 0.4182 - val_avg_f1: 0.7825\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.4377 - avg_f1: 0.7659 - val_loss: 0.4120 - val_avg_f1: 0.7992\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.4155 - avg_f1: 0.7793 - val_loss: 0.4227 - val_avg_f1: 0.7554\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4097 - avg_f1: 0.7885 - val_loss: 0.4236 - val_avg_f1: 0.7535\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.3788 - avg_f1: 0.7990 - val_loss: 0.4212 - val_avg_f1: 0.7548\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.3693 - avg_f1: 0.8128 - val_loss: 0.4151 - val_avg_f1: 0.7891\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.3596 - avg_f1: 0.8152 - val_loss: 0.4202 - val_avg_f1: 0.8022\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.3305 - avg_f1: 0.8423 - val_loss: 0.4185 - val_avg_f1: 0.7841\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.3099 - avg_f1: 0.8589 - val_loss: 0.4273 - val_avg_f1: 0.7661\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.2920 - avg_f1: 0.8667 - val_loss: 0.4450 - val_avg_f1: 0.7905\n",
      "\n",
      "\n",
      "Epoch 15/25 - loss: 0.2827 - avg_f1: 0.8745 - val_loss: 0.4284 - val_avg_f1: 0.7800\n",
      "\n",
      "\n",
      "Epoch 16/25 - loss: 0.2629 - avg_f1: 0.8933 - val_loss: 0.4333 - val_avg_f1: 0.7610\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "\n",
      "Epoch 16: early stopping.\n",
      "Best validation macro F1-score: 0.8022474050521851\n",
      "\n",
      "Evaluating Config #5 [of 30]:\n",
      " {'learning_rate': 0.00035183097090823665, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.03190960150410254, 'reg': 0.0009194190312555929, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 0.7309 - avg_f1: 0.5040 - val_loss: 0.6056 - val_avg_f1: 0.4402\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6679 - avg_f1: 0.5359 - val_loss: 0.5313 - val_avg_f1: 0.5785\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.5988 - avg_f1: 0.6181 - val_loss: 0.4719 - val_avg_f1: 0.6816\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5315 - avg_f1: 0.6795 - val_loss: 0.4402 - val_avg_f1: 0.7144\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.5150 - avg_f1: 0.6944 - val_loss: 0.4191 - val_avg_f1: 0.7621\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.4922 - avg_f1: 0.7127 - val_loss: 0.4102 - val_avg_f1: 0.7754\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.4755 - avg_f1: 0.7283 - val_loss: 0.4291 - val_avg_f1: 0.7340\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4523 - avg_f1: 0.7437 - val_loss: 0.4051 - val_avg_f1: 0.7713\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4326 - avg_f1: 0.7727 - val_loss: 0.4030 - val_avg_f1: 0.7801\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4183 - avg_f1: 0.7710 - val_loss: 0.4087 - val_avg_f1: 0.7955\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.4056 - avg_f1: 0.7921 - val_loss: 0.4169 - val_avg_f1: 0.7607\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.3936 - avg_f1: 0.7953 - val_loss: 0.4087 - val_avg_f1: 0.7709\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.3805 - avg_f1: 0.8087 - val_loss: 0.4083 - val_avg_f1: 0.7801\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.3649 - avg_f1: 0.8019 - val_loss: 0.4060 - val_avg_f1: 0.7973\n",
      "\n",
      "\n",
      "Epoch 15/25 - loss: 0.3560 - avg_f1: 0.8214 - val_loss: 0.4064 - val_avg_f1: 0.7985\n",
      "\n",
      "\n",
      "Epoch 16/25 - loss: 0.3441 - avg_f1: 0.8269 - val_loss: 0.4323 - val_avg_f1: 0.7376\n",
      "\n",
      "\n",
      "Epoch 17/25 - loss: 0.3399 - avg_f1: 0.8216 - val_loss: 0.4101 - val_avg_f1: 0.7993\n",
      "\n",
      "\n",
      "Epoch 18/25 - loss: 0.3184 - avg_f1: 0.8441 - val_loss: 0.4092 - val_avg_f1: 0.8048\n",
      "\n",
      "\n",
      "Epoch 19/25 - loss: 0.3090 - avg_f1: 0.8508 - val_loss: 0.4171 - val_avg_f1: 0.7953\n",
      "\n",
      "\n",
      "Epoch 20/25 - loss: 0.3042 - avg_f1: 0.8598 - val_loss: 0.4176 - val_avg_f1: 0.7845\n",
      "\n",
      "\n",
      "Epoch 21/25 - loss: 0.2906 - avg_f1: 0.8613 - val_loss: 0.4208 - val_avg_f1: 0.7878\n",
      "\n",
      "\n",
      "Epoch 22/25 - loss: 0.2751 - avg_f1: 0.8777 - val_loss: 0.4206 - val_avg_f1: 0.7956\n",
      "\n",
      "\n",
      "Epoch 23/25 - loss: 0.2723 - avg_f1: 0.8753 - val_loss: 0.4450 - val_avg_f1: 0.7580\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 18.\n",
      "\n",
      "Epoch 23: early stopping.\n",
      "Best validation macro F1-score: 0.8047963380813599\n",
      "\n",
      "Evaluating Config #6 [of 30]:\n",
      " {'learning_rate': 0.0005592802643070267, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.035093027534258185, 'reg': 0.0009078378290921233, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 1.3613 - avg_f1: 0.3775 - val_loss: 0.8648 - val_avg_f1: 0.4036\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6948 - avg_f1: 0.5190 - val_loss: 0.5873 - val_avg_f1: 0.6922\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.6025 - avg_f1: 0.5298 - val_loss: 0.5352 - val_avg_f1: 0.5053\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5705 - avg_f1: 0.6157 - val_loss: 0.4890 - val_avg_f1: 0.6925\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.5373 - avg_f1: 0.6448 - val_loss: 0.4619 - val_avg_f1: 0.7562\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.5126 - avg_f1: 0.6800 - val_loss: 0.4457 - val_avg_f1: 0.7712\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.5010 - avg_f1: 0.7032 - val_loss: 0.4377 - val_avg_f1: 0.7579\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4781 - avg_f1: 0.7273 - val_loss: 0.4283 - val_avg_f1: 0.7929\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4742 - avg_f1: 0.7286 - val_loss: 0.4238 - val_avg_f1: 0.7835\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4518 - avg_f1: 0.7400 - val_loss: 0.4175 - val_avg_f1: 0.7875\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.4308 - avg_f1: 0.7594 - val_loss: 0.4198 - val_avg_f1: 0.7616\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.4231 - avg_f1: 0.7608 - val_loss: 0.4117 - val_avg_f1: 0.7951\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.4050 - avg_f1: 0.7801 - val_loss: 0.4090 - val_avg_f1: 0.7930\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.3914 - avg_f1: 0.7925 - val_loss: 0.4088 - val_avg_f1: 0.7853\n",
      "\n",
      "\n",
      "Epoch 15/25 - loss: 0.3910 - avg_f1: 0.7836 - val_loss: 0.4204 - val_avg_f1: 0.7521\n",
      "\n",
      "\n",
      "Epoch 16/25 - loss: 0.3692 - avg_f1: 0.8062 - val_loss: 0.4106 - val_avg_f1: 0.7915\n",
      "\n",
      "\n",
      "Epoch 17/25 - loss: 0.3538 - avg_f1: 0.8288 - val_loss: 0.4130 - val_avg_f1: 0.7803\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "\n",
      "Epoch 17: early stopping.\n",
      "Best validation macro F1-score: 0.7951231002807617\n",
      "\n",
      "Evaluating Config #7 [of 30]:\n",
      " {'learning_rate': 0.00039272352885827216, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.03390590679026416, 'reg': 0.0011391858031321183, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 0.6970 - avg_f1: 0.4869 - val_loss: 0.6113 - val_avg_f1: 0.4184\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6251 - avg_f1: 0.5527 - val_loss: 0.5262 - val_avg_f1: 0.6116\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.5419 - avg_f1: 0.6532 - val_loss: 0.4579 - val_avg_f1: 0.7150\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5019 - avg_f1: 0.7057 - val_loss: 0.4331 - val_avg_f1: 0.7405\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.4794 - avg_f1: 0.7361 - val_loss: 0.4210 - val_avg_f1: 0.7660\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.4623 - avg_f1: 0.7273 - val_loss: 0.4148 - val_avg_f1: 0.8006\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.4501 - avg_f1: 0.7472 - val_loss: 0.4134 - val_avg_f1: 0.7656\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4331 - avg_f1: 0.7553 - val_loss: 0.4101 - val_avg_f1: 0.7796\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4267 - avg_f1: 0.7622 - val_loss: 0.4179 - val_avg_f1: 0.7628\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4214 - avg_f1: 0.7732 - val_loss: 0.4173 - val_avg_f1: 0.8013\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.4055 - avg_f1: 0.7781 - val_loss: 0.4059 - val_avg_f1: 0.7879\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.3820 - avg_f1: 0.8029 - val_loss: 0.4045 - val_avg_f1: 0.7924\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.3656 - avg_f1: 0.8073 - val_loss: 0.4177 - val_avg_f1: 0.7625\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.3599 - avg_f1: 0.8163 - val_loss: 0.4122 - val_avg_f1: 0.7859\n",
      "\n",
      "\n",
      "Epoch 15/25 - loss: 0.3484 - avg_f1: 0.8204 - val_loss: 0.4129 - val_avg_f1: 0.7937\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\n",
      "Epoch 15: early stopping.\n",
      "Best validation macro F1-score: 0.80125892162323\n",
      "\n",
      "Evaluating Config #8 [of 30]:\n",
      " {'learning_rate': 0.00043236811383413345, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.03318338736519438, 'reg': 0.001260820019603779, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 1.0351 - avg_f1: 0.4109 - val_loss: 0.6965 - val_avg_f1: 0.4036\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6986 - avg_f1: 0.5302 - val_loss: 0.5690 - val_avg_f1: 0.5411\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.6332 - avg_f1: 0.5571 - val_loss: 0.5213 - val_avg_f1: 0.6622\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5940 - avg_f1: 0.6121 - val_loss: 0.4827 - val_avg_f1: 0.7062\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.5588 - avg_f1: 0.6639 - val_loss: 0.4535 - val_avg_f1: 0.7645\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.5367 - avg_f1: 0.6934 - val_loss: 0.4424 - val_avg_f1: 0.7999\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.5152 - avg_f1: 0.7103 - val_loss: 0.4283 - val_avg_f1: 0.8016\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4919 - avg_f1: 0.7309 - val_loss: 0.4219 - val_avg_f1: 0.7836\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4846 - avg_f1: 0.7311 - val_loss: 0.4143 - val_avg_f1: 0.8069\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4591 - avg_f1: 0.7502 - val_loss: 0.4114 - val_avg_f1: 0.7956\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.4410 - avg_f1: 0.7555 - val_loss: 0.4129 - val_avg_f1: 0.7804\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.4287 - avg_f1: 0.7756 - val_loss: 0.4052 - val_avg_f1: 0.8073\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.4170 - avg_f1: 0.7691 - val_loss: 0.4361 - val_avg_f1: 0.7218\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.4058 - avg_f1: 0.7747 - val_loss: 0.4184 - val_avg_f1: 0.7522\n",
      "\n",
      "\n",
      "Epoch 15/25 - loss: 0.3821 - avg_f1: 0.7939 - val_loss: 0.4269 - val_avg_f1: 0.7514\n",
      "\n",
      "\n",
      "Epoch 16/25 - loss: 0.3672 - avg_f1: 0.8164 - val_loss: 0.4145 - val_avg_f1: 0.7961\n",
      "\n",
      "\n",
      "Epoch 17/25 - loss: 0.3529 - avg_f1: 0.8270 - val_loss: 0.4180 - val_avg_f1: 0.7887\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "\n",
      "Epoch 17: early stopping.\n",
      "Best validation macro F1-score: 0.8073251843452454\n",
      "\n",
      "Evaluating Config #9 [of 30]:\n",
      " {'learning_rate': 0.0004088172291040285, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.016206147780837593, 'reg': 0.0017951054992052982, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 0.6870 - avg_f1: 0.4818 - val_loss: 0.6160 - val_avg_f1: 0.4969\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6152 - avg_f1: 0.5778 - val_loss: 0.5217 - val_avg_f1: 0.6188\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.5439 - avg_f1: 0.6516 - val_loss: 0.4572 - val_avg_f1: 0.7319\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5086 - avg_f1: 0.6989 - val_loss: 0.4415 - val_avg_f1: 0.7871\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.4857 - avg_f1: 0.7269 - val_loss: 0.4283 - val_avg_f1: 0.7475\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.4585 - avg_f1: 0.7467 - val_loss: 0.4196 - val_avg_f1: 0.7681\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.4470 - avg_f1: 0.7528 - val_loss: 0.4121 - val_avg_f1: 0.7924\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4311 - avg_f1: 0.7714 - val_loss: 0.4143 - val_avg_f1: 0.8090\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4080 - avg_f1: 0.7850 - val_loss: 0.4292 - val_avg_f1: 0.7334\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4065 - avg_f1: 0.7781 - val_loss: 0.4118 - val_avg_f1: 0.8047\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.3936 - avg_f1: 0.7860 - val_loss: 0.4187 - val_avg_f1: 0.7805\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.3779 - avg_f1: 0.8067 - val_loss: 0.4148 - val_avg_f1: 0.7997\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.3640 - avg_f1: 0.8115 - val_loss: 0.4131 - val_avg_f1: 0.7976\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "Epoch 13: early stopping.\n",
      "Best validation macro F1-score: 0.8090313673019409\n",
      "\n",
      "Evaluating Config #10 [of 30]:\n",
      " {'learning_rate': 0.0002670267757545043, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.017946689891150808, 'reg': 0.0011034796075773692, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 2.0871 - avg_f1: 0.4132 - val_loss: 1.1522 - val_avg_f1: 0.4036\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.8383 - avg_f1: 0.4524 - val_loss: 0.6755 - val_avg_f1: 0.5427\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.6875 - avg_f1: 0.4867 - val_loss: 0.6112 - val_avg_f1: 0.4897\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.6296 - avg_f1: 0.5885 - val_loss: 0.5674 - val_avg_f1: 0.5920\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.5986 - avg_f1: 0.6052 - val_loss: 0.5321 - val_avg_f1: 0.6549\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.5722 - avg_f1: 0.6403 - val_loss: 0.5009 - val_avg_f1: 0.6921\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.5531 - avg_f1: 0.6573 - val_loss: 0.4814 - val_avg_f1: 0.6909\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.5299 - avg_f1: 0.6802 - val_loss: 0.4568 - val_avg_f1: 0.7323\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.5129 - avg_f1: 0.7022 - val_loss: 0.4455 - val_avg_f1: 0.7484\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4962 - avg_f1: 0.7122 - val_loss: 0.4353 - val_avg_f1: 0.7558\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.4757 - avg_f1: 0.7367 - val_loss: 0.4275 - val_avg_f1: 0.7620\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.4840 - avg_f1: 0.7281 - val_loss: 0.4221 - val_avg_f1: 0.7796\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.4624 - avg_f1: 0.7387 - val_loss: 0.4170 - val_avg_f1: 0.7879\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.4457 - avg_f1: 0.7631 - val_loss: 0.4170 - val_avg_f1: 0.7614\n",
      "\n",
      "\n",
      "Epoch 15/25 - loss: 0.4428 - avg_f1: 0.7503 - val_loss: 0.4114 - val_avg_f1: 0.7914\n",
      "\n",
      "\n",
      "Epoch 16/25 - loss: 0.4307 - avg_f1: 0.7568 - val_loss: 0.4106 - val_avg_f1: 0.7991\n",
      "\n",
      "\n",
      "Epoch 17/25 - loss: 0.4228 - avg_f1: 0.7807 - val_loss: 0.4082 - val_avg_f1: 0.7936\n",
      "\n",
      "\n",
      "Epoch 18/25 - loss: 0.4219 - avg_f1: 0.7715 - val_loss: 0.4087 - val_avg_f1: 0.8015\n",
      "\n",
      "\n",
      "Epoch 19/25 - loss: 0.4132 - avg_f1: 0.7927 - val_loss: 0.4074 - val_avg_f1: 0.7853\n",
      "\n",
      "\n",
      "Epoch 20/25 - loss: 0.4012 - avg_f1: 0.7893 - val_loss: 0.4098 - val_avg_f1: 0.7812\n",
      "\n",
      "\n",
      "Epoch 21/25 - loss: 0.3935 - avg_f1: 0.7921 - val_loss: 0.4072 - val_avg_f1: 0.7895\n",
      "\n",
      "\n",
      "Epoch 22/25 - loss: 0.3932 - avg_f1: 0.7959 - val_loss: 0.4069 - val_avg_f1: 0.7839\n",
      "\n",
      "\n",
      "Epoch 23/25 - loss: 0.3826 - avg_f1: 0.8060 - val_loss: 0.4066 - val_avg_f1: 0.7958\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 18.\n",
      "\n",
      "Epoch 23: early stopping.\n",
      "Best validation macro F1-score: 0.8015251159667969\n",
      "\n",
      "Evaluating Config #11 [of 30]:\n",
      " {'learning_rate': 0.00038510302367219206, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.0312915274877653, 'reg': 0.0009681775892455978, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 0.7707 - avg_f1: 0.4230 - val_loss: 0.6342 - val_avg_f1: 0.4036\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6572 - avg_f1: 0.5094 - val_loss: 0.5824 - val_avg_f1: 0.4424\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.6025 - avg_f1: 0.5429 - val_loss: 0.5234 - val_avg_f1: 0.6364\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5565 - avg_f1: 0.6171 - val_loss: 0.4694 - val_avg_f1: 0.7337\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.5228 - avg_f1: 0.6954 - val_loss: 0.4590 - val_avg_f1: 0.6945\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.4917 - avg_f1: 0.7136 - val_loss: 0.4310 - val_avg_f1: 0.7448\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.4817 - avg_f1: 0.7284 - val_loss: 0.4345 - val_avg_f1: 0.7145\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4664 - avg_f1: 0.7350 - val_loss: 0.4203 - val_avg_f1: 0.7578\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4431 - avg_f1: 0.7513 - val_loss: 0.4121 - val_avg_f1: 0.7766\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4354 - avg_f1: 0.7577 - val_loss: 0.4102 - val_avg_f1: 0.8013\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.4186 - avg_f1: 0.7639 - val_loss: 0.4055 - val_avg_f1: 0.8010\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.4109 - avg_f1: 0.7823 - val_loss: 0.4220 - val_avg_f1: 0.7454\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.3978 - avg_f1: 0.7795 - val_loss: 0.4079 - val_avg_f1: 0.7984\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.3683 - avg_f1: 0.8101 - val_loss: 0.4101 - val_avg_f1: 0.7841\n",
      "\n",
      "\n",
      "Epoch 15/25 - loss: 0.3608 - avg_f1: 0.8085 - val_loss: 0.4125 - val_avg_f1: 0.8055\n",
      "\n",
      "\n",
      "Epoch 16/25 - loss: 0.3499 - avg_f1: 0.8231 - val_loss: 0.4152 - val_avg_f1: 0.7743\n",
      "\n",
      "\n",
      "Epoch 17/25 - loss: 0.3361 - avg_f1: 0.8345 - val_loss: 0.4175 - val_avg_f1: 0.7944\n",
      "\n",
      "\n",
      "Epoch 18/25 - loss: 0.3309 - avg_f1: 0.8351 - val_loss: 0.4289 - val_avg_f1: 0.7533\n",
      "\n",
      "\n",
      "Epoch 19/25 - loss: 0.3117 - avg_f1: 0.8510 - val_loss: 0.4177 - val_avg_f1: 0.7993\n",
      "\n",
      "\n",
      "Epoch 20/25 - loss: 0.2990 - avg_f1: 0.8643 - val_loss: 0.4280 - val_avg_f1: 0.7639\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "\n",
      "Epoch 20: early stopping.\n",
      "Best validation macro F1-score: 0.8055070638656616\n",
      "\n",
      "Evaluating Config #12 [of 30]:\n",
      " {'learning_rate': 0.00031323519327140065, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.024605971342657212, 'reg': 0.0009855032956958907, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 0.7715 - avg_f1: 0.4757 - val_loss: 0.6677 - val_avg_f1: 0.5087\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6574 - avg_f1: 0.4874 - val_loss: 0.5953 - val_avg_f1: 0.5043\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.6028 - avg_f1: 0.5649 - val_loss: 0.5390 - val_avg_f1: 0.5936\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5505 - avg_f1: 0.6400 - val_loss: 0.4844 - val_avg_f1: 0.7371\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.5186 - avg_f1: 0.6857 - val_loss: 0.4450 - val_avg_f1: 0.7515\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.4913 - avg_f1: 0.7181 - val_loss: 0.4282 - val_avg_f1: 0.7641\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.4781 - avg_f1: 0.7227 - val_loss: 0.4185 - val_avg_f1: 0.7766\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4587 - avg_f1: 0.7341 - val_loss: 0.4290 - val_avg_f1: 0.7107\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4398 - avg_f1: 0.7553 - val_loss: 0.4106 - val_avg_f1: 0.7780\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4298 - avg_f1: 0.7695 - val_loss: 0.4070 - val_avg_f1: 0.7974\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.4156 - avg_f1: 0.7806 - val_loss: 0.4130 - val_avg_f1: 0.7592\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.4080 - avg_f1: 0.7821 - val_loss: 0.4045 - val_avg_f1: 0.8023\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.3982 - avg_f1: 0.7833 - val_loss: 0.4079 - val_avg_f1: 0.8016\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.3846 - avg_f1: 0.7974 - val_loss: 0.4097 - val_avg_f1: 0.7997\n",
      "\n",
      "\n",
      "Epoch 15/25 - loss: 0.3756 - avg_f1: 0.8107 - val_loss: 0.4344 - val_avg_f1: 0.7344\n",
      "\n",
      "\n",
      "Epoch 16/25 - loss: 0.3649 - avg_f1: 0.8028 - val_loss: 0.4149 - val_avg_f1: 0.7764\n",
      "\n",
      "\n",
      "Epoch 17/25 - loss: 0.3509 - avg_f1: 0.8272 - val_loss: 0.4153 - val_avg_f1: 0.7857\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "\n",
      "Epoch 17: early stopping.\n",
      "Best validation macro F1-score: 0.8022834062576294\n",
      "\n",
      "Evaluating Config #13 [of 30]:\n",
      " {'learning_rate': 0.0003636818188977324, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.016694643382180623, 'reg': 0.0010502914096482533, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 1.4034 - avg_f1: 0.3570 - val_loss: 0.8791 - val_avg_f1: 0.4036\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.7498 - avg_f1: 0.4413 - val_loss: 0.6141 - val_avg_f1: 0.6775\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.6238 - avg_f1: 0.5591 - val_loss: 0.5539 - val_avg_f1: 0.4232\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5852 - avg_f1: 0.5468 - val_loss: 0.5027 - val_avg_f1: 0.5681\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.5530 - avg_f1: 0.5954 - val_loss: 0.4688 - val_avg_f1: 0.7703\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.5237 - avg_f1: 0.6657 - val_loss: 0.4476 - val_avg_f1: 0.7848\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.5018 - avg_f1: 0.6918 - val_loss: 0.4333 - val_avg_f1: 0.7836\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4897 - avg_f1: 0.7194 - val_loss: 0.4324 - val_avg_f1: 0.7435\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4740 - avg_f1: 0.7195 - val_loss: 0.4220 - val_avg_f1: 0.7839\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4718 - avg_f1: 0.7278 - val_loss: 0.4159 - val_avg_f1: 0.8002\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.4594 - avg_f1: 0.7413 - val_loss: 0.4111 - val_avg_f1: 0.7919\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.4471 - avg_f1: 0.7534 - val_loss: 0.4100 - val_avg_f1: 0.7766\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.4378 - avg_f1: 0.7621 - val_loss: 0.4081 - val_avg_f1: 0.7860\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.4270 - avg_f1: 0.7677 - val_loss: 0.4166 - val_avg_f1: 0.7586\n",
      "\n",
      "\n",
      "Epoch 15/25 - loss: 0.4161 - avg_f1: 0.7707 - val_loss: 0.4055 - val_avg_f1: 0.7975\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\n",
      "Epoch 15: early stopping.\n",
      "Best validation macro F1-score: 0.8002324104309082\n",
      "\n",
      "Evaluating Config #14 [of 30]:\n",
      " {'learning_rate': 0.0003477391575950414, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.02483736490751836, 'reg': 0.0009500838265455208, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 4.3160 - avg_f1: 0.2678 - val_loss: 1.0648 - val_avg_f1: 0.3432\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.9324 - avg_f1: 0.4349 - val_loss: 0.7008 - val_avg_f1: 0.4031\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.6759 - avg_f1: 0.5792 - val_loss: 0.5462 - val_avg_f1: 0.6745\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.6038 - avg_f1: 0.5985 - val_loss: 0.5192 - val_avg_f1: 0.6276\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.5870 - avg_f1: 0.6211 - val_loss: 0.4930 - val_avg_f1: 0.6816\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.5591 - avg_f1: 0.6495 - val_loss: 0.4761 - val_avg_f1: 0.6915\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.5438 - avg_f1: 0.6581 - val_loss: 0.4607 - val_avg_f1: 0.7354\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.5274 - avg_f1: 0.6873 - val_loss: 0.4529 - val_avg_f1: 0.7137\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.5254 - avg_f1: 0.6850 - val_loss: 0.4496 - val_avg_f1: 0.7062\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4968 - avg_f1: 0.7015 - val_loss: 0.4395 - val_avg_f1: 0.7673\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.5034 - avg_f1: 0.7150 - val_loss: 0.4368 - val_avg_f1: 0.7368\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.4969 - avg_f1: 0.7049 - val_loss: 0.4335 - val_avg_f1: 0.7902\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.4800 - avg_f1: 0.7347 - val_loss: 0.4278 - val_avg_f1: 0.7803\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.4754 - avg_f1: 0.7365 - val_loss: 0.4259 - val_avg_f1: 0.7867\n",
      "\n",
      "\n",
      "Epoch 15/25 - loss: 0.4682 - avg_f1: 0.7257 - val_loss: 0.4347 - val_avg_f1: 0.7217\n",
      "\n",
      "\n",
      "Epoch 16/25 - loss: 0.4574 - avg_f1: 0.7459 - val_loss: 0.4225 - val_avg_f1: 0.7893\n",
      "\n",
      "\n",
      "Epoch 17/25 - loss: 0.4495 - avg_f1: 0.7546 - val_loss: 0.4233 - val_avg_f1: 0.7866\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "\n",
      "Epoch 17: early stopping.\n",
      "Best validation macro F1-score: 0.79017573595047\n",
      "\n",
      "Evaluating Config #15 [of 30]:\n",
      " {'learning_rate': 0.00026374934845319097, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.0316147532064868, 'reg': 0.0012449007865228577, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 1.5827 - avg_f1: 0.3606 - val_loss: 0.6750 - val_avg_f1: 0.4036\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.7293 - avg_f1: 0.4479 - val_loss: 0.5838 - val_avg_f1: 0.5019\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.6251 - avg_f1: 0.6025 - val_loss: 0.5580 - val_avg_f1: 0.4506\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5839 - avg_f1: 0.5873 - val_loss: 0.5263 - val_avg_f1: 0.6116\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.5868 - avg_f1: 0.6068 - val_loss: 0.5019 - val_avg_f1: 0.6382\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.5554 - avg_f1: 0.6336 - val_loss: 0.4821 - val_avg_f1: 0.6976\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.5416 - avg_f1: 0.6591 - val_loss: 0.4704 - val_avg_f1: 0.6944\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.5275 - avg_f1: 0.6815 - val_loss: 0.4573 - val_avg_f1: 0.7505\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.5140 - avg_f1: 0.6812 - val_loss: 0.4488 - val_avg_f1: 0.7595\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.5172 - avg_f1: 0.6921 - val_loss: 0.4445 - val_avg_f1: 0.7652\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.5053 - avg_f1: 0.7153 - val_loss: 0.4394 - val_avg_f1: 0.7645\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.4860 - avg_f1: 0.7108 - val_loss: 0.4342 - val_avg_f1: 0.7752\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.4732 - avg_f1: 0.7447 - val_loss: 0.4358 - val_avg_f1: 0.7497\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.4695 - avg_f1: 0.7422 - val_loss: 0.4322 - val_avg_f1: 0.7610\n",
      "\n",
      "\n",
      "Epoch 15/25 - loss: 0.4605 - avg_f1: 0.7386 - val_loss: 0.4274 - val_avg_f1: 0.7960\n",
      "\n",
      "\n",
      "Epoch 16/25 - loss: 0.4609 - avg_f1: 0.7304 - val_loss: 0.4228 - val_avg_f1: 0.8000\n",
      "\n",
      "\n",
      "Epoch 17/25 - loss: 0.4357 - avg_f1: 0.7680 - val_loss: 0.4231 - val_avg_f1: 0.7897\n",
      "\n",
      "\n",
      "Epoch 18/25 - loss: 0.4380 - avg_f1: 0.7440 - val_loss: 0.4214 - val_avg_f1: 0.7981\n",
      "\n",
      "\n",
      "Epoch 19/25 - loss: 0.4281 - avg_f1: 0.7650 - val_loss: 0.4202 - val_avg_f1: 0.8075\n",
      "\n",
      "\n",
      "Epoch 20/25 - loss: 0.4251 - avg_f1: 0.7742 - val_loss: 0.4207 - val_avg_f1: 0.7955\n",
      "\n",
      "\n",
      "Epoch 21/25 - loss: 0.4193 - avg_f1: 0.7798 - val_loss: 0.4330 - val_avg_f1: 0.7442\n",
      "\n",
      "\n",
      "Epoch 22/25 - loss: 0.4098 - avg_f1: 0.7879 - val_loss: 0.4214 - val_avg_f1: 0.7960\n",
      "\n",
      "\n",
      "Epoch 23/25 - loss: 0.3958 - avg_f1: 0.7887 - val_loss: 0.4391 - val_avg_f1: 0.7361\n",
      "\n",
      "\n",
      "Epoch 24/25 - loss: 0.3932 - avg_f1: 0.7904 - val_loss: 0.4249 - val_avg_f1: 0.7939\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "\n",
      "Epoch 24: early stopping.\n",
      "Best validation macro F1-score: 0.8075402975082397\n",
      "\n",
      "Evaluating Config #16 [of 30]:\n",
      " {'learning_rate': 0.0003780310553250499, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.02961811167427769, 'reg': 0.0010456933039266316, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 0.8310 - avg_f1: 0.4229 - val_loss: 0.6368 - val_avg_f1: 0.4036\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6608 - avg_f1: 0.5186 - val_loss: 0.5786 - val_avg_f1: 0.4624\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.6104 - avg_f1: 0.5273 - val_loss: 0.5319 - val_avg_f1: 0.7076\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5768 - avg_f1: 0.6213 - val_loss: 0.4793 - val_avg_f1: 0.7284\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.5410 - avg_f1: 0.6670 - val_loss: 0.4504 - val_avg_f1: 0.7529\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.5062 - avg_f1: 0.7010 - val_loss: 0.4335 - val_avg_f1: 0.7791\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.4894 - avg_f1: 0.7192 - val_loss: 0.4231 - val_avg_f1: 0.7914\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4656 - avg_f1: 0.7402 - val_loss: 0.4163 - val_avg_f1: 0.7995\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4630 - avg_f1: 0.7441 - val_loss: 0.4199 - val_avg_f1: 0.8008\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4409 - avg_f1: 0.7651 - val_loss: 0.4116 - val_avg_f1: 0.8102\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.4188 - avg_f1: 0.7735 - val_loss: 0.4099 - val_avg_f1: 0.7783\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.4126 - avg_f1: 0.7776 - val_loss: 0.4055 - val_avg_f1: 0.8003\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.3913 - avg_f1: 0.7944 - val_loss: 0.4233 - val_avg_f1: 0.7592\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.3755 - avg_f1: 0.8026 - val_loss: 0.4124 - val_avg_f1: 0.7864\n",
      "\n",
      "\n",
      "Epoch 15/25 - loss: 0.3623 - avg_f1: 0.8215 - val_loss: 0.4094 - val_avg_f1: 0.8044\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\n",
      "Epoch 15: early stopping.\n",
      "Best validation macro F1-score: 0.8102418780326843\n",
      "\n",
      "Evaluating Config #17 [of 30]:\n",
      " {'learning_rate': 0.0005126165126425172, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.031942970752597825, 'reg': 0.0011278325020739129, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 0.7816 - avg_f1: 0.4569 - val_loss: 0.6135 - val_avg_f1: 0.6161\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6576 - avg_f1: 0.5567 - val_loss: 0.5701 - val_avg_f1: 0.4576\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.5785 - avg_f1: 0.6157 - val_loss: 0.5047 - val_avg_f1: 0.6107\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5503 - avg_f1: 0.6515 - val_loss: 0.4483 - val_avg_f1: 0.7432\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.4984 - avg_f1: 0.7140 - val_loss: 0.4305 - val_avg_f1: 0.7566\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.4794 - avg_f1: 0.7313 - val_loss: 0.4202 - val_avg_f1: 0.7720\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.4667 - avg_f1: 0.7437 - val_loss: 0.4188 - val_avg_f1: 0.7609\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4446 - avg_f1: 0.7482 - val_loss: 0.4100 - val_avg_f1: 0.7968\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4337 - avg_f1: 0.7660 - val_loss: 0.4279 - val_avg_f1: 0.7387\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.3958 - avg_f1: 0.7866 - val_loss: 0.4170 - val_avg_f1: 0.7721\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.3951 - avg_f1: 0.7979 - val_loss: 0.4188 - val_avg_f1: 0.8006\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.3694 - avg_f1: 0.8142 - val_loss: 0.4087 - val_avg_f1: 0.7990\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.3513 - avg_f1: 0.8268 - val_loss: 0.4149 - val_avg_f1: 0.8007\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.3299 - avg_f1: 0.8362 - val_loss: 0.4230 - val_avg_f1: 0.7770\n",
      "\n",
      "\n",
      "Epoch 15/25 - loss: 0.3163 - avg_f1: 0.8473 - val_loss: 0.4209 - val_avg_f1: 0.7885\n",
      "\n",
      "\n",
      "Epoch 16/25 - loss: 0.2875 - avg_f1: 0.8745 - val_loss: 0.4269 - val_avg_f1: 0.7813\n",
      "\n",
      "\n",
      "Epoch 17/25 - loss: 0.2772 - avg_f1: 0.8796 - val_loss: 0.4373 - val_avg_f1: 0.7783\n",
      "\n",
      "\n",
      "Epoch 18/25 - loss: 0.2569 - avg_f1: 0.8865 - val_loss: 0.4458 - val_avg_f1: 0.7689\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "\n",
      "Epoch 18: early stopping.\n",
      "Best validation macro F1-score: 0.8006541132926941\n",
      "\n",
      "Evaluating Config #18 [of 30]:\n",
      " {'learning_rate': 0.00046764905296626456, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.031603069951125815, 'reg': 0.0012734440971713021, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 0.7933 - avg_f1: 0.4017 - val_loss: 0.6176 - val_avg_f1: 0.4036\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6406 - avg_f1: 0.5238 - val_loss: 0.5704 - val_avg_f1: 0.4373\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.5730 - avg_f1: 0.5989 - val_loss: 0.5153 - val_avg_f1: 0.5463\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5408 - avg_f1: 0.6663 - val_loss: 0.4777 - val_avg_f1: 0.6655\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.5114 - avg_f1: 0.6902 - val_loss: 0.4411 - val_avg_f1: 0.7553\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.4894 - avg_f1: 0.7132 - val_loss: 0.4305 - val_avg_f1: 0.7616\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.4739 - avg_f1: 0.7186 - val_loss: 0.4235 - val_avg_f1: 0.7779\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4482 - avg_f1: 0.7532 - val_loss: 0.4168 - val_avg_f1: 0.8153\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4392 - avg_f1: 0.7621 - val_loss: 0.4131 - val_avg_f1: 0.8045\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4241 - avg_f1: 0.7705 - val_loss: 0.4081 - val_avg_f1: 0.8129\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.4098 - avg_f1: 0.7805 - val_loss: 0.4122 - val_avg_f1: 0.8087\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.3923 - avg_f1: 0.7937 - val_loss: 0.4223 - val_avg_f1: 0.7759\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.3692 - avg_f1: 0.8059 - val_loss: 0.4167 - val_avg_f1: 0.7835\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "Epoch 13: early stopping.\n",
      "Best validation macro F1-score: 0.8153221011161804\n",
      "\n",
      "Evaluating Config #19 [of 30]:\n",
      " {'learning_rate': 0.0003588460499576948, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.015957361170138747, 'reg': 0.0010141288630349013, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 0.7395 - avg_f1: 0.4097 - val_loss: 0.6337 - val_avg_f1: 0.4036\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6374 - avg_f1: 0.4524 - val_loss: 0.5753 - val_avg_f1: 0.4036\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.5887 - avg_f1: 0.5425 - val_loss: 0.5063 - val_avg_f1: 0.5980\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5377 - avg_f1: 0.6463 - val_loss: 0.4585 - val_avg_f1: 0.7005\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.5008 - avg_f1: 0.7052 - val_loss: 0.4343 - val_avg_f1: 0.7652\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.4809 - avg_f1: 0.7193 - val_loss: 0.4237 - val_avg_f1: 0.7667\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.4628 - avg_f1: 0.7377 - val_loss: 0.4180 - val_avg_f1: 0.8008\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4494 - avg_f1: 0.7560 - val_loss: 0.4190 - val_avg_f1: 0.7418\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4346 - avg_f1: 0.7579 - val_loss: 0.4065 - val_avg_f1: 0.7941\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4230 - avg_f1: 0.7680 - val_loss: 0.4058 - val_avg_f1: 0.8006\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.4128 - avg_f1: 0.7864 - val_loss: 0.4064 - val_avg_f1: 0.7709\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.3934 - avg_f1: 0.7992 - val_loss: 0.4031 - val_avg_f1: 0.7948\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\n",
      "Epoch 12: early stopping.\n",
      "Best validation macro F1-score: 0.8008130192756653\n",
      "\n",
      "Evaluating Config #20 [of 30]:\n",
      " {'learning_rate': 0.0003742734889798584, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.02569359387644939, 'reg': 0.0009484470805644961, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 0.9380 - avg_f1: 0.3845 - val_loss: 0.7031 - val_avg_f1: 0.4036\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6592 - avg_f1: 0.5214 - val_loss: 0.5840 - val_avg_f1: 0.4597\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.6261 - avg_f1: 0.4976 - val_loss: 0.5366 - val_avg_f1: 0.5488\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5789 - avg_f1: 0.6131 - val_loss: 0.5024 - val_avg_f1: 0.5500\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.5379 - avg_f1: 0.6311 - val_loss: 0.4652 - val_avg_f1: 0.6933\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.5139 - avg_f1: 0.6865 - val_loss: 0.4401 - val_avg_f1: 0.7348\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.4947 - avg_f1: 0.7073 - val_loss: 0.4299 - val_avg_f1: 0.7961\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4790 - avg_f1: 0.7261 - val_loss: 0.4172 - val_avg_f1: 0.8039\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4701 - avg_f1: 0.7364 - val_loss: 0.4150 - val_avg_f1: 0.7532\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4528 - avg_f1: 0.7418 - val_loss: 0.4091 - val_avg_f1: 0.7748\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.4262 - avg_f1: 0.7726 - val_loss: 0.4118 - val_avg_f1: 0.7613\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.4176 - avg_f1: 0.7757 - val_loss: 0.4031 - val_avg_f1: 0.7892\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.4093 - avg_f1: 0.7849 - val_loss: 0.4030 - val_avg_f1: 0.7973\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "Epoch 13: early stopping.\n",
      "Best validation macro F1-score: 0.8038784265518188\n",
      "\n",
      "Evaluating Config #21 [of 30]:\n",
      " {'learning_rate': 0.0003706192820179295, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.029470513256713002, 'reg': 0.0010627703330473414, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 0.7460 - avg_f1: 0.4595 - val_loss: 0.6222 - val_avg_f1: 0.4883\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6489 - avg_f1: 0.5297 - val_loss: 0.5528 - val_avg_f1: 0.4867\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.5828 - avg_f1: 0.6193 - val_loss: 0.4787 - val_avg_f1: 0.6989\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5322 - avg_f1: 0.6775 - val_loss: 0.4461 - val_avg_f1: 0.7797\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.5037 - avg_f1: 0.7041 - val_loss: 0.4321 - val_avg_f1: 0.7378\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.4908 - avg_f1: 0.7164 - val_loss: 0.4348 - val_avg_f1: 0.7173\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.4678 - avg_f1: 0.7403 - val_loss: 0.4136 - val_avg_f1: 0.7850\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4454 - avg_f1: 0.7612 - val_loss: 0.4059 - val_avg_f1: 0.8026\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4267 - avg_f1: 0.7578 - val_loss: 0.4171 - val_avg_f1: 0.7452\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4196 - avg_f1: 0.7717 - val_loss: 0.4051 - val_avg_f1: 0.8042\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.4121 - avg_f1: 0.7765 - val_loss: 0.4072 - val_avg_f1: 0.7953\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.3932 - avg_f1: 0.7948 - val_loss: 0.4110 - val_avg_f1: 0.7960\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.3779 - avg_f1: 0.7962 - val_loss: 0.4096 - val_avg_f1: 0.8035\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.3645 - avg_f1: 0.8115 - val_loss: 0.4071 - val_avg_f1: 0.7980\n",
      "\n",
      "\n",
      "Epoch 15/25 - loss: 0.3597 - avg_f1: 0.8181 - val_loss: 0.4103 - val_avg_f1: 0.7994\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\n",
      "Epoch 15: early stopping.\n",
      "Best validation macro F1-score: 0.8041967153549194\n",
      "\n",
      "Evaluating Config #22 [of 30]:\n",
      " {'learning_rate': 0.00028463944117714424, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.03158563034405017, 'reg': 0.0007550933073285856, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 1.4030 - avg_f1: 0.4516 - val_loss: 0.7031 - val_avg_f1: 0.5212\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.7680 - avg_f1: 0.5126 - val_loss: 0.6558 - val_avg_f1: 0.4533\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.6882 - avg_f1: 0.5508 - val_loss: 0.5926 - val_avg_f1: 0.6247\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.6380 - avg_f1: 0.5870 - val_loss: 0.5582 - val_avg_f1: 0.6065\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.6187 - avg_f1: 0.6136 - val_loss: 0.5226 - val_avg_f1: 0.6361\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.5816 - avg_f1: 0.6420 - val_loss: 0.4811 - val_avg_f1: 0.7153\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.5508 - avg_f1: 0.6736 - val_loss: 0.4508 - val_avg_f1: 0.7416\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.5237 - avg_f1: 0.6926 - val_loss: 0.4333 - val_avg_f1: 0.7514\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4959 - avg_f1: 0.7241 - val_loss: 0.4221 - val_avg_f1: 0.7499\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4948 - avg_f1: 0.7183 - val_loss: 0.4136 - val_avg_f1: 0.7579\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.4806 - avg_f1: 0.7259 - val_loss: 0.4096 - val_avg_f1: 0.7934\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.4566 - avg_f1: 0.7421 - val_loss: 0.4056 - val_avg_f1: 0.7944\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.4507 - avg_f1: 0.7553 - val_loss: 0.4042 - val_avg_f1: 0.7846\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.4301 - avg_f1: 0.7745 - val_loss: 0.4075 - val_avg_f1: 0.7666\n",
      "\n",
      "\n",
      "Epoch 15/25 - loss: 0.4207 - avg_f1: 0.7800 - val_loss: 0.4031 - val_avg_f1: 0.7860\n",
      "\n",
      "\n",
      "Epoch 16/25 - loss: 0.4079 - avg_f1: 0.7884 - val_loss: 0.4074 - val_avg_f1: 0.7637\n",
      "\n",
      "\n",
      "Epoch 17/25 - loss: 0.3899 - avg_f1: 0.7994 - val_loss: 0.4042 - val_avg_f1: 0.8017\n",
      "\n",
      "\n",
      "Epoch 18/25 - loss: 0.3901 - avg_f1: 0.8005 - val_loss: 0.4084 - val_avg_f1: 0.7640\n",
      "\n",
      "\n",
      "Epoch 19/25 - loss: 0.3778 - avg_f1: 0.8040 - val_loss: 0.4060 - val_avg_f1: 0.7730\n",
      "\n",
      "\n",
      "Epoch 20/25 - loss: 0.3791 - avg_f1: 0.8055 - val_loss: 0.4086 - val_avg_f1: 0.7753\n",
      "\n",
      "\n",
      "Epoch 21/25 - loss: 0.3623 - avg_f1: 0.8181 - val_loss: 0.4116 - val_avg_f1: 0.7722\n",
      "\n",
      "\n",
      "Epoch 22/25 - loss: 0.3432 - avg_f1: 0.8378 - val_loss: 0.4144 - val_avg_f1: 0.7641\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "\n",
      "Epoch 22: early stopping.\n",
      "Best validation macro F1-score: 0.8016682863235474\n",
      "\n",
      "Evaluating Config #23 [of 30]:\n",
      " {'learning_rate': 0.0003876972776429699, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.027269491388270686, 'reg': 0.001659991052528001, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 0.7627 - avg_f1: 0.4591 - val_loss: 0.6361 - val_avg_f1: 0.4950\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6404 - avg_f1: 0.5434 - val_loss: 0.5459 - val_avg_f1: 0.5915\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.5760 - avg_f1: 0.6166 - val_loss: 0.4772 - val_avg_f1: 0.7318\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5367 - avg_f1: 0.6939 - val_loss: 0.4438 - val_avg_f1: 0.7632\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.5192 - avg_f1: 0.7028 - val_loss: 0.4353 - val_avg_f1: 0.7305\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.5026 - avg_f1: 0.7132 - val_loss: 0.4216 - val_avg_f1: 0.7755\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.4603 - avg_f1: 0.7606 - val_loss: 0.4523 - val_avg_f1: 0.6969\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4656 - avg_f1: 0.7456 - val_loss: 0.4120 - val_avg_f1: 0.7905\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4256 - avg_f1: 0.7785 - val_loss: 0.4148 - val_avg_f1: 0.8083\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4210 - avg_f1: 0.7773 - val_loss: 0.4234 - val_avg_f1: 0.7569\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.4121 - avg_f1: 0.7948 - val_loss: 0.4062 - val_avg_f1: 0.8092\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.3903 - avg_f1: 0.8036 - val_loss: 0.4071 - val_avg_f1: 0.7964\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.3814 - avg_f1: 0.8046 - val_loss: 0.4038 - val_avg_f1: 0.8016\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.3646 - avg_f1: 0.8127 - val_loss: 0.4169 - val_avg_f1: 0.7727\n",
      "\n",
      "\n",
      "Epoch 15/25 - loss: 0.3572 - avg_f1: 0.8195 - val_loss: 0.4370 - val_avg_f1: 0.7393\n",
      "\n",
      "\n",
      "Epoch 16/25 - loss: 0.3378 - avg_f1: 0.8415 - val_loss: 0.4177 - val_avg_f1: 0.8053\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "\n",
      "Epoch 16: early stopping.\n",
      "Best validation macro F1-score: 0.8092373013496399\n",
      "\n",
      "Evaluating Config #24 [of 30]:\n",
      " {'learning_rate': 0.00024360414314315233, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.021611493568991263, 'reg': 0.001192134082371643, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 0.7382 - avg_f1: 0.4216 - val_loss: 0.6523 - val_avg_f1: 0.4036\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6637 - avg_f1: 0.4865 - val_loss: 0.5995 - val_avg_f1: 0.4036\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.6142 - avg_f1: 0.4970 - val_loss: 0.5375 - val_avg_f1: 0.6206\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5580 - avg_f1: 0.6182 - val_loss: 0.4867 - val_avg_f1: 0.6687\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.5338 - avg_f1: 0.6597 - val_loss: 0.4528 - val_avg_f1: 0.7541\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.5110 - avg_f1: 0.6950 - val_loss: 0.4379 - val_avg_f1: 0.7709\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.4896 - avg_f1: 0.7163 - val_loss: 0.4307 - val_avg_f1: 0.7794\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4801 - avg_f1: 0.7311 - val_loss: 0.4229 - val_avg_f1: 0.7848\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4705 - avg_f1: 0.7257 - val_loss: 0.4225 - val_avg_f1: 0.7962\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4625 - avg_f1: 0.7374 - val_loss: 0.4167 - val_avg_f1: 0.7890\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.4383 - avg_f1: 0.7485 - val_loss: 0.4104 - val_avg_f1: 0.7824\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.4240 - avg_f1: 0.7593 - val_loss: 0.4099 - val_avg_f1: 0.7760\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.4196 - avg_f1: 0.7687 - val_loss: 0.4089 - val_avg_f1: 0.7807\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.4035 - avg_f1: 0.7840 - val_loss: 0.4067 - val_avg_f1: 0.7894\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "\n",
      "Epoch 14: early stopping.\n",
      "Best validation macro F1-score: 0.7961620092391968\n",
      "\n",
      "Evaluating Config #25 [of 30]:\n",
      " {'learning_rate': 0.0005408889439447971, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.015113119722641599, 'reg': 0.0015883246884896094, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 2.2845 - avg_f1: 0.3302 - val_loss: 1.1294 - val_avg_f1: 0.4036\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.7804 - avg_f1: 0.4972 - val_loss: 0.5982 - val_avg_f1: 0.7003\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.6086 - avg_f1: 0.5442 - val_loss: 0.5191 - val_avg_f1: 0.6613\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5621 - avg_f1: 0.6339 - val_loss: 0.4928 - val_avg_f1: 0.6413\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.5294 - avg_f1: 0.6463 - val_loss: 0.4670 - val_avg_f1: 0.7632\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.5070 - avg_f1: 0.6939 - val_loss: 0.4553 - val_avg_f1: 0.7279\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.5052 - avg_f1: 0.6908 - val_loss: 0.4436 - val_avg_f1: 0.7824\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4869 - avg_f1: 0.7216 - val_loss: 0.4431 - val_avg_f1: 0.7304\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4768 - avg_f1: 0.7273 - val_loss: 0.4350 - val_avg_f1: 0.7559\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4586 - avg_f1: 0.7492 - val_loss: 0.4337 - val_avg_f1: 0.7496\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.4517 - avg_f1: 0.7559 - val_loss: 0.4264 - val_avg_f1: 0.7718\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.4476 - avg_f1: 0.7597 - val_loss: 0.4213 - val_avg_f1: 0.7799\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\n",
      "Epoch 12: early stopping.\n",
      "Best validation macro F1-score: 0.7824063301086426\n",
      "\n",
      "Evaluating Config #26 [of 30]:\n",
      " {'learning_rate': 0.0002501335047926774, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.02339831117951461, 'reg': 0.001087512005690888, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 1.7455 - avg_f1: 0.3556 - val_loss: 0.6390 - val_avg_f1: 0.4753\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.7424 - avg_f1: 0.4483 - val_loss: 0.6169 - val_avg_f1: 0.4341\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.6652 - avg_f1: 0.5834 - val_loss: 0.5756 - val_avg_f1: 0.6336\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.6190 - avg_f1: 0.5708 - val_loss: 0.5535 - val_avg_f1: 0.5897\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.6065 - avg_f1: 0.6087 - val_loss: 0.5252 - val_avg_f1: 0.6459\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.5724 - avg_f1: 0.6517 - val_loss: 0.5025 - val_avg_f1: 0.6800\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.5564 - avg_f1: 0.6500 - val_loss: 0.4836 - val_avg_f1: 0.6961\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.5401 - avg_f1: 0.6752 - val_loss: 0.4683 - val_avg_f1: 0.7342\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.5200 - avg_f1: 0.6944 - val_loss: 0.4551 - val_avg_f1: 0.7507\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.5096 - avg_f1: 0.7125 - val_loss: 0.4467 - val_avg_f1: 0.7563\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.5021 - avg_f1: 0.7145 - val_loss: 0.4394 - val_avg_f1: 0.7602\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.4992 - avg_f1: 0.7092 - val_loss: 0.4356 - val_avg_f1: 0.7599\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.4914 - avg_f1: 0.7331 - val_loss: 0.4294 - val_avg_f1: 0.7646\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.4754 - avg_f1: 0.7471 - val_loss: 0.4258 - val_avg_f1: 0.7938\n",
      "\n",
      "\n",
      "Epoch 15/25 - loss: 0.4594 - avg_f1: 0.7514 - val_loss: 0.4220 - val_avg_f1: 0.7870\n",
      "\n",
      "\n",
      "Epoch 16/25 - loss: 0.4649 - avg_f1: 0.7547 - val_loss: 0.4262 - val_avg_f1: 0.7493\n",
      "\n",
      "\n",
      "Epoch 17/25 - loss: 0.4514 - avg_f1: 0.7583 - val_loss: 0.4247 - val_avg_f1: 0.7480\n",
      "\n",
      "\n",
      "Epoch 18/25 - loss: 0.4424 - avg_f1: 0.7626 - val_loss: 0.4155 - val_avg_f1: 0.7908\n",
      "\n",
      "\n",
      "Epoch 19/25 - loss: 0.4301 - avg_f1: 0.7716 - val_loss: 0.4140 - val_avg_f1: 0.7965\n",
      "\n",
      "\n",
      "Epoch 20/25 - loss: 0.4167 - avg_f1: 0.7807 - val_loss: 0.4127 - val_avg_f1: 0.7988\n",
      "\n",
      "\n",
      "Epoch 21/25 - loss: 0.4118 - avg_f1: 0.7807 - val_loss: 0.4092 - val_avg_f1: 0.8055\n",
      "\n",
      "\n",
      "Epoch 22/25 - loss: 0.4048 - avg_f1: 0.7949 - val_loss: 0.4146 - val_avg_f1: 0.7720\n",
      "\n",
      "\n",
      "Epoch 23/25 - loss: 0.3935 - avg_f1: 0.7922 - val_loss: 0.4115 - val_avg_f1: 0.7763\n",
      "\n",
      "\n",
      "Epoch 24/25 - loss: 0.3828 - avg_f1: 0.8042 - val_loss: 0.4299 - val_avg_f1: 0.7471\n",
      "\n",
      "\n",
      "Epoch 25/25 - loss: 0.3777 - avg_f1: 0.7956 - val_loss: 0.4106 - val_avg_f1: 0.7866\n",
      "\n",
      "Restoring best model weights.\n",
      "Best validation macro F1-score: 0.8054981231689453\n",
      "\n",
      "Evaluating Config #27 [of 30]:\n",
      " {'learning_rate': 0.0005861165093032375, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.01881151456940562, 'reg': 0.001219775447939676, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 0.6916 - avg_f1: 0.4980 - val_loss: 0.5857 - val_avg_f1: 0.4248\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6019 - avg_f1: 0.5650 - val_loss: 0.4844 - val_avg_f1: 0.6758\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.5243 - avg_f1: 0.6897 - val_loss: 0.4300 - val_avg_f1: 0.7625\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.4949 - avg_f1: 0.7111 - val_loss: 0.4357 - val_avg_f1: 0.7899\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.4738 - avg_f1: 0.7287 - val_loss: 0.4222 - val_avg_f1: 0.7457\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.4568 - avg_f1: 0.7414 - val_loss: 0.4098 - val_avg_f1: 0.8114\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.4268 - avg_f1: 0.7635 - val_loss: 0.4137 - val_avg_f1: 0.7673\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4108 - avg_f1: 0.7843 - val_loss: 0.4144 - val_avg_f1: 0.7962\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.3786 - avg_f1: 0.8036 - val_loss: 0.4099 - val_avg_f1: 0.7767\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.3815 - avg_f1: 0.8123 - val_loss: 0.4234 - val_avg_f1: 0.7502\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.3573 - avg_f1: 0.8232 - val_loss: 0.4168 - val_avg_f1: 0.7633\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\n",
      "Epoch 11: early stopping.\n",
      "Best validation macro F1-score: 0.8114408254623413\n",
      "\n",
      "Evaluating Config #28 [of 30]:\n",
      " {'learning_rate': 0.00026337987691083887, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.0352732169855685, 'reg': 0.001270847008355976, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 0.7881 - avg_f1: 0.4625 - val_loss: 0.6584 - val_avg_f1: 0.4036\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6972 - avg_f1: 0.5032 - val_loss: 0.6051 - val_avg_f1: 0.4607\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.6519 - avg_f1: 0.5581 - val_loss: 0.5445 - val_avg_f1: 0.5943\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5913 - avg_f1: 0.6275 - val_loss: 0.4856 - val_avg_f1: 0.6837\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.5433 - avg_f1: 0.6732 - val_loss: 0.4510 - val_avg_f1: 0.7493\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.5270 - avg_f1: 0.7002 - val_loss: 0.4322 - val_avg_f1: 0.7652\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.4939 - avg_f1: 0.7123 - val_loss: 0.4239 - val_avg_f1: 0.7857\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4957 - avg_f1: 0.7182 - val_loss: 0.4195 - val_avg_f1: 0.7264\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4747 - avg_f1: 0.7267 - val_loss: 0.4098 - val_avg_f1: 0.7562\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4635 - avg_f1: 0.7453 - val_loss: 0.4037 - val_avg_f1: 0.7907\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.4454 - avg_f1: 0.7530 - val_loss: 0.4092 - val_avg_f1: 0.7498\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.4328 - avg_f1: 0.7614 - val_loss: 0.4001 - val_avg_f1: 0.7905\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.4255 - avg_f1: 0.7665 - val_loss: 0.3995 - val_avg_f1: 0.7890\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.4121 - avg_f1: 0.7774 - val_loss: 0.3999 - val_avg_f1: 0.7853\n",
      "\n",
      "\n",
      "Epoch 15/25 - loss: 0.3989 - avg_f1: 0.7846 - val_loss: 0.4101 - val_avg_f1: 0.7648\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\n",
      "Epoch 15: early stopping.\n",
      "Best validation macro F1-score: 0.7906530499458313\n",
      "\n",
      "Evaluating Config #29 [of 30]:\n",
      " {'learning_rate': 0.0004112207112806718, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.031921711970833724, 'reg': 0.0010349539014249793, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 0.7083 - avg_f1: 0.4868 - val_loss: 0.5839 - val_avg_f1: 0.5481\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6248 - avg_f1: 0.5836 - val_loss: 0.4938 - val_avg_f1: 0.7272\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.5555 - avg_f1: 0.6571 - val_loss: 0.4539 - val_avg_f1: 0.7746\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5172 - avg_f1: 0.7067 - val_loss: 0.4291 - val_avg_f1: 0.7463\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.4844 - avg_f1: 0.7181 - val_loss: 0.4222 - val_avg_f1: 0.7501\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.4655 - avg_f1: 0.7401 - val_loss: 0.4164 - val_avg_f1: 0.7758\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.4591 - avg_f1: 0.7485 - val_loss: 0.4436 - val_avg_f1: 0.7050\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4409 - avg_f1: 0.7521 - val_loss: 0.4162 - val_avg_f1: 0.7582\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4140 - avg_f1: 0.7803 - val_loss: 0.4113 - val_avg_f1: 0.7841\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4074 - avg_f1: 0.7756 - val_loss: 0.4249 - val_avg_f1: 0.7516\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.3901 - avg_f1: 0.8025 - val_loss: 0.4124 - val_avg_f1: 0.7942\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.3661 - avg_f1: 0.8171 - val_loss: 0.4154 - val_avg_f1: 0.7894\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.3595 - avg_f1: 0.8166 - val_loss: 0.4256 - val_avg_f1: 0.7857\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.3532 - avg_f1: 0.8266 - val_loss: 0.4337 - val_avg_f1: 0.7578\n",
      "\n",
      "\n",
      "Epoch 15/25 - loss: 0.3331 - avg_f1: 0.8396 - val_loss: 0.4267 - val_avg_f1: 0.7802\n",
      "\n",
      "\n",
      "Epoch 16/25 - loss: 0.3111 - avg_f1: 0.8574 - val_loss: 0.4305 - val_avg_f1: 0.7829\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "\n",
      "Epoch 16: early stopping.\n",
      "Best validation macro F1-score: 0.7942202687263489\n",
      "\n",
      "Evaluating Config #30 [of 30]:\n",
      " {'learning_rate': 0.0003496349924582731, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.024773135545174296, 'reg': 0.001182211509669324, 'batch_size': 128}\n",
      "\n",
      "Epoch 1/25 - loss: 0.7578 - avg_f1: 0.5003 - val_loss: 0.6397 - val_avg_f1: 0.4427\n",
      "\n",
      "\n",
      "Epoch 2/25 - loss: 0.6579 - avg_f1: 0.5452 - val_loss: 0.5813 - val_avg_f1: 0.6941\n",
      "\n",
      "\n",
      "Epoch 3/25 - loss: 0.6090 - avg_f1: 0.6180 - val_loss: 0.5092 - val_avg_f1: 0.6837\n",
      "\n",
      "\n",
      "Epoch 4/25 - loss: 0.5557 - avg_f1: 0.6691 - val_loss: 0.4634 - val_avg_f1: 0.7146\n",
      "\n",
      "\n",
      "Epoch 5/25 - loss: 0.5200 - avg_f1: 0.7032 - val_loss: 0.4282 - val_avg_f1: 0.7574\n",
      "\n",
      "\n",
      "Epoch 6/25 - loss: 0.4937 - avg_f1: 0.7178 - val_loss: 0.4182 - val_avg_f1: 0.7854\n",
      "\n",
      "\n",
      "Epoch 7/25 - loss: 0.4773 - avg_f1: 0.7394 - val_loss: 0.4409 - val_avg_f1: 0.7105\n",
      "\n",
      "\n",
      "Epoch 8/25 - loss: 0.4652 - avg_f1: 0.7366 - val_loss: 0.4083 - val_avg_f1: 0.7997\n",
      "\n",
      "\n",
      "Epoch 9/25 - loss: 0.4441 - avg_f1: 0.7678 - val_loss: 0.4124 - val_avg_f1: 0.7612\n",
      "\n",
      "\n",
      "Epoch 10/25 - loss: 0.4222 - avg_f1: 0.7672 - val_loss: 0.4071 - val_avg_f1: 0.7965\n",
      "\n",
      "\n",
      "Epoch 11/25 - loss: 0.4161 - avg_f1: 0.7735 - val_loss: 0.4208 - val_avg_f1: 0.7585\n",
      "\n",
      "\n",
      "Epoch 12/25 - loss: 0.4054 - avg_f1: 0.7838 - val_loss: 0.4057 - val_avg_f1: 0.8110\n",
      "\n",
      "\n",
      "Epoch 13/25 - loss: 0.3873 - avg_f1: 0.8036 - val_loss: 0.4065 - val_avg_f1: 0.8128\n",
      "\n",
      "\n",
      "Epoch 14/25 - loss: 0.3704 - avg_f1: 0.8101 - val_loss: 0.4130 - val_avg_f1: 0.7617\n",
      "\n",
      "\n",
      "Epoch 15/25 - loss: 0.3642 - avg_f1: 0.8128 - val_loss: 0.4100 - val_avg_f1: 0.7978\n",
      "\n",
      "\n",
      "Epoch 16/25 - loss: 0.3523 - avg_f1: 0.8305 - val_loss: 0.4240 - val_avg_f1: 0.7511\n",
      "\n",
      "\n",
      "Epoch 17/25 - loss: 0.3464 - avg_f1: 0.8270 - val_loss: 0.4077 - val_avg_f1: 0.8094\n",
      "\n",
      "\n",
      "Epoch 18/25 - loss: 0.3332 - avg_f1: 0.8433 - val_loss: 0.4165 - val_avg_f1: 0.8037\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "\n",
      "Epoch 18: early stopping.\n",
      "Best validation macro F1-score: 0.8128039240837097\n",
      "\n",
      "Search done. Best val_avg_f1 = 0.8153221011161804\n",
      "Best Config: {'learning_rate': 0.00046764905296626456, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.031603069951125815, 'reg': 0.0012734440971713021, 'batch_size': 128}\n"
     ]
    }
   ],
   "source": [
    "# Random search (fine-tune)\n",
    "best_config, best_model, results = random_search(\n",
    "    get_dcnn_model, input_train, y_train, input_val, y_val,\n",
    "    random_search_spaces=random_search_spaces_finetune, TARGET=TARGET, NUM_SEARCH=30, EPOCHS=25, PATIENCE=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "id": "96bda701-48a7-41ad-bfa2-8f39f700b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TARGET == 'val_avg_f1':\n",
    "    new_best_target = max(results, key=lambda x: x[1][TARGET])[1][TARGET]\n",
    "    if new_best_target > best_target:\n",
    "        best_target = new_best_target\n",
    "        hparams = max(results, key=lambda x: x[1][TARGET])[0]\n",
    "        \n",
    "tuning_result = hparams.copy()\n",
    "tuning_result[TARGET] = best_target\n",
    "tuning_result['n_filters'] = int(tuning_result['n_filters'])\n",
    "tuning_result['h_dim'] = int(tuning_result['h_dim'])\n",
    "tuning_result['batch_size'] = int(tuning_result['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "id": "2ad65896-13e6-4eda-ba19-4338d4feb42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store it\n",
    "output_path = results_dir + task_name + '/best_hparams.json'\n",
    "with open(output_path, 'w') as outf:\n",
    "    json.dump(tuning_result, outf, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f78d0-c53d-499e-8c0f-e74727ca62eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'learning_rate': 0.0003811681796003376, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.02233406550456435, 'reg': 0.0011433511889232344, 'batch_size': 128}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/nennomp/opt/anaconda3/envs/i2dl/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/nennomp/opt/anaconda3/envs/i2dl/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/nennomp/opt/anaconda3/envs/i2dl/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/nennomp/opt/anaconda3/envs/i2dl/lib/python3.10/site-packages/keras/engine/training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/nennomp/opt/anaconda3/envs/i2dl/lib/python3.10/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/nennomp/opt/anaconda3/envs/i2dl/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/nennomp/opt/anaconda3/envs/i2dl/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/nennomp/opt/anaconda3/envs/i2dl/lib/python3.10/site-packages/keras/losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/nennomp/opt/anaconda3/envs/i2dl/lib/python3.10/site-packages/keras/losses.py\", line 2162, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/Users/nennomp/opt/anaconda3/envs/i2dl/lib/python3.10/site-packages/keras/backend.py\", line 5677, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 2) vs (None, 1)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m get_dcnn_model(hparams)\n\u001b[1;32m      8\u001b[0m solver \u001b[38;5;241m=\u001b[39m Solver(model, input_train, y_train, input_val, y_val, TARGET)\n\u001b[0;32m----> 9\u001b[0m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/jupyter-workspace/MyHaSpeeDe-1/code/training/solver.py:118\u001b[0m, in \u001b[0;36mSolver.train\u001b[0;34m(self, epochs, batch_size, patience)\u001b[0m\n\u001b[1;32m    115\u001b[0m     early_stopping \u001b[38;5;241m=\u001b[39m CustomBestEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget, patience\u001b[38;5;241m=\u001b[39mpatience, mode\u001b[38;5;241m=\u001b[39mmode, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    116\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(early_stopping)\n\u001b[0;32m--> 118\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Record history loss\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss_history \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/i2dl/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/y1/8q3rg1dd5yldkpv3rs5nzflm0000gn/T/__autograph_generated_file98v7ujjf.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/nennomp/opt/anaconda3/envs/i2dl/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/nennomp/opt/anaconda3/envs/i2dl/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/nennomp/opt/anaconda3/envs/i2dl/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/nennomp/opt/anaconda3/envs/i2dl/lib/python3.10/site-packages/keras/engine/training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/nennomp/opt/anaconda3/envs/i2dl/lib/python3.10/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/nennomp/opt/anaconda3/envs/i2dl/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/nennomp/opt/anaconda3/envs/i2dl/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/nennomp/opt/anaconda3/envs/i2dl/lib/python3.10/site-packages/keras/losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/nennomp/opt/anaconda3/envs/i2dl/lib/python3.10/site-packages/keras/losses.py\", line 2162, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/Users/nennomp/opt/anaconda3/envs/i2dl/lib/python3.10/site-packages/keras/backend.py\", line 5677, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 2) vs (None, 1)).\n"
     ]
    }
   ],
   "source": [
    "with open(results_dir + task_name + '/best_hparams.json', 'r') as inf:\n",
    "    hparams = json.load(inf)\n",
    "del hparams[TARGET]\n",
    "\n",
    "\n",
    "print(f'Config: {hparams}')\n",
    "model = get_dcnn_model(hparams)\n",
    "solver = Solver(model, input_train, y_train, input_val, y_val, TARGET)\n",
    "solver.train(epochs=50, patience=5, batch_size=hparams['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "id": "6c4e652a-1aaf-454b-9555-ac09c5b28346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_path = results_dir + task_name + '/history.png'\n",
    "solver.plot_history(out_path=out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "id": "6153478e-048c-4a5e-bab8-a8c76ca12e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6130 - avg_f1: 0.6231\n",
      "Test loss: 0.613031268119812 - Test val_avg_f1: 0.6230508089065552\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4675    0.6687    0.5503       323\n",
      "           1     0.8011    0.6366    0.7095       677\n",
      "\n",
      "    accuracy                         0.6470      1000\n",
      "   macro avg     0.6343    0.6527    0.6299      1000\n",
      "weighted avg     0.6934    0.6470    0.6581      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_test = {'text': X_test, 'PoS': X_test_pos, 'extra': test_data_extra.values}\n",
    "y_test = test_data['label']\n",
    "\n",
    "loss, metric = model.evaluate(input_test, y_test)\n",
    "print(f'Test loss: {loss} - Test {TARGET}: {metric}')\n",
    "string = f'Test loss: {loss} - Test {TARGET}: {metric}'\n",
    "\n",
    "y_pred = np.where(model.predict(input_test) > 0.5, 1, 0)\n",
    "report = classification_report(y_test, y_pred, digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "id": "03a9d48e-1166-4064-979a-6d190b0d0437",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_dir + task_name + '/test_eval.txt', 'w') as outf:\n",
    "    string = f\"Test Loss - Average F1 Score: {loss:.5f} - {metric:.5f}\\n {report}\"\n",
    "    outf.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "id": "dcb239ce-8970-452c-9e96-25d8ef1acd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'learning_rate': 0.0003811681796003376, 'n_filters': 25, 'h_dim': 64, 'dropout': 0.02233406550456435, 'reg': 0.0011433511889232344, 'batch_size': 128}\n",
      "Training on fold 1/10\n",
      "\n",
      "Epoch 1/30 - loss: 0.6807 - avg_f1: 0.4873 - val_loss: 0.6004 - val_avg_f1: 0.4280\n",
      "\n",
      "\n",
      "Epoch 2/30 - loss: 0.5835 - avg_f1: 0.5657 - val_loss: 0.5428 - val_avg_f1: 0.6782\n",
      "\n",
      "\n",
      "Epoch 3/30 - loss: 0.5351 - avg_f1: 0.6619 - val_loss: 0.5273 - val_avg_f1: 0.6138\n",
      "\n",
      "\n",
      "Epoch 4/30 - loss: 0.5046 - avg_f1: 0.6952 - val_loss: 0.5009 - val_avg_f1: 0.6718\n",
      "\n",
      "\n",
      "Epoch 5/30 - loss: 0.4728 - avg_f1: 0.7240 - val_loss: 0.4952 - val_avg_f1: 0.6840\n",
      "\n",
      "\n",
      "Epoch 6/30 - loss: 0.4555 - avg_f1: 0.7422 - val_loss: 0.4850 - val_avg_f1: 0.7029\n",
      "\n",
      "\n",
      "Epoch 7/30 - loss: 0.4387 - avg_f1: 0.7532 - val_loss: 0.4849 - val_avg_f1: 0.7101\n",
      "\n",
      "\n",
      "Epoch 8/30 - loss: 0.4273 - avg_f1: 0.7693 - val_loss: 0.4752 - val_avg_f1: 0.7093\n",
      "\n",
      "\n",
      "Epoch 9/30 - loss: 0.4154 - avg_f1: 0.7788 - val_loss: 0.4734 - val_avg_f1: 0.7110\n",
      "\n",
      "\n",
      "Epoch 10/30 - loss: 0.4046 - avg_f1: 0.7797 - val_loss: 0.4814 - val_avg_f1: 0.6874\n",
      "\n",
      "\n",
      "Epoch 11/30 - loss: 0.3881 - avg_f1: 0.7906 - val_loss: 0.4732 - val_avg_f1: 0.7048\n",
      "\n",
      "\n",
      "Epoch 12/30 - loss: 0.3740 - avg_f1: 0.8013 - val_loss: 0.4798 - val_avg_f1: 0.6898\n",
      "\n",
      "\n",
      "Epoch 13/30 - loss: 0.3705 - avg_f1: 0.8141 - val_loss: 0.4755 - val_avg_f1: 0.7204\n",
      "\n",
      "\n",
      "Epoch 14/30 - loss: 0.3524 - avg_f1: 0.8290 - val_loss: 0.4903 - val_avg_f1: 0.6862\n",
      "\n",
      "\n",
      "Epoch 15/30 - loss: 0.3435 - avg_f1: 0.8067 - val_loss: 0.4862 - val_avg_f1: 0.6756\n",
      "\n",
      "\n",
      "Epoch 16/30 - loss: 0.3317 - avg_f1: 0.8367 - val_loss: 0.4905 - val_avg_f1: 0.7003\n",
      "\n",
      "\n",
      "Epoch 17/30 - loss: 0.3252 - avg_f1: 0.8515 - val_loss: 0.4912 - val_avg_f1: 0.7065\n",
      "\n",
      "\n",
      "Epoch 18/30 - loss: 0.3096 - avg_f1: 0.8583 - val_loss: 0.5011 - val_avg_f1: 0.6954\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "\n",
      "Epoch 18: early stopping.\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4755 - avg_f1: 0.7247\n",
      "Training on fold 2/10\n",
      "\n",
      "Epoch 1/30 - loss: 2.2692 - avg_f1: 0.3367 - val_loss: 0.9336 - val_avg_f1: 0.3910\n",
      "\n",
      "\n",
      "Epoch 2/30 - loss: 0.7451 - avg_f1: 0.4806 - val_loss: 0.6347 - val_avg_f1: 0.6048\n",
      "\n",
      "\n",
      "Epoch 3/30 - loss: 0.6125 - avg_f1: 0.5873 - val_loss: 0.5727 - val_avg_f1: 0.4951\n",
      "\n",
      "\n",
      "Epoch 4/30 - loss: 0.5731 - avg_f1: 0.6185 - val_loss: 0.5309 - val_avg_f1: 0.5859\n",
      "\n",
      "\n",
      "Epoch 5/30 - loss: 0.5360 - avg_f1: 0.6823 - val_loss: 0.5268 - val_avg_f1: 0.5953\n",
      "\n",
      "\n",
      "Epoch 6/30 - loss: 0.5259 - avg_f1: 0.6596 - val_loss: 0.5051 - val_avg_f1: 0.6487\n",
      "\n",
      "\n",
      "Epoch 7/30 - loss: 0.5163 - avg_f1: 0.6915 - val_loss: 0.4986 - val_avg_f1: 0.6563\n",
      "\n",
      "\n",
      "Epoch 8/30 - loss: 0.5033 - avg_f1: 0.7029 - val_loss: 0.4919 - val_avg_f1: 0.6522\n",
      "\n",
      "\n",
      "Epoch 9/30 - loss: 0.4893 - avg_f1: 0.7204 - val_loss: 0.4853 - val_avg_f1: 0.6832\n",
      "\n",
      "\n",
      "Epoch 10/30 - loss: 0.4681 - avg_f1: 0.7479 - val_loss: 0.4859 - val_avg_f1: 0.6551\n",
      "\n",
      "\n",
      "Epoch 11/30 - loss: 0.4591 - avg_f1: 0.7479 - val_loss: 0.4840 - val_avg_f1: 0.6523\n",
      "\n",
      "\n",
      "Epoch 12/30 - loss: 0.4493 - avg_f1: 0.7358 - val_loss: 0.4847 - val_avg_f1: 0.6523\n",
      "\n",
      "\n",
      "Epoch 13/30 - loss: 0.4436 - avg_f1: 0.7485 - val_loss: 0.4684 - val_avg_f1: 0.7071\n",
      "\n",
      "\n",
      "Epoch 14/30 - loss: 0.4394 - avg_f1: 0.7562 - val_loss: 0.4644 - val_avg_f1: 0.7109\n",
      "\n",
      "\n",
      "Epoch 15/30 - loss: 0.4209 - avg_f1: 0.7796 - val_loss: 0.4591 - val_avg_f1: 0.7221\n",
      "\n",
      "\n",
      "Epoch 16/30 - loss: 0.4134 - avg_f1: 0.7837 - val_loss: 0.4560 - val_avg_f1: 0.7134\n",
      "\n",
      "\n",
      "Epoch 17/30 - loss: 0.4126 - avg_f1: 0.7874 - val_loss: 0.4566 - val_avg_f1: 0.7231\n",
      "\n",
      "\n",
      "Epoch 18/30 - loss: 0.3947 - avg_f1: 0.7847 - val_loss: 0.4497 - val_avg_f1: 0.7290\n",
      "\n",
      "\n",
      "Epoch 19/30 - loss: 0.3883 - avg_f1: 0.8084 - val_loss: 0.4593 - val_avg_f1: 0.7086\n",
      "\n",
      "\n",
      "Epoch 20/30 - loss: 0.3795 - avg_f1: 0.7994 - val_loss: 0.4443 - val_avg_f1: 0.7176\n",
      "\n",
      "\n",
      "Epoch 21/30 - loss: 0.3710 - avg_f1: 0.8164 - val_loss: 0.4411 - val_avg_f1: 0.7550\n",
      "\n",
      "\n",
      "Epoch 22/30 - loss: 0.3639 - avg_f1: 0.8138 - val_loss: 0.4410 - val_avg_f1: 0.7200\n",
      "\n",
      "\n",
      "Epoch 23/30 - loss: 0.3590 - avg_f1: 0.8165 - val_loss: 0.4407 - val_avg_f1: 0.7209\n",
      "\n",
      "\n",
      "Epoch 24/30 - loss: 0.3371 - avg_f1: 0.8323 - val_loss: 0.4346 - val_avg_f1: 0.7288\n",
      "\n",
      "\n",
      "Epoch 25/30 - loss: 0.3286 - avg_f1: 0.8366 - val_loss: 0.4452 - val_avg_f1: 0.7330\n",
      "\n",
      "\n",
      "Epoch 26/30 - loss: 0.3196 - avg_f1: 0.8519 - val_loss: 0.4314 - val_avg_f1: 0.7376\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 21.\n",
      "\n",
      "Epoch 26: early stopping.\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4411 - avg_f1: 0.7877\n",
      "Training on fold 3/10\n",
      "\n",
      "Epoch 1/30 - loss: 0.6989 - avg_f1: 0.4998 - val_loss: 0.6179 - val_avg_f1: 0.4871\n",
      "\n",
      "\n",
      "Epoch 2/30 - loss: 0.6116 - avg_f1: 0.5533 - val_loss: 0.5690 - val_avg_f1: 0.6410\n",
      "\n",
      "\n",
      "Epoch 3/30 - loss: 0.5384 - avg_f1: 0.6786 - val_loss: 0.5341 - val_avg_f1: 0.6427\n",
      "\n",
      "\n",
      "Epoch 4/30 - loss: 0.4991 - avg_f1: 0.7130 - val_loss: 0.5218 - val_avg_f1: 0.6918\n",
      "\n",
      "\n",
      "Epoch 5/30 - loss: 0.4771 - avg_f1: 0.7272 - val_loss: 0.5131 - val_avg_f1: 0.7145\n",
      "\n",
      "\n",
      "Epoch 6/30 - loss: 0.4481 - avg_f1: 0.7601 - val_loss: 0.5070 - val_avg_f1: 0.7127\n",
      "\n",
      "\n",
      "Epoch 7/30 - loss: 0.4337 - avg_f1: 0.7566 - val_loss: 0.5038 - val_avg_f1: 0.7077\n",
      "\n",
      "\n",
      "Epoch 8/30 - loss: 0.4152 - avg_f1: 0.7815 - val_loss: 0.5017 - val_avg_f1: 0.6849\n",
      "\n",
      "\n",
      "Epoch 9/30 - loss: 0.4117 - avg_f1: 0.7844 - val_loss: 0.4974 - val_avg_f1: 0.7043\n",
      "\n",
      "\n",
      "Epoch 10/30 - loss: 0.3929 - avg_f1: 0.7889 - val_loss: 0.5199 - val_avg_f1: 0.7348\n",
      "\n",
      "\n",
      "Epoch 11/30 - loss: 0.3714 - avg_f1: 0.8124 - val_loss: 0.5092 - val_avg_f1: 0.6801\n",
      "\n",
      "\n",
      "Epoch 12/30 - loss: 0.3602 - avg_f1: 0.8133 - val_loss: 0.5082 - val_avg_f1: 0.7134\n",
      "\n",
      "\n",
      "Epoch 13/30 - loss: 0.3592 - avg_f1: 0.8179 - val_loss: 0.5042 - val_avg_f1: 0.7242\n",
      "\n",
      "\n",
      "Epoch 14/30 - loss: 0.3396 - avg_f1: 0.8349 - val_loss: 0.5099 - val_avg_f1: 0.7024\n",
      "\n",
      "\n",
      "Epoch 15/30 - loss: 0.3230 - avg_f1: 0.8577 - val_loss: 0.5056 - val_avg_f1: 0.6874\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\n",
      "Epoch 15: early stopping.\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5199 - avg_f1: 0.7395\n",
      "Training on fold 4/10\n",
      "\n",
      "Epoch 1/30 - loss: 0.7675 - avg_f1: 0.4451 - val_loss: 0.6597 - val_avg_f1: 0.4453\n",
      "\n",
      "\n",
      "Epoch 2/30 - loss: 0.6420 - avg_f1: 0.5056 - val_loss: 0.5527 - val_avg_f1: 0.4979\n",
      "\n",
      "\n",
      "Epoch 3/30 - loss: 0.5525 - avg_f1: 0.6347 - val_loss: 0.5010 - val_avg_f1: 0.6632\n",
      "\n",
      "\n",
      "Epoch 4/30 - loss: 0.5023 - avg_f1: 0.6754 - val_loss: 0.4851 - val_avg_f1: 0.6805\n",
      "\n",
      "\n",
      "Epoch 5/30 - loss: 0.5016 - avg_f1: 0.6910 - val_loss: 0.4688 - val_avg_f1: 0.7218\n",
      "\n",
      "\n",
      "Epoch 6/30 - loss: 0.4785 - avg_f1: 0.7326 - val_loss: 0.4742 - val_avg_f1: 0.7453\n",
      "\n",
      "\n",
      "Epoch 7/30 - loss: 0.4637 - avg_f1: 0.7450 - val_loss: 0.4556 - val_avg_f1: 0.7419\n",
      "\n",
      "\n",
      "Epoch 8/30 - loss: 0.4428 - avg_f1: 0.7628 - val_loss: 0.4498 - val_avg_f1: 0.7270\n",
      "\n",
      "\n",
      "Epoch 9/30 - loss: 0.4285 - avg_f1: 0.7569 - val_loss: 0.4491 - val_avg_f1: 0.7356\n",
      "\n",
      "\n",
      "Epoch 10/30 - loss: 0.4180 - avg_f1: 0.7662 - val_loss: 0.4406 - val_avg_f1: 0.7634\n",
      "\n",
      "\n",
      "Epoch 11/30 - loss: 0.4158 - avg_f1: 0.7848 - val_loss: 0.4378 - val_avg_f1: 0.7506\n",
      "\n",
      "\n",
      "Epoch 12/30 - loss: 0.4075 - avg_f1: 0.7550 - val_loss: 0.4445 - val_avg_f1: 0.7297\n",
      "\n",
      "\n",
      "Epoch 13/30 - loss: 0.3889 - avg_f1: 0.7832 - val_loss: 0.4292 - val_avg_f1: 0.7533\n",
      "\n",
      "\n",
      "Epoch 14/30 - loss: 0.3728 - avg_f1: 0.8045 - val_loss: 0.4267 - val_avg_f1: 0.7483\n",
      "\n",
      "\n",
      "Epoch 15/30 - loss: 0.3690 - avg_f1: 0.7938 - val_loss: 0.4328 - val_avg_f1: 0.7458\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\n",
      "Epoch 15: early stopping.\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4406 - avg_f1: 0.7648\n",
      "Training on fold 5/10\n",
      "\n",
      "Epoch 1/30 - loss: 1.1031 - avg_f1: 0.4370 - val_loss: 0.6993 - val_avg_f1: 0.5097\n",
      "\n",
      "\n",
      "Epoch 2/30 - loss: 0.6686 - avg_f1: 0.5175 - val_loss: 0.5936 - val_avg_f1: 0.6220\n",
      "\n",
      "\n",
      "Epoch 3/30 - loss: 0.5959 - avg_f1: 0.6133 - val_loss: 0.5419 - val_avg_f1: 0.6063\n",
      "\n",
      "\n",
      "Epoch 4/30 - loss: 0.5597 - avg_f1: 0.6462 - val_loss: 0.5095 - val_avg_f1: 0.6812\n",
      "\n",
      "\n",
      "Epoch 5/30 - loss: 0.5146 - avg_f1: 0.6969 - val_loss: 0.4881 - val_avg_f1: 0.7132\n",
      "\n",
      "\n",
      "Epoch 6/30 - loss: 0.4938 - avg_f1: 0.7188 - val_loss: 0.4723 - val_avg_f1: 0.7354\n",
      "\n",
      "\n",
      "Epoch 7/30 - loss: 0.4688 - avg_f1: 0.7261 - val_loss: 0.4618 - val_avg_f1: 0.7217\n",
      "\n",
      "\n",
      "Epoch 8/30 - loss: 0.4592 - avg_f1: 0.7531 - val_loss: 0.4809 - val_avg_f1: 0.6617\n",
      "\n",
      "\n",
      "Epoch 9/30 - loss: 0.4583 - avg_f1: 0.7393 - val_loss: 0.4493 - val_avg_f1: 0.7407\n",
      "\n",
      "\n",
      "Epoch 10/30 - loss: 0.4413 - avg_f1: 0.7548 - val_loss: 0.4466 - val_avg_f1: 0.7598\n",
      "\n",
      "\n",
      "Epoch 11/30 - loss: 0.4233 - avg_f1: 0.7593 - val_loss: 0.4376 - val_avg_f1: 0.7613\n",
      "\n",
      "\n",
      "Epoch 12/30 - loss: 0.4165 - avg_f1: 0.7797 - val_loss: 0.4470 - val_avg_f1: 0.7648\n",
      "\n",
      "\n",
      "Epoch 13/30 - loss: 0.4003 - avg_f1: 0.7862 - val_loss: 0.4361 - val_avg_f1: 0.7627\n",
      "\n",
      "\n",
      "Epoch 14/30 - loss: 0.3856 - avg_f1: 0.7925 - val_loss: 0.4440 - val_avg_f1: 0.7518\n",
      "\n",
      "\n",
      "Epoch 15/30 - loss: 0.3796 - avg_f1: 0.8190 - val_loss: 0.4307 - val_avg_f1: 0.7687\n",
      "\n",
      "\n",
      "Epoch 16/30 - loss: 0.3698 - avg_f1: 0.8055 - val_loss: 0.4325 - val_avg_f1: 0.7653\n",
      "\n",
      "\n",
      "Epoch 17/30 - loss: 0.3605 - avg_f1: 0.8240 - val_loss: 0.4347 - val_avg_f1: 0.7588\n",
      "\n",
      "\n",
      "Epoch 18/30 - loss: 0.3537 - avg_f1: 0.8243 - val_loss: 0.4315 - val_avg_f1: 0.7532\n",
      "\n",
      "\n",
      "Epoch 19/30 - loss: 0.3435 - avg_f1: 0.8311 - val_loss: 0.4337 - val_avg_f1: 0.7481\n",
      "\n",
      "\n",
      "Epoch 20/30 - loss: 0.3228 - avg_f1: 0.8530 - val_loss: 0.4397 - val_avg_f1: 0.7565\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "\n",
      "Epoch 20: early stopping.\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4307 - avg_f1: 0.7736\n",
      "Training on fold 6/10\n",
      "\n",
      "Epoch 1/30 - loss: 1.9725 - avg_f1: 0.3274 - val_loss: 0.8941 - val_avg_f1: 0.4022\n",
      "\n",
      "\n",
      "Epoch 2/30 - loss: 0.7312 - avg_f1: 0.4971 - val_loss: 0.5955 - val_avg_f1: 0.6513\n",
      "\n",
      "\n",
      "Epoch 3/30 - loss: 0.5938 - avg_f1: 0.5287 - val_loss: 0.5414 - val_avg_f1: 0.4755\n",
      "\n",
      "\n",
      "Epoch 4/30 - loss: 0.5483 - avg_f1: 0.6274 - val_loss: 0.5128 - val_avg_f1: 0.6787\n",
      "\n",
      "\n",
      "Epoch 5/30 - loss: 0.5340 - avg_f1: 0.6503 - val_loss: 0.4952 - val_avg_f1: 0.7263\n",
      "\n",
      "\n",
      "Epoch 6/30 - loss: 0.5101 - avg_f1: 0.6754 - val_loss: 0.4868 - val_avg_f1: 0.7157\n",
      "\n",
      "\n",
      "Epoch 7/30 - loss: 0.4979 - avg_f1: 0.6934 - val_loss: 0.4913 - val_avg_f1: 0.6992\n",
      "\n",
      "\n",
      "Epoch 8/30 - loss: 0.4934 - avg_f1: 0.7153 - val_loss: 0.4798 - val_avg_f1: 0.7305\n",
      "\n",
      "\n",
      "Epoch 9/30 - loss: 0.4775 - avg_f1: 0.7268 - val_loss: 0.4741 - val_avg_f1: 0.7343\n",
      "\n",
      "\n",
      "Epoch 10/30 - loss: 0.4746 - avg_f1: 0.7326 - val_loss: 0.4701 - val_avg_f1: 0.7374\n",
      "\n",
      "\n",
      "Epoch 11/30 - loss: 0.4535 - avg_f1: 0.7465 - val_loss: 0.4630 - val_avg_f1: 0.7357\n",
      "\n",
      "\n",
      "Epoch 12/30 - loss: 0.4443 - avg_f1: 0.7617 - val_loss: 0.4599 - val_avg_f1: 0.7400\n",
      "\n",
      "\n",
      "Epoch 13/30 - loss: 0.4362 - avg_f1: 0.7678 - val_loss: 0.4663 - val_avg_f1: 0.7495\n",
      "\n",
      "\n",
      "Epoch 14/30 - loss: 0.4296 - avg_f1: 0.7649 - val_loss: 0.4537 - val_avg_f1: 0.7568\n",
      "\n",
      "\n",
      "Epoch 15/30 - loss: 0.4252 - avg_f1: 0.7670 - val_loss: 0.4496 - val_avg_f1: 0.7591\n",
      "\n",
      "\n",
      "Epoch 16/30 - loss: 0.4098 - avg_f1: 0.7639 - val_loss: 0.4494 - val_avg_f1: 0.7568\n",
      "\n",
      "\n",
      "Epoch 17/30 - loss: 0.4069 - avg_f1: 0.7759 - val_loss: 0.4445 - val_avg_f1: 0.7668\n",
      "\n",
      "\n",
      "Epoch 18/30 - loss: 0.3970 - avg_f1: 0.7857 - val_loss: 0.4432 - val_avg_f1: 0.7615\n",
      "\n",
      "\n",
      "Epoch 19/30 - loss: 0.3912 - avg_f1: 0.7892 - val_loss: 0.4441 - val_avg_f1: 0.7476\n",
      "\n",
      "\n",
      "Epoch 20/30 - loss: 0.3748 - avg_f1: 0.7822 - val_loss: 0.4387 - val_avg_f1: 0.7626\n",
      "\n",
      "\n",
      "Epoch 21/30 - loss: 0.3734 - avg_f1: 0.7844 - val_loss: 0.4371 - val_avg_f1: 0.7399\n",
      "\n",
      "\n",
      "Epoch 22/30 - loss: 0.3555 - avg_f1: 0.8278 - val_loss: 0.4363 - val_avg_f1: 0.7680\n",
      "\n",
      "\n",
      "Epoch 23/30 - loss: 0.3491 - avg_f1: 0.8285 - val_loss: 0.4361 - val_avg_f1: 0.7611\n",
      "\n",
      "\n",
      "Epoch 24/30 - loss: 0.3399 - avg_f1: 0.8339 - val_loss: 0.4367 - val_avg_f1: 0.7637\n",
      "\n",
      "\n",
      "Epoch 25/30 - loss: 0.3359 - avg_f1: 0.8371 - val_loss: 0.4350 - val_avg_f1: 0.7419\n",
      "\n",
      "\n",
      "Epoch 26/30 - loss: 0.3220 - avg_f1: 0.8470 - val_loss: 0.4324 - val_avg_f1: 0.7535\n",
      "\n",
      "\n",
      "Epoch 27/30 - loss: 0.3122 - avg_f1: 0.8553 - val_loss: 0.4366 - val_avg_f1: 0.7559\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 22.\n",
      "\n",
      "Epoch 27: early stopping.\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4363 - avg_f1: 0.7677\n",
      "Training on fold 7/10\n",
      "\n",
      "Epoch 1/30 - loss: 0.7835 - avg_f1: 0.4079 - val_loss: 0.6313 - val_avg_f1: 0.4057\n",
      "\n",
      "\n",
      "Epoch 2/30 - loss: 0.6334 - avg_f1: 0.4928 - val_loss: 0.5852 - val_avg_f1: 0.4057\n",
      "\n",
      "\n",
      "Epoch 3/30 - loss: 0.5725 - avg_f1: 0.5697 - val_loss: 0.5238 - val_avg_f1: 0.6433\n",
      "\n",
      "\n",
      "Epoch 4/30 - loss: 0.5258 - avg_f1: 0.6815 - val_loss: 0.4878 - val_avg_f1: 0.7416\n",
      "\n",
      "\n",
      "Epoch 5/30 - loss: 0.4986 - avg_f1: 0.6916 - val_loss: 0.4722 - val_avg_f1: 0.7100\n",
      "\n",
      "\n",
      "Epoch 6/30 - loss: 0.4767 - avg_f1: 0.7262 - val_loss: 0.4553 - val_avg_f1: 0.7557\n",
      "\n",
      "\n",
      "Epoch 7/30 - loss: 0.4720 - avg_f1: 0.7338 - val_loss: 0.4471 - val_avg_f1: 0.7912\n",
      "\n",
      "\n",
      "Epoch 8/30 - loss: 0.4474 - avg_f1: 0.7482 - val_loss: 0.4405 - val_avg_f1: 0.7853\n",
      "\n",
      "\n",
      "Epoch 9/30 - loss: 0.4308 - avg_f1: 0.7721 - val_loss: 0.4329 - val_avg_f1: 0.7939\n",
      "\n",
      "\n",
      "Epoch 10/30 - loss: 0.4255 - avg_f1: 0.7744 - val_loss: 0.4339 - val_avg_f1: 0.7512\n",
      "\n",
      "\n",
      "Epoch 11/30 - loss: 0.4248 - avg_f1: 0.7708 - val_loss: 0.4326 - val_avg_f1: 0.7211\n",
      "\n",
      "\n",
      "Epoch 12/30 - loss: 0.4128 - avg_f1: 0.7669 - val_loss: 0.4218 - val_avg_f1: 0.7565\n",
      "\n",
      "\n",
      "Epoch 13/30 - loss: 0.4042 - avg_f1: 0.7812 - val_loss: 0.4402 - val_avg_f1: 0.7390\n",
      "\n",
      "\n",
      "Epoch 14/30 - loss: 0.3949 - avg_f1: 0.7753 - val_loss: 0.4321 - val_avg_f1: 0.7355\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "\n",
      "Epoch 14: early stopping.\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4329 - avg_f1: 0.7779\n",
      "Training on fold 8/10\n",
      "\n",
      "Epoch 1/30 - loss: 1.3204 - avg_f1: 0.3946 - val_loss: 0.7873 - val_avg_f1: 0.4022\n",
      "\n",
      "\n",
      "Epoch 2/30 - loss: 0.6849 - avg_f1: 0.5647 - val_loss: 0.5885 - val_avg_f1: 0.5717\n",
      "\n",
      "\n",
      "Epoch 3/30 - loss: 0.6426 - avg_f1: 0.5722 - val_loss: 0.5480 - val_avg_f1: 0.6331\n",
      "\n",
      "\n",
      "Epoch 4/30 - loss: 0.5904 - avg_f1: 0.6336 - val_loss: 0.5143 - val_avg_f1: 0.6747\n",
      "\n",
      "\n",
      "Epoch 5/30 - loss: 0.5483 - avg_f1: 0.6803 - val_loss: 0.4884 - val_avg_f1: 0.6718\n",
      "\n",
      "\n",
      "Epoch 6/30 - loss: 0.5306 - avg_f1: 0.6889 - val_loss: 0.4693 - val_avg_f1: 0.6740\n",
      "\n",
      "\n",
      "Epoch 7/30 - loss: 0.5034 - avg_f1: 0.7096 - val_loss: 0.4530 - val_avg_f1: 0.7059\n",
      "\n",
      "\n",
      "Epoch 8/30 - loss: 0.4804 - avg_f1: 0.7386 - val_loss: 0.4459 - val_avg_f1: 0.7071\n",
      "\n",
      "\n",
      "Epoch 9/30 - loss: 0.4710 - avg_f1: 0.7297 - val_loss: 0.4393 - val_avg_f1: 0.7134\n",
      "\n",
      "\n",
      "Epoch 10/30 - loss: 0.4627 - avg_f1: 0.7457 - val_loss: 0.4317 - val_avg_f1: 0.7211\n",
      "\n",
      "\n",
      "Epoch 11/30 - loss: 0.4479 - avg_f1: 0.7549 - val_loss: 0.4262 - val_avg_f1: 0.7270\n",
      "\n",
      "\n",
      "Epoch 12/30 - loss: 0.4345 - avg_f1: 0.7653 - val_loss: 0.4204 - val_avg_f1: 0.7223\n",
      "\n",
      "\n",
      "Epoch 13/30 - loss: 0.4334 - avg_f1: 0.7645 - val_loss: 0.4177 - val_avg_f1: 0.7221\n",
      "\n",
      "\n",
      "Epoch 14/30 - loss: 0.4130 - avg_f1: 0.7726 - val_loss: 0.4149 - val_avg_f1: 0.7287\n",
      "\n",
      "\n",
      "Epoch 15/30 - loss: 0.4007 - avg_f1: 0.7956 - val_loss: 0.4125 - val_avg_f1: 0.7299\n",
      "\n",
      "\n",
      "Epoch 16/30 - loss: 0.3841 - avg_f1: 0.8120 - val_loss: 0.4098 - val_avg_f1: 0.7293\n",
      "\n",
      "\n",
      "Epoch 17/30 - loss: 0.3818 - avg_f1: 0.8058 - val_loss: 0.4086 - val_avg_f1: 0.7514\n",
      "\n",
      "\n",
      "Epoch 18/30 - loss: 0.3743 - avg_f1: 0.8238 - val_loss: 0.4293 - val_avg_f1: 0.6934\n",
      "\n",
      "\n",
      "Epoch 19/30 - loss: 0.3588 - avg_f1: 0.8070 - val_loss: 0.4105 - val_avg_f1: 0.7227\n",
      "\n",
      "\n",
      "Epoch 20/30 - loss: 0.3461 - avg_f1: 0.8359 - val_loss: 0.4160 - val_avg_f1: 0.6920\n",
      "\n",
      "\n",
      "Epoch 21/30 - loss: 0.3397 - avg_f1: 0.8330 - val_loss: 0.4423 - val_avg_f1: 0.6814\n",
      "\n",
      "\n",
      "Epoch 22/30 - loss: 0.3253 - avg_f1: 0.8476 - val_loss: 0.4117 - val_avg_f1: 0.7219\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "\n",
      "Epoch 22: early stopping.\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4086 - avg_f1: 0.7883\n",
      "Training on fold 9/10\n",
      "\n",
      "Epoch 1/30 - loss: 0.9797 - avg_f1: 0.4537 - val_loss: 0.6233 - val_avg_f1: 0.4046\n",
      "\n",
      "\n",
      "Epoch 2/30 - loss: 0.6590 - avg_f1: 0.5085 - val_loss: 0.5745 - val_avg_f1: 0.6977\n",
      "\n",
      "\n",
      "Epoch 3/30 - loss: 0.5882 - avg_f1: 0.6213 - val_loss: 0.5576 - val_avg_f1: 0.4760\n",
      "\n",
      "\n",
      "Epoch 4/30 - loss: 0.5528 - avg_f1: 0.6431 - val_loss: 0.5169 - val_avg_f1: 0.6902\n",
      "\n",
      "\n",
      "Epoch 5/30 - loss: 0.5148 - avg_f1: 0.6904 - val_loss: 0.5138 - val_avg_f1: 0.7004\n",
      "\n",
      "\n",
      "Epoch 6/30 - loss: 0.4844 - avg_f1: 0.7343 - val_loss: 0.4961 - val_avg_f1: 0.7368\n",
      "\n",
      "\n",
      "Epoch 7/30 - loss: 0.4698 - avg_f1: 0.7371 - val_loss: 0.4945 - val_avg_f1: 0.7464\n",
      "\n",
      "\n",
      "Epoch 8/30 - loss: 0.4471 - avg_f1: 0.7608 - val_loss: 0.4893 - val_avg_f1: 0.7522\n",
      "\n",
      "\n",
      "Epoch 9/30 - loss: 0.4396 - avg_f1: 0.7620 - val_loss: 0.4777 - val_avg_f1: 0.7737\n",
      "\n",
      "\n",
      "Epoch 10/30 - loss: 0.4243 - avg_f1: 0.7669 - val_loss: 0.4760 - val_avg_f1: 0.7712\n",
      "\n",
      "\n",
      "Epoch 11/30 - loss: 0.4024 - avg_f1: 0.7825 - val_loss: 0.4973 - val_avg_f1: 0.7765\n",
      "\n",
      "\n",
      "Epoch 12/30 - loss: 0.3974 - avg_f1: 0.7895 - val_loss: 0.4894 - val_avg_f1: 0.7075\n",
      "\n",
      "\n",
      "Epoch 13/30 - loss: 0.3980 - avg_f1: 0.7937 - val_loss: 0.4871 - val_avg_f1: 0.7575\n",
      "\n",
      "\n",
      "Epoch 14/30 - loss: 0.3720 - avg_f1: 0.8094 - val_loss: 0.4828 - val_avg_f1: 0.7412\n",
      "\n",
      "\n",
      "Epoch 15/30 - loss: 0.3640 - avg_f1: 0.8132 - val_loss: 0.4801 - val_avg_f1: 0.7773\n",
      "\n",
      "\n",
      "Epoch 16/30 - loss: 0.3435 - avg_f1: 0.8136 - val_loss: 0.4796 - val_avg_f1: 0.7856\n",
      "\n",
      "\n",
      "Epoch 17/30 - loss: 0.3364 - avg_f1: 0.8459 - val_loss: 0.4865 - val_avg_f1: 0.7883\n",
      "\n",
      "\n",
      "Epoch 18/30 - loss: 0.3262 - avg_f1: 0.8550 - val_loss: 0.4902 - val_avg_f1: 0.7478\n",
      "\n",
      "\n",
      "Epoch 19/30 - loss: 0.3142 - avg_f1: 0.8616 - val_loss: 0.5032 - val_avg_f1: 0.7812\n",
      "\n",
      "\n",
      "Epoch 20/30 - loss: 0.2980 - avg_f1: 0.8645 - val_loss: 0.5094 - val_avg_f1: 0.7205\n",
      "\n",
      "\n",
      "Epoch 21/30 - loss: 0.2952 - avg_f1: 0.8603 - val_loss: 0.4972 - val_avg_f1: 0.7573\n",
      "\n",
      "\n",
      "Epoch 22/30 - loss: 0.2758 - avg_f1: 0.8833 - val_loss: 0.5100 - val_avg_f1: 0.7844\n",
      "\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "\n",
      "Epoch 22: early stopping.\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4865 - avg_f1: 0.7875\n",
      "Training on fold 10/10\n",
      "\n",
      "Epoch 1/30 - loss: 0.7722 - avg_f1: 0.4784 - val_loss: 0.6197 - val_avg_f1: 0.4347\n",
      "\n",
      "\n",
      "Epoch 2/30 - loss: 0.6302 - avg_f1: 0.5099 - val_loss: 0.5565 - val_avg_f1: 0.5485\n",
      "\n",
      "\n",
      "Epoch 3/30 - loss: 0.5758 - avg_f1: 0.6249 - val_loss: 0.4908 - val_avg_f1: 0.6927\n",
      "\n",
      "\n",
      "Epoch 4/30 - loss: 0.5221 - avg_f1: 0.6773 - val_loss: 0.4440 - val_avg_f1: 0.7431\n",
      "\n",
      "\n",
      "Epoch 5/30 - loss: 0.4865 - avg_f1: 0.7081 - val_loss: 0.4276 - val_avg_f1: 0.7681\n",
      "\n",
      "\n",
      "Epoch 6/30 - loss: 0.4653 - avg_f1: 0.7411 - val_loss: 0.4147 - val_avg_f1: 0.7476\n",
      "\n",
      "\n",
      "Epoch 7/30 - loss: 0.4547 - avg_f1: 0.7464 - val_loss: 0.4248 - val_avg_f1: 0.7572\n",
      "\n",
      "\n",
      "Epoch 8/30 - loss: 0.4372 - avg_f1: 0.7636 - val_loss: 0.4059 - val_avg_f1: 0.7676\n",
      "\n",
      "\n",
      "Epoch 9/30 - loss: 0.4241 - avg_f1: 0.7697 - val_loss: 0.4037 - val_avg_f1: 0.7688\n",
      "\n",
      "\n",
      "Epoch 10/30 - loss: 0.4158 - avg_f1: 0.7711 - val_loss: 0.4006 - val_avg_f1: 0.7670\n",
      "\n",
      "\n",
      "Epoch 11/30 - loss: 0.3973 - avg_f1: 0.7841 - val_loss: 0.3990 - val_avg_f1: 0.7847\n",
      "\n",
      "\n",
      "Epoch 12/30 - loss: 0.3914 - avg_f1: 0.8066 - val_loss: 0.3973 - val_avg_f1: 0.7785\n",
      "\n",
      "\n",
      "Epoch 13/30 - loss: 0.3784 - avg_f1: 0.7966 - val_loss: 0.3967 - val_avg_f1: 0.7534\n",
      "\n",
      "\n",
      "Epoch 14/30 - loss: 0.3736 - avg_f1: 0.8125 - val_loss: 0.3925 - val_avg_f1: 0.7761\n",
      "\n",
      "\n",
      "Epoch 15/30 - loss: 0.3598 - avg_f1: 0.7947 - val_loss: 0.4028 - val_avg_f1: 0.7550\n",
      "\n",
      "\n",
      "Epoch 16/30 - loss: 0.3730 - avg_f1: 0.8011 - val_loss: 0.3921 - val_avg_f1: 0.8020\n",
      "\n",
      "\n",
      "Epoch 17/30 - loss: 0.3577 - avg_f1: 0.8239 - val_loss: 0.3868 - val_avg_f1: 0.7992\n",
      "\n",
      "\n",
      "Epoch 18/30 - loss: 0.3418 - avg_f1: 0.8274 - val_loss: 0.3921 - val_avg_f1: 0.7628\n",
      "\n",
      "\n",
      "Epoch 19/30 - loss: 0.3304 - avg_f1: 0.8423 - val_loss: 0.3868 - val_avg_f1: 0.8041\n",
      "\n",
      "\n",
      "Epoch 20/30 - loss: 0.3145 - avg_f1: 0.8404 - val_loss: 0.3923 - val_avg_f1: 0.8041\n",
      "\n",
      "\n",
      "Epoch 21/30 - loss: 0.3044 - avg_f1: 0.8497 - val_loss: 0.3958 - val_avg_f1: 0.7950\n",
      "\n",
      "\n",
      "Epoch 22/30 - loss: 0.3088 - avg_f1: 0.8448 - val_loss: 0.3976 - val_avg_f1: 0.8046\n",
      "\n",
      "\n",
      "Epoch 23/30 - loss: 0.2983 - avg_f1: 0.8542 - val_loss: 0.4241 - val_avg_f1: 0.7435\n",
      "\n",
      "\n",
      "Epoch 24/30 - loss: 0.2885 - avg_f1: 0.8671 - val_loss: 0.3900 - val_avg_f1: 0.7978\n",
      "\n",
      "\n",
      "Epoch 25/30 - loss: 0.2721 - avg_f1: 0.8778 - val_loss: 0.3875 - val_avg_f1: 0.8124\n",
      "\n",
      "\n",
      "Epoch 26/30 - loss: 0.2652 - avg_f1: 0.8868 - val_loss: 0.3978 - val_avg_f1: 0.7925\n",
      "\n",
      "\n",
      "Epoch 27/30 - loss: 0.2588 - avg_f1: 0.8912 - val_loss: 0.4031 - val_avg_f1: 0.7831\n",
      "\n",
      "\n",
      "Epoch 28/30 - loss: 0.2405 - avg_f1: 0.8940 - val_loss: 0.3895 - val_avg_f1: 0.8151\n",
      "\n",
      "\n",
      "Epoch 29/30 - loss: 0.2321 - avg_f1: 0.8987 - val_loss: 0.4285 - val_avg_f1: 0.7552\n",
      "\n",
      "\n",
      "Epoch 30/30 - loss: 0.2271 - avg_f1: 0.9149 - val_loss: 0.4036 - val_avg_f1: 0.8056\n",
      "\n",
      "Restoring best model weights.\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3895 - avg_f1: 0.8086\n",
      "Average scores across 10 folds: [0.44615186 0.77202788]\n"
     ]
    }
   ],
   "source": [
    "with open(results_dir + task_name + '/best_hparams.json', 'r') as inf:\n",
    "    hparams = json.load(inf)\n",
    "del hparams[TARGET]\n",
    "\n",
    "input_dev = {'text': X_dev, 'PoS': X_dev_pos, 'extra': dev_data_extra.values, 'label': dev_data['label']}\n",
    "\n",
    "print(f'Config: {hparams}')\n",
    "solver = Solver(None, input_train, y_train, input_val, y_val, TARGET)\n",
    "kfold_models = solver.train_with_kfold(\n",
    "    get_dcnn_model, hparams, input_dev, n_splits=10, \n",
    "    batch_size=hparams['batch_size'], epochs=30, patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "id": "4a079013-5a5f-4c48-84ab-d4872a6dd4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Average F1 Score for Ensemble: 0.67652\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4535    0.7399    0.5624       323\n",
      "           1     0.8224    0.5746    0.6765       677\n",
      "\n",
      "    accuracy                         0.6280      1000\n",
      "   macro avg     0.6380    0.6573    0.6194      1000\n",
      "weighted avg     0.7033    0.6280    0.6396      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_test = {'text': X_test, 'PoS': X_test_pos, 'extra': test_data_extra.values}\n",
    "y_test = test_data['label']\n",
    "\n",
    "predictions = solver.ensemble_predict(input_test)\n",
    "report = classification_report(y_test, predictions, digits=4)\n",
    "avg_f1 = f1_score(y_test, predictions)\n",
    "\n",
    "print(f'Average F1 Score for Ensemble: {avg_f1:.5f}')\n",
    "print(f'\\n{report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "id": "30af1687-752a-4c00-8592-3622f982ffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_dir + task_name + '/test_kfold_eval.txt', 'w') as outf:\n",
    "    string = f\"Average F1 Score for Ensemble: {avg_f1:.5f}\\n {report}\"\n",
    "    outf.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "id": "c30e0ddc-4593-42c2-8a1e-6a10b7dda29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "---LogisticRegression---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8414    0.8713    0.8561       676\n",
      "           1     0.7100    0.6574    0.6827       324\n",
      "\n",
      "    accuracy                         0.8020      1000\n",
      "   macro avg     0.7757    0.7644    0.7694      1000\n",
      "weighted avg     0.7988    0.8020    0.7999      1000\n",
      "\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "---RandomForestClassifier---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8374    0.8683    0.8526       676\n",
      "           1     0.7023    0.6481    0.6742       324\n",
      "\n",
      "    accuracy                         0.7970      1000\n",
      "   macro avg     0.7699    0.7582    0.7634      1000\n",
      "weighted avg     0.7936    0.7970    0.7948      1000\n",
      "\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "---XGB---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8207    0.8669    0.8432       676\n",
      "           1     0.6853    0.6049    0.6426       324\n",
      "\n",
      "    accuracy                         0.7820      1000\n",
      "   macro avg     0.7530    0.7359    0.7429      1000\n",
      "weighted avg     0.7769    0.7820    0.7782      1000\n",
      "\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "---SVC---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8333    0.8654    0.8491       676\n",
      "           1     0.6946    0.6389    0.6656       324\n",
      "\n",
      "    accuracy                         0.7920      1000\n",
      "   macro avg     0.7640    0.7521    0.7573      1000\n",
      "weighted avg     0.7884    0.7920    0.7896      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "meta_learners = {\n",
    "    'LogisticRegression': LogisticRegression(), \n",
    "    'RandomForestClassifier': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'XGB': xgb.XGBClassifier(n_estimators=100, random_state=42),\n",
    "    'SVC': SVC(probability=True, kernel='linear', C=1),\n",
    "}\n",
    "\n",
    "for meta_learner in meta_learners.items():\n",
    "    predictions = solver.meta_learner_predict(input_test, meta_learner=meta_learner[1])\n",
    "    report = classification_report(y_test, predictions, digits=4)\n",
    "    print(f'---{meta_learner[0]}---\\n{report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ee33db7-7fb3-4896-8dbf-977767d0873d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "X_dev_reshaped = X_dev.reshape(X_dev.shape[0], -1)  # This flattens the data\n",
    "\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_dev_reshaped, dev_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8210fc2c-ee77-41b4-b60e-8523ad734c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_combined_test = np.concatenate((X_test, X_test_pos, test_data_extra), axis=1)\n",
    "# Make predictions on the test data\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], -1)  # This flattens the data\n",
    "y_pred = svm.predict(X_test_reshaped)\n",
    "\n",
    "report = classification_report(test_data['label'], y_pred, digits=4)\n",
    "print(f\"Classification Report:\\n{report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c56f1e9-8255-4949-bc28-2f68afcd3c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78cbb727-e4d1-4807-8879-35d3158b9093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([1, 0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677354bd-274e-4bc9-9b82-a6a2afb53fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
