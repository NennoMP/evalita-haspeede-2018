{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d3416e-3aee-496b-9a46-d86fab3fe008",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70e0cff-cbad-458a-b79f-8c7f78b4d532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.layers import LeakyReLU, Reshape, GlobalMaxPooling1D, Input, concatenate, Embedding, Flatten, Dropout, Dense, Conv1D, Activation, BatchNormalization\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dir_parts = os.getcwd().split(os.path.sep)\n",
    "root_index = dir_parts.index('MyHaSpeeDe-1')\n",
    "root_path = os.path.sep.join(dir_parts[:root_index + 1])\n",
    "sys.path.append(root_path + '/code/')\n",
    "from dcnn.layers import SemiDynamicKMaxPooling, Folding\n",
    "from hyperparameter_tuning import bayesian_optimization, random_search\n",
    "from training.metrics import avg_f1\n",
    "from training.solver import Solver\n",
    "from sentence_statistics import max_sentence_length, average_sentence_length\n",
    "from word_embedding import get_key_index_mappings, get_embedding_matrix, get_key_index_pos_mappings, get_pos_matrix, sentence_to_embedding, data_to_embedding, pos_to_embedding\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84564330-d564-4700-9865-88576073b71b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b342ae0-1ef2-4bdb-9128-cd10ac83adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "fb_dir = root_path + '/data/facebook/'\n",
    "tw_dir = root_path + '/data/twitter/'\n",
    "preprocessed_dir = 'preprocessed/'\n",
    "w2v_dir = root_path + '/data/word2vec/'\n",
    "results_dir = root_path + '/results/DCNN/'\n",
    "\n",
    "# Filepaths (Facebook dataset)\n",
    "fb_dev_preprocessed_path = fb_dir + 'dev/' + preprocessed_dir + 'fb_dev_preprocessed.csv'\n",
    "fb_test_preprocessed_path = fb_dir + 'test/' + preprocessed_dir + 'fb_test_preprocessed.csv'\n",
    "\n",
    "# Filepaths (Twitter dataset)\n",
    "tw_dev_preprocessed_path = tw_dir + 'dev/' + preprocessed_dir + 'tw_dev_preprocessed.csv'\n",
    "tw_test_preprocessed_path = tw_dir + 'test/' + preprocessed_dir + 'tw_test_preprocessed.csv'\n",
    "\n",
    "# W2V + Corpus\n",
    "w2v_pretrained_path = w2v_dir + 'twitter128.bin' # w2v\n",
    "dictionary_path = root_path + '/data/italian_words.txt' # vocabulary\n",
    "bad_words_path = root_path + '/data/italian_bad_words.txt' # bad words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e8bfc-eca0-4202-8838-2fc0473d09ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task selection\n",
    "The model will be evaluated and fine-tuned w.r.t the three HaSpeeDe-1 tasks:\n",
    "- **Task 1 (HaSpeeDe-FB)**: only the FB dataset can be used to classify the FB test set;\n",
    "- **Task 2 (HaSpeeDe-TW)**: only the TW dataset can be used to classify the TW test set;\n",
    "- **Task 2 (Cross-HaspeeDe)**: only the FB dataset can be used to clasify the TW data set and viceversa (i.e. Cross-HaSpeeDe-FB and Cross-HasPeeDe-TW respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907756af-1ced-4edd-ad48-adba634c40a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Task(Enum):\n",
    "    HASPEEDE_FB = ('haspeede-fb', fb_dev_preprocessed_path, fb_test_preprocessed_path)\n",
    "    HASPEEDE_TW = ('haspeede-tw', tw_dev_preprocessed_path, tw_test_preprocessed_path)\n",
    "    CROSS_HASPEEDE_FB = ('cross-haspeede-fb', fb_dev_preprocessed_path, tw_test_preprocessed_path)\n",
    "    CROSS_HASPEEDE_TW = ('cross-haspeede-tw', tw_dev_preprocessed_path, fb_test_preprocessed_path)\n",
    "\n",
    "    def __init__(self, task_name, dev_path, test_path):\n",
    "        self.task_name = task_name\n",
    "        self.dev_path = dev_path\n",
    "        self.test_path = test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39704978-e26f-48a1-8074-c058a64e2853",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose task\n",
    "#TASK = Task.HASPEEDE_FB\n",
    "#TASK = Task.HASPEEDE_TW\n",
    "#TASK = Task.CROSS_HASPEEDE_FB\n",
    "TASK = Task.CROSS_HASPEEDE_TW\n",
    "\n",
    "task_name = TASK.task_name\n",
    "dev_path = TASK.dev_path\n",
    "test_path = TASK.test_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e06dd7-2865-47e4-88bc-45125d43f219",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a996d178-64df-4e0c-8abc-26fa0c0be8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b48c15-c9c2-4198-bc4c-0b63f615209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Twitter dev/test dataset\n",
    "dev_inf = open(dev_path, encoding='utf-8')\n",
    "dev_data = pd.read_csv(dev_inf, sep=',', converters={'tokens': pd.eval, 'lemmas': pd.eval})\n",
    "\n",
    "test_inf = open(test_path, encoding='utf-8')\n",
    "test_data = pd.read_csv(test_inf, sep=',', converters={'tokens': pd.eval, 'lemmas': pd.eval})\n",
    "\n",
    "# Separate extra features\n",
    "dev_data_extra = dev_data.drop(['id', 'text', 'label', 'hashtags', 'tokens', 'lemmas', 'PoS', 'text_en'], axis=1, errors='ignore')\n",
    "test_data_extra = test_data.drop(['id', 'text', 'label', 'hashtags', 'tokens', 'lemmas', 'PoS', 'text_en'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65da1ac-fb2d-4b44-8993-5d7477b3b102",
   "metadata": {
    "tags": []
   },
   "source": [
    "## W2V Embedding\n",
    "Load pre-trained W2V model of Italian Twitter embeddings from the Italian NLP Lab [[1]](http://www.italianlp.it/resources/italian-word-embeddings/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872d44a6-fdf9-49b8-88d3-6ce66e0937b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OOV_TOKEN = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e841f4-8b46-430a-b245-7633b34cd783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W2V embedding\n",
    "w2v = KeyedVectors.load_word2vec_format(w2v_pretrained_path, binary=True)\n",
    "\n",
    "key_to_idx, idx_to_key = get_key_index_mappings(w2v, OOV_TOKEN)\n",
    "embedding_matrix, vocab_size = get_embedding_matrix(w2v, idx_to_key, OOV_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ea587-22d2-4a52-a617-293a87d993f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = vocab_size\n",
    "EMB_DIMS = embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1a68b8-b436-4180-aa57-545a856552b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PoS Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6594c-1cd2-4488-9ea9-e58bd2639274",
   "metadata": {},
   "outputs": [],
   "source": [
    "OOV_TOKEN = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d999d-8287-4f15-9221-a248edcf7ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_idx_pos, idx_to_key_pos = get_key_index_pos_mappings(dev_data[\"PoS\"], OOV_TOKEN)\n",
    "idx_to_onehot_pos = get_pos_matrix(idx_to_key_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a0ec22-eea4-4c38-8af4-e4002f620698",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f6ced2-fa52-43e4-8d9d-0c92caaa46ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = (math.ceil(max_sentence_length(dev_data['tokens']) / 2.) * 2) - 1 # max sentence length\n",
    "AVG_LEN = average_sentence_length(dev_data['tokens']) # average sentence length\n",
    "VAL_SPLIT = 0.2 # val set percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210012fe-3ea4-4ffe-aeaf-9e15e903d2a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data to embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fffd704-6441-4593-94e1-15f289dd9e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = data_to_embedding(dev_data['tokens'], embedding_matrix, key_to_idx, truncation=MAX_LEN, padding=True)\n",
    "X_dev_pos = pos_to_embedding(dev_data['PoS'], key_to_idx_pos, max_text_len=MAX_LEN)\n",
    "\n",
    "X_test = data_to_embedding(test_data['tokens'], embedding_matrix, key_to_idx, truncation=MAX_LEN, padding=True)\n",
    "X_test_pos = pos_to_embedding(test_data['PoS'], key_to_idx_pos, max_text_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9ada02-9501-4e08-8fa9-a3b318b27b1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Split Train-Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e377154-6ded-4db7-bac2-8fd636624dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_train_pos, x_val_pos, x_train_extra, x_val_extra, y_train, y_val = train_test_split(X_dev, X_dev_pos, dev_data_extra.values, dev_data['label'], \n",
    "                                                                                                      test_size=VAL_SPLIT, random_state=128, stratify=dev_data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb1a507-b763-4ee0-b545-a0134f0a32c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0833f1c-1922-49fe-9946-daa4170143b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'val_avg_f1' # optimization target\n",
    "\n",
    "# Train data\n",
    "input_train = {'text': x_train, 'PoS': x_train_pos, 'extra': x_train_extra}\n",
    "\n",
    "# Val data\n",
    "input_val = {'text': x_val, 'PoS': x_val_pos, 'extra': x_val_extra}\n",
    "\n",
    "# Dataset-specific dimensions\n",
    "POS_SHAPE = x_train_pos.shape\n",
    "EXTRA_SHAPE = dev_data_extra.shape\n",
    "\n",
    "# To-tune hyperparameters\n",
    "K_TOP = 10 # fixed pooling parameter for topmost conv layer\n",
    "hparams = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e354c81-1cec-4519-98ed-be7952adcfc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7895e967-a1c1-46ab-81f0-db147a04c62b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dcnn_model(hparams, kernels=[2, 4, 6]):\n",
    "    # Input\n",
    "    in_text = Input(name='text', shape=(MAX_LEN, EMB_DIMS,))\n",
    "    in_pos = Input(name='PoS', shape=(POS_SHAPE[1],))\n",
    "    in_extra = Input(name='extra', shape=(EXTRA_SHAPE[1],))\n",
    "        \n",
    "    # Embedding PoS layer\n",
    "    pos_emb = Embedding(24, 23, input_length=MAX_LEN)(in_pos)\n",
    "    \n",
    "    # Conv1D + Folding + DynamicKMaxPooling + Activation (text)\n",
    "    convs_text = []\n",
    "    for i, kernel_size in enumerate(kernels, start=1):\n",
    "        l_conv = Conv1D(filters=hparams['n_filters'], kernel_size=kernel_size)(in_text)\n",
    "        l_fold = Folding()(l_conv)  # Apply folding after convolution\n",
    "        l_pool = SemiDynamicKMaxPooling(k_top=K_TOP, L=len(kernels), l=i, avg_s=AVG_LEN)(l_fold)\n",
    "        l_activation = LeakyReLU(alpha=0.1)(l_pool)\n",
    "        convs_text.append(l_activation)\n",
    "        \n",
    "    l_concat_text = concatenate(convs_text)\n",
    "    l_flat_text = Flatten()(l_concat_text)\n",
    "    \n",
    "    # Conv1D + Folding + DynamicKMaxPooling + Activation (PoS)\n",
    "    convs_pos = []\n",
    "    for j, kernel_size in enumerate(kernels, start=1):\n",
    "        l_conv = Conv1D(filters=hparams['n_filters'], kernel_size=kernel_size)(pos_emb)\n",
    "        l_fold = Folding()(l_conv)  # Apply folding after convolution\n",
    "        l_pool = SemiDynamicKMaxPooling(k_top=K_TOP, L=len(kernels), l=j, avg_s=AVG_LEN)(l_fold)\n",
    "        l_activation = LeakyReLU(alpha=0.1)(l_pool)\n",
    "        convs_pos.append(l_activation)\n",
    "        \n",
    "    l_concat_pos = concatenate(convs_pos)\n",
    "    l_flat_pos = Flatten()(l_concat_pos)\n",
    "    \n",
    "     \n",
    "    \n",
    "    # Concat text - PoS - extra features\n",
    "    input_merge = concatenate([l_flat_text, l_flat_pos, in_extra])\n",
    "    \n",
    "    # Add a Dense layer\n",
    "    l_dense = Dropout(hparams['dropout'])(input_merge)\n",
    "    l_dense = Dense(units=hparams['h_dim'])(l_dense)\n",
    "    l_dense = LeakyReLU(alpha=0.1)(l_dense)\n",
    "    l_dense = Dropout(hparams['dropout'])(l_dense)\n",
    "    \n",
    "    \n",
    "    # Fully connected layer for binary classification with regularization (L2)\n",
    "    output_layer = Dense(1, activation='sigmoid', kernel_regularizer=l1_l2(l1=hparams['reg'], l2=hparams['reg']))(l_dense)\n",
    "    \n",
    "    model = Model(inputs=[in_text, in_pos, in_extra], outputs=output_layer)\n",
    "    optimizer = Adam(learning_rate=hparams['learning_rate'])\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[avg_f1])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f6e012-85ca-4278-9774-219dc1d887bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Hyper-parameters Tuning\n",
    "A common approach is to start with a coarse random searcg across a wide range of values to find promising sub-ranges of our parameter space. Then, we can zoom into these ranges and perform another random search (or a grid search) to finetune the configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcd4283-cd0c-4943-bebd-da224c9b2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_spaces = {\n",
    "    'learning_rate': ([1e-5, 1e-1], 'log'),\n",
    "    'n_filters': ([25, 50, 100], 'item'),\n",
    "    'h_dim': ([16, 32, 64, 128], 'item'),\n",
    "    'dropout': ([0.0, 0.5], 'float'),\n",
    "    'reg': ([1e-5, 1e-1], 'log'),\n",
    "    'batch_size': ([16, 32, 64, 128], 'item')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e30b09-91c9-4782-b46d-d661772876ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b72746-077a-4d88-b2d2-d47c55ddcd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Optimization\n",
    "best_target, best_config = bayesian_optimization(\n",
    "    get_dcnn_model, input_train, y_train, input_val, y_val, \n",
    "    bayesian_optimization_spaces=hparams_spaces, TARGET=TARGET, N_TRIALS=50, EPOCHS=25, PATIENCE=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fbac69-80d3-415d-9ea4-5a5697f7bb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = best_config\n",
    "\n",
    "# set new intervals for fine-tune random search\n",
    "lr = hparams['learning_rate']\n",
    "n_filters = hparams['n_filters']\n",
    "h_dim = hparams['h_dim']\n",
    "dropout = hparams['dropout']\n",
    "reg = hparams['reg']\n",
    "batch_size = hparams['batch_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cc4ca3-da37-4944-9ba8-a90e63da6198",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Random search (fine-tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325d2528-0979-4deb-8871-f7554ec31646",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.2\n",
    "random_search_spaces_finetune = {\n",
    "    'learning_rate': ([10 ** (np.log10(lr) - epsilon), 10 ** (np.log10(lr) + epsilon)], 'float'),\n",
    "    'n_filters': ([n_filters], 'item'),\n",
    "    'h_dim': ([h_dim], 'item'),\n",
    "    'dropout': ([10 ** (np.log10(dropout) - epsilon), 10 ** (np.log10(dropout) + epsilon)], 'float'),\n",
    "    'reg': ([10 ** (np.log10(reg) - epsilon), 10 ** (np.log10(reg) + epsilon)], 'float'),\n",
    "    'batch_size': ([batch_size], 'item'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada58c9a-c4c0-4333-87e3-4fd7e01a7f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search (fine-tune)\n",
    "best_config, best_model, results = random_search(\n",
    "    get_dcnn_model, input_train, y_train, input_val, y_val,\n",
    "    random_search_spaces=random_search_spaces_finetune, TARGET=TARGET, NUM_SEARCH=30, EPOCHS=25, PATIENCE=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1681ff12-8040-4001-a195-8741ed058a50",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save best configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bda701-48a7-41ad-bfa2-8f39f700b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TARGET == 'val_avg_f1':\n",
    "    new_best_target = max(results, key=lambda x: x[1][TARGET])[1][TARGET]\n",
    "    if new_best_target > best_target:\n",
    "        best_target = new_best_target\n",
    "        hparams = max(results, key=lambda x: x[1][TARGET])[0]\n",
    "        \n",
    "tuning_result = hparams.copy()\n",
    "tuning_result[TARGET] = best_target\n",
    "tuning_result['n_filters'] = int(tuning_result['n_filters'])\n",
    "tuning_result['h_dim'] = int(tuning_result['h_dim'])\n",
    "tuning_result['batch_size'] = int(tuning_result['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad65896-13e6-4eda-ba19-4338d4feb42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store it\n",
    "output_path = results_dir + task_name + '/best_hparams.json'\n",
    "with open(output_path, 'w') as outf:\n",
    "    json.dump(tuning_result, outf, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def0e078-4969-4a52-ac3b-b4a217b958d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f78d0-c53d-499e-8c0f-e74727ca62eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_dir + task_name + '/best_hparams.json', 'r') as inf:\n",
    "    hparams = json.load(inf)\n",
    "del hparams[TARGET]\n",
    "\n",
    "\n",
    "print(f'Config: {hparams}')\n",
    "model = get_dcnn_model(hparams)\n",
    "solver = Solver(model, input_train, y_train, input_val, y_val, TARGET)\n",
    "solver.train(epochs=50, patience=5, batch_size=hparams['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4e652a-1aaf-454b-9555-ac09c5b28346",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = results_dir + task_name + '/history.png'\n",
    "solver.plot_history(out_path=out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bc9af5-64e0-48a7-92c9-3e388d9374f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6153478e-048c-4a5e-bab8-a8c76ca12e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test = {'text': X_test, 'PoS': X_test_pos, 'extra': test_data_extra.values}\n",
    "y_test = test_data['label']\n",
    "\n",
    "loss, metric = model.evaluate(input_test, y_test)\n",
    "print(f'Test loss: {loss} - Test {TARGET}: {metric}')\n",
    "string = f'Test loss: {loss} - Test {TARGET}: {metric}'\n",
    "\n",
    "y_pred = np.where(model.predict(input_test) > 0.5, 1, 0)\n",
    "report = classification_report(y_test, y_pred, digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a9d48e-1166-4064-979a-6d190b0d0437",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_dir + task_name + '/test_eval.txt', 'w') as outf:\n",
    "    string = f\"Test Loss - Average F1 Score: {loss:.5f} - {metric:.5f}\\n {report}\"\n",
    "    outf.write(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede7a26c-8391-4a45-a3ff-b1dbab06ddbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## KFold + Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb239ce-8970-452c-9e96-25d8ef1acd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_dir + task_name + '/best_hparams.json', 'r') as inf:\n",
    "    hparams = json.load(inf)\n",
    "del hparams[TARGET]\n",
    "\n",
    "input_dev = {'text': X_dev, 'PoS': X_dev_pos, 'extra': dev_data_extra.values, 'label': dev_data['label']}\n",
    "\n",
    "print(f'Config: {hparams}')\n",
    "solver = Solver(None, input_train, y_train, input_val, y_val, TARGET)\n",
    "kfold_models = solver.train_with_kfold(\n",
    "    get_dcnn_model, hparams, input_dev, n_splits=5, \n",
    "    batch_size=hparams['batch_size'], epochs=30, patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a079013-5a5f-4c48-84ab-d4872a6dd4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test = {'text': X_test, 'PoS': X_test_pos, 'extra': test_data_extra.values}\n",
    "y_test = test_data['label']\n",
    "\n",
    "predictions = solver.ensemble_predict(input_test)\n",
    "report = classification_report(y_test, predictions, digits=4)\n",
    "avg_f1 = f1_score(y_test, predictions)\n",
    "\n",
    "print(f'Average F1 Score for Ensemble: {avg_f1:.5f}')\n",
    "print(f'\\n{report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af1687-752a-4c00-8592-3622f982ffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_dir + task_name + '/test_kfold_eval.txt', 'w') as outf:\n",
    "    string = f\"Average F1 Score for Ensemble: {avg_f1:.5f}\\n {report}\"\n",
    "    outf.write(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bacb23-c856-4e54-98e7-1a7b6d6367a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Meta-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30e0ddc-4593-42c2-8a1e-6a10b7dda29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "meta_learners = {\n",
    "    'LogisticRegression': LogisticRegression(), \n",
    "    'RandomForestClassifier': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'XGB': xgb.XGBClassifier(n_estimators=100, random_state=42),\n",
    "    'SVC': SVC(probability=True, kernel='linear', C=1),\n",
    "}\n",
    "\n",
    "for meta_learner in meta_learners.items():\n",
    "    predictions = solver.meta_learner_predict(input_test, meta_learner=meta_learner[1])\n",
    "    report = classification_report(y_test, predictions, digits=4)\n",
    "    print(f'---{meta_learner[0]}---\\n{report}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1d9ed3-c11d-44f9-8816-a74378d95420",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Baseline SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee33db7-7fb3-4896-8dbf-977767d0873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "X_dev_reshaped = X_dev.reshape(X_dev.shape[0], -1)  # This flattens the data\n",
    "\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_dev_reshaped, dev_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efb8b86-5088-4bb6-94ad-d570ed6f65e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_combined_test = np.concatenate((X_test, X_test_pos, test_data_extra), axis=1)\n",
    "# Make predictions on the test data\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], -1)  # This flattens the data\n",
    "y_pred = svm.predict(X_test_reshaped)\n",
    "\n",
    "report = classification_report(test_data['label'], y_pred, digits=4)\n",
    "print(f\"Classification Report:\\n{report}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
