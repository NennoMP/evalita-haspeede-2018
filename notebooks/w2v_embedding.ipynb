{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ab69acb-3033-4fce-91d9-c86c56457b79",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b28cb49-82a8-45bb-90a3-19b6ada43ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "dir_parts = os.getcwd().split(os.path.sep)\n",
    "root_index = dir_parts.index('MyHaSpeeDe-1')\n",
    "root_path = os.path.sep.join(dir_parts[:root_index + 1])\n",
    "sys.path.append(root_path + '/code/')\n",
    "from sentence_statistics import average_sentence_length, max_sentence_length, median_sentence_length, mode_sentence_length, plot_sentence_lengths_distribution\n",
    "from word_embedding import get_key_index_mappings, get_key_index_mappings_ft, get_embedding_matrix, get_embedding_matrix_ft, sentence_to_embedding, data_to_embedding_ft, data_to_embedding\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ed496-b990-495e-870d-0149b8a73134",
   "metadata": {},
   "source": [
    "## Path\n",
    "Loading the pre-processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d501251-3b54-421f-a3ea-2a882a05c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "fb_dir = root_path + '/data/facebook/'\n",
    "tw_dir = root_path + '/data/twitter/'\n",
    "\n",
    "preprocessed_dir = 'preprocessed/'\n",
    "w2v_dir = root_path + '/data/word2vec/'\n",
    "\n",
    "results_dir = root_path + '/results/ConvolutionGRU/'\n",
    "\n",
    "# Filepaths (Facebook dataset)\n",
    "fb_dev_preprocessed_path = fb_dir + 'dev/' + preprocessed_dir + 'fb_dev_preprocessed.csv'\n",
    "fb_test_preprocessed_path = fb_dir + 'test/' + preprocessed_dir + 'fb_test_preprocessed.csv'\n",
    "\n",
    "# Filepaths (Twitter dataset)\n",
    "tw_dev_preprocessed_path = tw_dir + 'dev/' + preprocessed_dir + 'tw_dev_preprocessed.csv'\n",
    "tw_test_preprocessed_path = tw_dir + 'test/' + preprocessed_dir + 'tw_test_preprocessed.csv'\n",
    "\n",
    "# W2V + Corpus\n",
    "w2v_pretrained_path = w2v_dir + 'twitter128.bin' # w2v\n",
    "ft_pretrained_path = w2v_dir + 'cc.it.300.bin' # fasttex\n",
    "dictionary_path = root_path + '/data/italian_words.txt' # vocabulary\n",
    "bad_words_path = root_path + '/data/italian_bad_words.txt' # bad words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4619771-13a0-4355-ae0d-52ae77c0c2a9",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8f32c4-ca2d-45b2-945d-97b2afe765b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06191afa-5c12-497c-bc54-975404febaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook dataset\n",
    "fb_dev_inf = open(fb_dev_preprocessed_path, encoding=\"utf-8\")\n",
    "fb_test_inf = open(fb_test_preprocessed_path, encoding=\"utf-8\")\n",
    "\n",
    "fb_dev = pd.read_csv(fb_dev_inf, sep=',', converters={'tokens': pd.eval, 'lemmas': pd.eval})\n",
    "fb_test = pd.read_csv(fb_test_inf, sep=',', converters={'tokens': pd.eval, 'lemmas': pd.eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160c1ef-3ecc-43ec-93e4-f3386396ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter dataset\n",
    "tw_dev_inf = open(tw_dev_preprocessed_path, encoding=\"utf-8\")\n",
    "tw_test_inf = open(tw_test_preprocessed_path, encoding=\"utf-8\")\n",
    "\n",
    "tw_dev = pd.read_csv(tw_dev_inf, sep=',', converters={'tokens': pd.eval, 'lemmas': pd.eval})\n",
    "tw_test = pd.read_csv(tw_test_inf, sep=',', converters={'tokens': pd.eval, 'lemmas': pd.eval})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474e64e-7b8e-4a91-9234-1777b6420bbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Word Embeddings\n",
    "We leverage a pre-trained W2V model of Italian Twitter embeddings from the Italian NLP Lab [[1]](http://www.italianlp.it/resources/italian-word-embeddings/).\n",
    "Here we provide a tutorial on how to create an embedding matrix for out twitter dataset, leveraging the w2v model. \n",
    "\n",
    "*Note: the same w2v model will be used also for the Facebook dataset (i.e., transfer learning) with some fine-tuning, as a corresponding w2v for Facebook posts was not found.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4b29b9-9e12-4258-8bc1-15aee475eb22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Useful constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499260d3-0c4b-421b-b927-eeca747e5ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "OOV_TOKEN = '<OOV>'\n",
    "\n",
    "samples = fb_dev['tokens']\n",
    "unique_words = set([word for words in samples for word in words])\n",
    "words = [word.lower() for word in unique_words]\n",
    "\n",
    "n_samples = len(samples)\n",
    "n_unique_words = len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dc0a3b-53fb-4bc8-a44a-7627e1735d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'#samples: {n_samples} - #unique words: {n_unique_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01894606-7db2-4d8b-ba43-462b2aba01ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Sentence length analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e4623a-1b54-40ca-8295-bdae23af4e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_len      = average_sentence_length(fb_dev['tokens'])\n",
    "median_len   = median_sentence_length(fb_dev['tokens'])\n",
    "mode_len     = mode_sentence_length(fb_dev['tokens'])\n",
    "max_len      = max_sentence_length(fb_dev['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f28c2-a340-4d49-aff9-f09006258401",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'avg_len: {avg_len} - max_len: {max_len} - median_len: {median_len} - mode_len: {mode_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052db367-e511-4075-9cf6-fa13ffc77be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sentence length distribution dev/test\n",
    "plot_sentence_lengths_distribution(fb_dev, fb_test, dataset='Facebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79982b45-0159-4e62-8128-fffff97e008e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Word2Vec\n",
    "Leveraging a pre-trained W2V model (http://www.italianlp.it/resources/italian-word-embeddings/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b375e9-ef40-4149-b54b-edbb8ef4720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format(w2v_pretrained_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6463f3-2807-4a0b-bda4-837a8d82c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b3748-fc28-4a28-8714-d7e5a14c72a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_set = set(w2v.index_to_key)\n",
    "\n",
    "known_words_w2v = [word for word in words if word in keys_set]\n",
    "unknown_words_w2v = [word for word in words if word not in keys_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315f20e2-82e5-4721-8bd5-84bf701fdf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'(W2V) #known words: {len(known_words_w2v)} - #unknown words: {len(unknown_words_w2v)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f1e0d7-4aa4-4d79-86d2-de9c066669c9",
   "metadata": {},
   "source": [
    "### Embedding matrix\n",
    "We need to add a specific token for unknown words (i.e. *\\<OOV\\>*) in the pre-trained W2V model. Thus, we create our own index-to-key and key-to-index mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defcd5f8-a384-48c6-900d-9a75fbeda155",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_idx, idx_to_key = get_key_index_mappings(w2v, OOV_TOKEN)\n",
    "embedding_matrix, vocab_size = get_embedding_matrix(w2v, idx_to_key, OOV_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3308c377-a8bc-4568-abf4-cdb585497b96",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset to embedding\n",
    "We use truncating and padding to normalize sentences of different lengths to a uniform max sentence length.\n",
    "\n",
    "Common approaches include:\n",
    "- Pre-fixed value\n",
    "- Average sentence length\n",
    "- Maximum sentence length\n",
    "\n",
    "As there is a huge disparity between betwen the average and maximum length in our case, a good approach to avoid too much padding, and consequently additional computational effort, could be to take the average.\n",
    "However, since our dataset is not excessively large we decided to consider the maximum length, so that no information is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b7a71e-aa63-4e0b-a93b-f5f7c78b3c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fb_dev = data_to_embedding(fb_dev['tokens'], embedding_matrix, key_to_idx, truncation=max_len, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122f667a-55d4-4814-a21f-f43bc0361d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fb_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86684c0f-d97d-4cad-b14f-f13171b87d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fb_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c6eab8-3772-4106-8aca-c66da06660a5",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac22a906-1e56-486e-a4d3-1d9d4752426a",
   "metadata": {},
   "source": [
    "Leveraging a pre-trained FastText model (https://fasttext.cc/docs/en/crawl-vectors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0977b58e-3a83-44b1-931d-ed4cd7048fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext.load_model(ft_pretrained_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec7bcb-907f-4159-872c-1d242ae54b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'({len(ft.get_words())}, {ft.get_dimension()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef54837-39f1-4df6-a55a-97ba2197715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_set = set(ft.get_words())\n",
    "\n",
    "known_words_ft = [word for word in words if word in keys_set]\n",
    "unknown_words_ft = [word for word in words if word not in keys_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25166e2-244d-4535-9a33-d6fe7a750d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'(FastText) #known words: {len(known_words_ft)} - #unknown words: {len(unknown_words_ft)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff40ece0-f840-4b24-89ed-e2ec9a0510eb",
   "metadata": {},
   "source": [
    "### Embedding Matrix\n",
    "FastText can handle out-of-vocabulary words by representing them as the sum of the vectors of their character n-grams (sub-units). Thus, no need to manually insert a dedicated token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06981f2-f8b3-45e8-8683-f49f75e61032",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_idx_ft, idx_to_key_ft = get_key_index_mappings_ft(ft)\n",
    "embedding_matrix_ft, vocab_size_ft = get_embedding_matrix_ft(ft, idx_to_key_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed335926-8aeb-44a7-8bac-c9d3d24d2977",
   "metadata": {},
   "source": [
    "### Dataset to embedding\n",
    "As before, truncating and padding are applied to normalize sentences of different lengths to a uniform max sentence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dac1ea-4146-4695-924d-4124a639f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fb_dev_ft = data_to_embedding_ft(fb_dev['tokens'], embedding_matrix_ft, key_to_idx_ft, truncation=max_len, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de72ed1-7b15-4a02-8e29-30d19465edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fb_dev_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca61c22-9daf-4a21-997c-d27d3e5b7326",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fb_dev_ft"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
