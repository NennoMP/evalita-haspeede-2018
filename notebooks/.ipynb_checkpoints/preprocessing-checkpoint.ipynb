{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c580d38-ff62-4460-bf68-66244db59f85",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f4c239-d862-4d75-bf03-07bf5b6ef635",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import csv\n",
    "import gzip\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import unicodedata\n",
    "\n",
    "import emoji\n",
    "import enchant\n",
    "import language_tool_python\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import spacy_udpipe\n",
    "import splitter\n",
    "import treetaggerwrapper\n",
    "import wordninja\n",
    "\n",
    "from itertools import groupby\n",
    "from typing import List, Set\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet, sentiwordnet\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from tokenizer import *\n",
    "\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "dir_parts = os.getcwd().split(os.path.sep)\n",
    "root_index = dir_parts.index('MyHaSpeeDe-1')\n",
    "root_path = os.path.sep.join(dir_parts[:root_index + 1])\n",
    "\n",
    "# Load the Italian model\n",
    "nlp = spacy.load(\"it_core_news_sm\")\n",
    "# Load NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7833c477-92ff-4574-8ee4-de11eb0aeb9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178f492d-362a-4d3e-853e-18c04a4ac7e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Directories\n",
    "fb_dir = root_path + '/data/facebook/'\n",
    "tw_dir = root_path + '/data/twitter/'\n",
    "preprocessed_dir = 'preprocessed/'\n",
    "w2v_dir = root_path + '/data/word2vec/'\n",
    "\n",
    "# Filepaths (Facebook dataset)\n",
    "fb_dev_path = fb_dir + 'dev/' + 'fb_dev.csv'\n",
    "fb_test_path = fb_dir + 'test/' + 'fb_test.csv'\n",
    "\n",
    "fb_dev_preprocessed_path = fb_dir + 'dev/' + preprocessed_dir + 'fb_dev_preprocessed.csv'\n",
    "fb_test_preprocessed_path = fb_dir + 'test/' + preprocessed_dir + 'fb_test_preprocessed.csv'\n",
    "\n",
    "# Filepaths (Twitter dataset)\n",
    "tw_dev_path = tw_dir + 'dev/' + 'tw_dev.csv'\n",
    "tw_test_path = tw_dir + 'test/' + 'tw_test.csv'\n",
    "\n",
    "tw_dev_preprocessed_path = tw_dir + 'dev/' + preprocessed_dir + 'tw_dev_preprocessed.csv'\n",
    "tw_test_preprocessed_path = tw_dir + 'test/' + preprocessed_dir + 'tw_test_preprocessed.csv'\n",
    "\n",
    "# Corpus + Lexicon\n",
    "dictionary_path = root_path + '/data/italian_words.txt' # vocabulary\n",
    "bad_words_path = root_path + '/data/italian_bad_words.txt' # bad words\n",
    "polarity_lexicon_path = root_path + '/data/DPL-IT_lrec2016.txt' # polarity lexicon (pos-neg-neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03085c69-3705-472b-8c42-771845639331",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102651f1-93a4-41b8-b9e1-7d59c58dc78d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de762c9d-cc17-4605-804c-2d5705367fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Facebook dev/test dataset\n",
    "fb_dev_inf = open(fb_dev_path, encoding='utf-8')\n",
    "fb_dev = pd.read_csv(fb_dev_inf, sep=',', header=0)\n",
    "\n",
    "fb_test_inf = open(fb_test_path, encoding='utf-8')\n",
    "fb_test = pd.read_csv(fb_test_inf, sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15842d8e-de98-42da-90c6-630f29665317",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Twitter dev/test dataset\n",
    "tw_dev_inf = open(tw_dev_path, encoding='utf-8')\n",
    "tw_dev = pd.read_csv(tw_dev_inf, sep=',', header=0)\n",
    "\n",
    "tw_test_inf = open(tw_test_path, encoding='utf-8')\n",
    "tw_test = pd.read_csv(tw_test_inf, sep=',', header=0)\n",
    "\n",
    "\"\"\"# Load Facebook dev/test dataset\n",
    "fb_dev_inf = open(fb_dev_preprocessed_path, encoding='utf-8')\n",
    "fb_dev = pd.read_csv(fb_dev_inf, sep=',', header=0)\n",
    "\n",
    "fb_test_inf = open(fb_test_preprocessed_path, encoding='utf-8')\n",
    "fb_test = pd.read_csv(fb_test_inf, sep=',', header=0)\n",
    "\n",
    "# Load Twitter dev/test dataset\n",
    "tw_dev_inf = open(tw_dev_preprocessed_path, encoding='utf-8')\n",
    "tw_dev = pd.read_csv(tw_dev_inf, sep=',', header=0)\n",
    "\n",
    "tw_test_inf = open(tw_test_preprocessed_path, encoding='utf-8')\n",
    "tw_test = pd.read_csv(tw_test_inf, sep=',', header=0)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f385408-b54a-4b9d-a58e-6d6079f8fffc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing\n",
    "The pre-processing phase is performed according to the *Preprocessing* section of [[1]](https://ceur-ws.org/Vol-2263/paper043.pdf) by Bianchini et al..\n",
    "\n",
    "Below a comprehensive list of the pre-processing steps:\n",
    "- Extraction of the first feature: length of the comment;\n",
    "- Extraction of the second feature: percentage of words written in CAPS-LOCK inside a tweet;\n",
    "- Remove mentions and URLs;\n",
    "- Handling special characters and newlines\n",
    "- Conversion of disguised bad words;\n",
    "- Hashtag splitting;\n",
    "- Removal of nearby redundant vowels and/or consonants;\n",
    "- Extraction of the third feature: number of sentences;\n",
    "- Extraction of the fourth feature: number of ‘?’ and ‘!’;\n",
    "- Extraction of the fifth feature: number of ‘.’ and ‘,’;\n",
    "- Punctuation removal;\n",
    "- Translation of emoticons;\n",
    "- Replacement of abbreviations with the respective words;\n",
    "- Replacement of acronyms with the respective words;\n",
    "- Removal of articles, pronouns, prepositions, conjuctions and numbers;\n",
    "- Removal of the laughs;\n",
    "- Removal of accented characters with the respective unaccented characters;\n",
    "- Tokenization;\n",
    "- Lemmatization;\n",
    "- Extraction of the sixth feature: percentage of spelling errors;\n",
    "- Replacement of spelling errors;\n",
    "- Extraction of the seventh feature: number of bad words;\n",
    "- Extraction of the eigth feature: percentage of bad words;\n",
    "- Extraction of the ninth feature: polarity of the message using SentiWordNet;\n",
    "- Extraction of tenth feature: Polarity TextBlob;\n",
    "- Extraction of the final feature: Subjectivity TextBlob;\n",
    "- Part of Speech (PoS) tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d6b69d-fc51-43c1-a98d-efdd032d076b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extraction of the first feature: length of the comment\n",
    "Length of the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9710e3dc-3c72-4ea1-836f-a336cf378bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_feature(text: str) -> int:\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5812a38d-7c08-44f2-a4d8-d93ca9161996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['text_len'] = fb_dev['text'].apply(first_feature)\n",
    "fb_test['text_len'] = fb_test['text'].apply(first_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f4c884-0307-4b9c-ac87-d17f096ab1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['text_len'] = tw_dev['text'].apply(first_feature)\n",
    "tw_test['text_len'] = tw_test['text'].apply(first_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0917000-ac0c-4a3e-990a-998b13797fbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extraction of the second feature: percentage of CAPS-LOCK words\n",
    "Percentage of words written in CAPS-LOCK inside the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22293e14-a42f-4309-8698-29e9dcb8d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_feature(text: str) -> int:\n",
    "    words = text.split()\n",
    "    count_caps = sum(w.isupper() for w in words)\n",
    "    \n",
    "    return (count_caps * 100) // len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ec047-f0b5-4118-9b33-331c830f2fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['n_caps_words'] = fb_dev['text'].apply(second_feature)\n",
    "fb_test['n_caps_words'] = fb_test['text'].apply(second_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30616a09-c2e5-4067-aec3-3524ca392e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['n_caps_words'] = tw_dev['text'].apply(second_feature)\n",
    "tw_test['n_caps_words'] = tw_test['text'].apply(second_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05423d29-8aac-4795-b110-497fcaa055f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Replacing Mentions\n",
    "Note: HaSpeeDe-1 organizers replaced mentions with anonymized placehoders with a different format for each dataset, specifically:\n",
    "- Facebook: *\\<PERSONA_i\\>* --> MENZ;\n",
    "- Twitter: *\\<MENTION_i\\>* --> MENZ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4707c5-6ac4-437f-852e-06d4c466f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_fb_mentions(text: str) -> str:\n",
    "    return re.sub(r'\\<PERSONA_\\d+\\>', 'MENZ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cfcd0a-8b3e-4508-882d-44821d6a501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tw_mentions(text: str) -> str:\n",
    "    return re.sub(r'\\<MENTION_\\d+\\>', 'MENZ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbbae0c-f160-41f7-b496-bd66f726bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['text'] = fb_dev['text'].apply(remove_fb_mentions)\n",
    "fb_test['text'] = fb_test['text'].apply(remove_fb_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cbdc35-e259-4007-a126-6d33dd3e8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['text'] = tw_dev['text'].apply(remove_tw_mentions)\n",
    "tw_test['text'] = tw_test['text'].apply(remove_tw_mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd0fc5a-da75-4161-af4c-930f65565da0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Replacing URLs\n",
    "Note: HaSpeeDe3 organzers replaced URLs with the placehorders *\\<URL\\> --> URL*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e9a2fe-2408-4004-af81-c01effd91efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text: str) -> str:\n",
    "    return re.sub(r'\\<URL\\>', 'URL', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf1b984-9ad2-4292-8653-9bfdacde6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['text'] = fb_dev['text'].apply(remove_urls)\n",
    "fb_test['text'] = fb_test['text'].apply(remove_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75970d64-5072-4f62-b2a0-d604e979aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['text'] = tw_dev['text'].apply(remove_urls)\n",
    "tw_test['text'] = tw_test['text'].apply(remove_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749b6e40-c4ee-464f-a7cc-4d75c70ddbc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Handling special characters and newlines\n",
    "- Replace characters ‘&’, ‘@’ with ‘e’, ‘a’ respectively.\n",
    "- Remove newlines ‘\\n’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a6d7a-8949-4127-92d9-c7f501c71294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_special_chars(text: str) -> str:\n",
    "    text = text.replace('&', 'e')\n",
    "    return text.replace('@', 'a')\n",
    "    \n",
    "def remove_newlines(text: str) -> str:\n",
    "    return re.sub(r'\\n', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cb013d-497e-4ff8-8fbf-d1bb0a2e71fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['text'] = fb_dev['text'].apply(replace_special_chars)\n",
    "fb_dev['text'] = fb_dev['text'].apply(remove_newlines)\n",
    "fb_test['text'] = fb_test['text'].apply(replace_special_chars)\n",
    "fb_test['text'] = fb_test['text'].apply(remove_newlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf5d7c3-a8c5-4670-b095-f988afdf9be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['text'] = tw_dev['text'].apply(replace_special_chars)\n",
    "tw_dev['text'] = tw_dev['text'].apply(remove_newlines)\n",
    "tw_test['text'] = tw_test['text'].apply(replace_special_chars)\n",
    "tw_test['text'] = tw_test['text'].apply(remove_newlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486d4187-e39a-4f1e-813b-8af482d7fcf8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hashtag splitting\n",
    "Hashtag splitting is a crucial and complex phase in preprocessing our Twitter dataset. Since hashtags are often used to compose sentence, we're looking to normalise them into words.\n",
    "\n",
    "The process, according to mentioned papers, can be summarize as follows:\n",
    "- Identify single words within the hastag;\n",
    "- Ignore words with only two characters for efficiency (except for digits);\n",
    "- Reconstruct the original hashtag into a normalized sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb95d9-7dc8-490f-9a54-9523c92091f6",
   "metadata": {},
   "source": [
    "For simplicity and flexibility, before peforming hashtag splitting, we identify and save the original hasthtags in a dedicated column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c7ef6-7f33-4f04-99e0-6784a02e4db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hashtags(text: str) -> List[str]:\n",
    "    return re.findall(r'#\\w+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ca4b34-4279-465c-a29b-61cc3f908eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['hashtags'] = fb_dev['text'].apply(save_hashtags)\n",
    "fb_test['hashtags'] = fb_test['text'].apply(save_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59772381-4353-43e0-9226-ca32051ffc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['hashtags'] = tw_dev['text'].apply(save_hashtags)\n",
    "tw_test['hashtags'] = tw_test['text'].apply(save_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29c40e9-d768-4873-a630-a83f288fd63c",
   "metadata": {},
   "source": [
    "Before performing hashtag splitting, we also need to load our italian dictionary (https://www.sketchengine.eu/italian-word-list/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d478b90-37fb-4e45-8236-38f6d5e6e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dictionary(file: str) -> Set[str]:\n",
    "    word_dict = set()\n",
    "    \n",
    "    with open(file, 'r', encoding='utf-8') as inf:\n",
    "        for line in inf:\n",
    "            word_dict.add(line.strip())\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b2684b-0d2b-4635-be7f-a60c7caea197",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = load_dictionary(dictionary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caa58fe-8628-4bcc-8ce3-ab96e5fef64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_hashtags(text):\n",
    "    \n",
    "    text = f' {text} '\n",
    "    result = re.findall(r'#\\w+', text)\n",
    "\n",
    "    for word in result:\n",
    "        hashtag_content = word[1:] # stripping the hash\n",
    "\n",
    "        new_word = \" \".join(splitter.split(hashtag_content.lower(), 'it_IT'))\n",
    "\n",
    "        # using strip to handle whitespace issues\n",
    "        if not new_word.strip():\n",
    "            new_word = hashtag_content\n",
    "\n",
    "        text = text.replace(word, new_word)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad3b820-973f-4e08-85da-fb00f16af28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['text'] = tw_dev['text'].apply(split_hashtags)\n",
    "tw_test['text'] = tw_test['text'].apply(split_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c809b76-493d-44f8-b3a2-ed6796feabc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Removing duplicate vowels and consonants\n",
    "Removing nearby equal vowels and/or nearby equal consonants if they are more than $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92158fee-3057-4cfb-9410-888f40f76959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words_with_consecutive_triples(df) -> Set[str]:\n",
    "    # Regular expression to match words with three or more consecutive duplicate characters\n",
    "    triple_pattern = r\"(.)\\1{2,}\"\n",
    "    only_digits_pattern = r\"^\\d+$\"  # to exclude words that are purely numbers\n",
    "\n",
    "    unique_words = set()\n",
    "\n",
    "    for tweet in df['text']:\n",
    "        # Tokenize the tweet into words\n",
    "        words = re.findall(r'\\b\\w+\\b', tweet)\n",
    "        for word in words:\n",
    "            # Exclude words that are only numbers\n",
    "            if re.match(only_digits_pattern, word):\n",
    "                continue\n",
    "            \n",
    "            # Check if word has three consecutive duplicate characters\n",
    "            if re.search(triple_pattern, word):\n",
    "                unique_words.add(word.lower())\n",
    "\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeb4447-9330-450c-aeea-dc7d55f61a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = unique_words_with_consecutive_triples(tw_dev)\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1632ede9-1cec-4531-bf2f-c7223aa1a6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = ['a','e','i','o','u','y']\n",
    "\n",
    "def delete_words_with_redundant_characters(text: str) -> str:\n",
    "    words = text.split()\n",
    "\n",
    "    result = \"\"\n",
    "    for word in words:\n",
    "        if word.lower() in word_dict: \n",
    "            result += f\"{word} \"\n",
    "            continue\n",
    "\n",
    "        grouped_word = [list(g) for k, g in groupby(word)]\n",
    "\n",
    "        corrected_word = \"\"\n",
    "        for group in grouped_word:\n",
    "            char = group[0]\n",
    "            corrected_word += char if char in vowels else char * min(2, len(group))\n",
    "        result += f\"{corrected_word} \"\n",
    "\n",
    "    return result.strip()  # remove trailing space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23acfe55-02d3-4246-93ce-3cbcdd35cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['text'] = fb_dev['text'].apply(delete_words_with_redundant_characters)\n",
    "fb_test['text'] = fb_test['text'].apply(delete_words_with_redundant_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f1075d-e411-4d52-8fbc-f0aa89a95092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['text'] = tw_dev['text'].apply(delete_words_with_redundant_characters)\n",
    "tw_test['text'] = tw_test['text'].apply(delete_words_with_redundant_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb385fc8-488c-4b2d-8080-b63483e83001",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conversion of disguised bad words\n",
    "Recognition and conversion of censored or disguised bad-words, i.e. bad-words where some of their middle letters have been replace by special characters so that they're recognizable by humans but not by automated systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e03cb2-903b-4870-bd9c-1a6fb993ce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bad_words(file: str) -> List[str]:\n",
    "    bad_words = []\n",
    "    with open(file, 'r') as inf:\n",
    "        for line in inf:\n",
    "            bad_words.append(line.strip())\n",
    "            \n",
    "    return bad_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a40fa-2942-48d1-b2c3-8cfbb60a8e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = load_bad_words(bad_words_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ca8747-1641-46ca-b5f5-ea833f6ae25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_disguised_bad_words(text: str) -> str:\n",
    "    words = text.split()\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        # Check if first and last characters are letters\n",
    "        if word[0].isalpha() and word[-1].isalpha():\n",
    "            \n",
    "            middle_word = word[1:-1]\n",
    "            # Check if middle part is only special characters or x\n",
    "            if re.match(r'^[.x*@%#$^]+$', middle_word):\n",
    "                \n",
    "                # Match against the list of bad words\n",
    "                for bad_word in bad_words:\n",
    "                    if bad_word.startswith(word[0]) and bad_word.endswith(word[-1]):\n",
    "                        words[i] = bad_word\n",
    "                        break\n",
    "                        \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eced82c-710b-4cea-b0fe-38699a360641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['text'] = fb_dev['text'].apply(convert_disguised_bad_words)\n",
    "fb_test['text'] = fb_test['text'].apply(convert_disguised_bad_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299cb4f0-cf5d-43e0-9018-f29f92a1a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['text'] = tw_dev['text'].apply(convert_disguised_bad_words)\n",
    "tw_test['text'] = tw_test['text'].apply(convert_disguised_bad_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff96df57-a00c-48d2-9a96-e835ead23c9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extraction of the third feature: number of sentences\n",
    "Number of sentences inside the text, i.e. list of words that end with ‘.’, ‘?’, ‘?’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6595c88-438f-4431-9b82-0057f51945bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def third_feature(text: str) -> int:\n",
    "    return len(re.findall(r'[^.!?]*[.!?]', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d58ff55-28ae-45cd-99b1-885bd0f94608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['#sentences'] = fb_dev['text'].apply(third_feature)\n",
    "fb_test['#sentences'] = fb_test['text'].apply(third_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3baebf-7b61-440b-b73c-8283fe09fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['#sentences'] = tw_dev['text'].apply(third_feature)\n",
    "tw_test['#sentences'] = tw_test['text'].apply(third_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1391c6d0-8bdf-4766-9956-0d248e0eb923",
   "metadata": {},
   "source": [
    "## Extraction of the fourth feature: number of question/exclamation marks\n",
    "Number of ‘?’ or ‘!’ inside the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548b7597-2d95-439c-9511-ed089e7ae9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourth_feature(text: str) -> int:\n",
    "    return text.count('?') + text.count('!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b71cde-46e4-480f-bc83-9437a15f701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['#?!'] = fb_dev['text'].apply(fourth_feature)\n",
    "fb_test['#?!'] = fb_test['text'].apply(fourth_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c8ecbc-c6f4-459e-b2f0-b91a08114a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['#?!'] = tw_dev['text'].apply(fourth_feature)\n",
    "tw_test['#?!'] = tw_test['text'].apply(fourth_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d35be03-5308-4fd7-8e9d-f2acdbeebb41",
   "metadata": {},
   "source": [
    "## Extraction of the fifth feature: number of punctuation symbols\n",
    "Number of ‘.’ or ‘,’ inside the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55883ce-adef-4064-aff2-bf955b751ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fifth_feature(text: str) -> int:\n",
    "    return text.count('.') + text.count(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d864922-bf2c-47d9-967a-64b6efc6e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['#.,'] = fb_dev['text'].apply(fifth_feature)\n",
    "fb_test['#.,'] = fb_test['text'].apply(fifth_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788a9cc7-6adf-460b-92cc-7bb39dcadd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['#.,'] = tw_dev['text'].apply(fifth_feature)\n",
    "tw_test['#.,'] = tw_test['text'].apply(fifth_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d34ae3d-5068-4690-964d-d98ea2cf76e1",
   "metadata": {},
   "source": [
    "## Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0689a4-0913-45b3-9b30-2056fe985df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text: str) -> str:\n",
    "    # Translation table for punctuation\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc0510-baf6-4667-ac94-7135643a6bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['text'] = fb_dev['text'].apply(remove_punctuation)\n",
    "fb_test['text'] = fb_test['text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730e3f8-c797-4948-8dff-5ac5dfdc230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['text'] = tw_dev['text'].apply(remove_punctuation)\n",
    "tw_test['text'] = tw_test['text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d257305c-b595-45a6-a96d-d7afbae6b306",
   "metadata": {},
   "source": [
    "## Translating and/or removing emoticons\n",
    "Considering the large presence of emojis in tweets, we translate them with the respective italian translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f7c7c-732a-48f8-bd1b-a5f34d866e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_emoticons(text: str) -> str:\n",
    "    text_result = emoji.demojize(text, language='it')\n",
    "    text_result=re.sub(r':', ' ', text_result)\n",
    "    text_result=re.sub(r'_', ' ', text_result)\n",
    "    return text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27f1bf3-5d47-4f2e-9e26-bde7a3f0b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['text'] = fb_dev['text'].apply(translate_emoticons)\n",
    "fb_test['text'] = fb_test['text'].apply(translate_emoticons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd57514-135c-47d9-bc23-c44238f3e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['text'] = tw_dev['text'].apply(translate_emoticons)\n",
    "tw_test['text'] = tw_test['text'].apply(translate_emoticons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b07a5a-c7b3-4178-aa6d-17b2172f239f",
   "metadata": {},
   "source": [
    "## Replacing abbreviations with respective words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc19392-7c77-41ef-a8b6-7574c22b76ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations_to_words = {\n",
    "    '6': 'sei',\n",
    "    '€': 'euro',\n",
    "    'mld': 'miliardi',\n",
    "    'mln': 'milioni',\n",
    "    'anke': 'anche',\n",
    "    'cm': 'come',\n",
    "    'cmq': 'comunque',\n",
    "    'cs': 'cosa',\n",
    "    'dlla': 'della',\n",
    "    'dlle': 'delle',\n",
    "    'dv': 'dove',\n",
    "    'dx': 'destra',\n",
    "    'fb': 'facebook',\n",
    "    'gov': 'governo',\n",
    "    'grz': 'grazie',\n",
    "    'ita': 'italia',\n",
    "    'ke': 'che',\n",
    "    'ki': 'chi',\n",
    "    'msg': 'messaggio',\n",
    "    'nn': 'non',\n",
    "    'pkè': 'perchè',\n",
    "    'qdo': 'quando',\n",
    "    'qnd': 'quando',\n",
    "    'qlcs': 'qualcosa',\n",
    "    'qst': 'questo',\n",
    "    'sn': 'sono',\n",
    "    'sx': 'sinistra',\n",
    "    'tv': 'televisore',\n",
    "    'tvb': 'ti voglio bene',\n",
    "    'tw': 'twitter',\n",
    "    'x': 'per',\n",
    "    'xchè': 'perchè',\n",
    "    'xkè': 'perchè',\n",
    "    'xò': 'però'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6abda2e-1e8f-4ee3-8ce9-fc9769003eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_abbreviations(text: str) -> str:\n",
    "    words = text.split()\n",
    "    result = [abbreviations_to_words.get(word.lower(), word.lower()) for word in words]\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff47725-a952-41ec-a544-5186211ff502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['text'] = fb_dev['text'].apply(replace_abbreviations)\n",
    "fb_test['text'] = fb_test['text'].apply(replace_abbreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1682539c-f2b1-44b5-bc81-d53cf0939fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['text'] = tw_dev['text'].apply(replace_abbreviations)\n",
    "tw_test['text'] = tw_test['text'].apply(replace_abbreviations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d10323-1ebc-4091-8266-de30f06e4354",
   "metadata": {},
   "source": [
    "## Replacing acronyms with respective words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c3fac-2865-4500-9b02-251dab750532",
   "metadata": {},
   "outputs": [],
   "source": [
    "acronyms_to_words = {\n",
    "    'ama': 'azienda municipale ambiente',\n",
    "    'anpi': 'associazione nazionale partigiani italia',\n",
    "    'cdm': 'consiglio dei ministri',\n",
    "    'cgil': 'confederazione generale lavoro',\n",
    "    'cnel': 'consiglio nazionale dell economia e del lavoro',\n",
    "    'ddl': 'disegno di legge',\n",
    "    'def': 'documento di economia e finanza',\n",
    "    'eu': 'unione europea',\n",
    "    'fdi': 'fratelli di italia',\n",
    "    'ffoo': 'forze dell ordine',\n",
    "    'gc': 'guardia costiera',\n",
    "    'inps': 'istituto nazionale previdenza sociale',\n",
    "    'lgbt': 'lesbiche gay bisessuali transgender',\n",
    "    'lgbtq': 'lesbiche gay bisessuali transgender queer',\n",
    "    'm5s': 'movimento cinque stelle',\n",
    "    'nwo': 'nuovo ordine mondiale',\n",
    "    'ong': 'organizzazione non governativa',\n",
    "    'pd': 'partito democratico',\n",
    "    'pil': 'prodotto interno lordo',\n",
    "    'rai': 'radiotelevisione italiana',\n",
    "    'rdc': 'reddito di cittadinanza',\n",
    "    'sprar': 'sistema protezione richiedenti asilo',\n",
    "    'tav': 'treno alta velocita',\n",
    "    'tg': 'telegiornale',\n",
    "    'ue': 'unione europea',\n",
    "    'usa': 'stati uniti di america',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb05de7-0b0d-482f-9029-8bb93a2b63d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_acronyms(text: str) -> str:\n",
    "    words = text.split()\n",
    "    result = [acronyms_to_words.get(word.lower(), word.lower()) for word in words]\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a13b34-a1e5-4c3d-bb76-018f5d8cc2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['text'] = fb_dev['text'].apply(replace_acronyms)\n",
    "fb_test['text'] = fb_test['text'].apply(replace_acronyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f3eba-e306-4a73-b7a3-7767590f34b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['text'] = tw_dev['text'].apply(replace_acronyms)\n",
    "tw_test['text'] = tw_test['text'].apply(replace_acronyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161fa758-4e6b-45ee-894c-c48bcb046b8c",
   "metadata": {},
   "source": [
    "## Removing articles, pronouns, prepositions, conjuctions and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab83206-609e-4ec1-bba4-add986bfef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [\n",
    "    \"il\", \"lo\", \"la\", \"l'\", \"i\", \"gli\", \"le\", \n",
    "    \"un\", \"uno\", \"una\", \"un'\", \n",
    "    \"del\", \"dello\", \"della\", \"dei\", \"degli\", \"delle\", \n",
    "    \"al\", \"allo\", \"alla\", \"ai\", \"agli\", \"alle\", \n",
    "    \"dal\", \"dallo\", \"dalla\", \"dai\", \"dagli\", \"dalle\", \n",
    "    \"nel\", \"nello\", \"nella\", \"nei\", \"negli\", \"nelle\", \n",
    "    \"sul\", \"sullo\", \"sulla\", \"sui\", \"sugli\", \"sulle\"\n",
    "]\n",
    "\n",
    "pronouns = [\n",
    "    \"io\", \"tu\", \"lui\", \"lei\", \"noi\", \"voi\", \"loro\", \n",
    "    \"mio\", \"mia\", \"miei\", \"mie\", \"tuo\", \"tua\", \"tuoi\", \"tue\", \"suo\", \"sua\", \"suoi\", \"sue\", \n",
    "    \"nostro\", \"nostra\", \"nostri\", \"nostre\", \"vostro\", \"vostra\", \"vostri\", \"vostre\",\n",
    "    \"esso\", \"essa\", \"essi\", \"esse\", \n",
    "    \"chi\", \"cui\", \"che\", \n",
    "    \"questo\", \"questa\", \"questi\", \"queste\", \n",
    "    \"quello\", \"quella\", \"quelli\", \"quelle\", \n",
    "    \"ci\", \"vi\", \"si\", \"ne\", \"se\", \n",
    "    \"me\", \"te\", \"lui\", \"lei\", \"noi\", \"voi\", \"loro\", \"li\", \"le\", \n",
    "    \"qualcuno\", \"qualcosa\", \"nessuno\", \"niente\", \"alcuni\", \"altro\"\n",
    "]\n",
    "\n",
    "prepositions = [\n",
    "    \"di\", \"a\", \"da\", \"in\", \"con\", \"su\", \"per\", \"tra\", \"fra\", \n",
    "    \"sopra\", \"sotto\", \"avanti\", \"dietro\", \"intorno\", \"attraverso\", \"verso\", \n",
    "    \"durante\", \"mediante\", \"entro\", \"senza\", \"vicino\", \"presso\", \n",
    "    \"fino\", \"dopo\", \"contro\", \"tra\", \"fra\"\n",
    "]\n",
    "\n",
    "conjunctions = [\n",
    "    \"e\", \"anche\", \"ma\", \"o\", \"se\", \"perché\", \"quindi\", \"né\", \"che\", \n",
    "    \"come\", \"dunque\", \"mentre\", \"oppure\", \"però\", \"tuttavia\", \n",
    "    \"anche se\", \"benche\", \"quantunque\", \"sebbene\", \"affinché\", \"così\", \n",
    "    \"quando\", \"nonostante\", \"malgrado\", \"benché\", \"finché\", \"purché\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9447e312-595c-444e-bcda-97969628f120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_function_words(text: str) -> str:\n",
    "    # Set containing all undesired words\n",
    "    unwanted_words = set(articles + pronouns + prepositions + conjunctions)\n",
    "    \n",
    "    words = text.split()\n",
    "    filtered_words = [\n",
    "        word for word in words \n",
    "        if word.lower() not in unwanted_words and not word.isnumeric()\n",
    "    ]\n",
    "\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07186b01-9f52-4f39-be5e-cb18c4bb06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['text'] = fb_dev['text'].apply(remove_function_words)\n",
    "fb_test['text'] = fb_test['text'].apply(remove_function_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4bcc47-75ea-485d-873a-0d31c24cda18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['text'] = tw_dev['text'].apply(remove_function_words)\n",
    "tw_test['text'] = tw_test['text'].apply(remove_function_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff7c91c-00dd-40f0-a7df-19e70032b318",
   "metadata": {},
   "source": [
    "## Removing the laughs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2215bb-b687-4b0f-a433-2b10b1adb18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "laughters = ['ha', 'ah', 'he', 'eh', 'hi', 'ih']\n",
    "\n",
    "def remove_laughters(text: str) -> str:\n",
    "    # Create a compound pattern for each laugh type that matches sequences\n",
    "    # of that laugh pattern with any number of characters from the pattern\n",
    "    patterns = ['[' + re.escape(laughter) + ']+' for laughter in laughters]\n",
    "    \n",
    "    # Combine compound patterns\n",
    "    pattern = r'\\b(?:' + '|'.join(patterns) + r')+\\b'\n",
    "    \n",
    "    # Replace the laughter patterns with an empty string\n",
    "    no_laughs = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return no_laughs.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f990dea3-b987-49f4-817f-86e1e858763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['text'] = fb_dev['text'].apply(remove_laughters)\n",
    "fb_test['text'] = fb_test['text'].apply(remove_laughters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132d463-3c94-4311-b366-0e15bb6a2530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['text'] = tw_dev['text'].apply(remove_laughters)\n",
    "tw_test['text'] = tw_test['text'].apply(remove_laughters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc115652-33d7-4398-b64a-53e0ed922f6c",
   "metadata": {},
   "source": [
    "## Replacing accented characters\n",
    "Replacement of accented characters with their unaccented counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29511237-027c-4d70-a1d7-382de97c59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(text: str) -> str:\n",
    "    nfkd_form = unicodedata.normalize('NFKD', text)\n",
    "    return ''.join([c for c in nfkd_form if not unicodedata.combining(c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe97162-238b-4df4-a4ce-e50d4a52e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['text'] = fb_dev['text'].apply(remove_accents)\n",
    "fb_test['text'] = fb_test['text'].apply(remove_accents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad061e1b-9f4f-4d4f-8c03-845a390447cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['text'] = tw_dev['text'].apply(remove_accents)\n",
    "tw_test['text'] = tw_test['text'].apply(remove_accents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ad3bc7-5046-4a41-a9f2-089a9dcf157b",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c70f70-cca2-459b-8037-0515e75921a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text: str) -> List[str]:\n",
    "    tknzr=SocialTokenizer(lowercase=False)\n",
    "    return tknzr.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a4c5df-1092-4401-b876-700367e9138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['tokens'] = fb_dev['text'].apply(tokenization)\n",
    "fb_test['tokens'] = fb_test['text'].apply(tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a00aa-7d17-47d7-8d4c-cbb1a1b2c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['tokens'] = tw_dev['text'].apply(tokenization)\n",
    "tw_test['tokens'] = tw_test['text'].apply(tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aa06f0-a73b-4a8a-94d2-8f3ddb4c6c9c",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f2e7c-d5cc-4733-8581-ef1e3551f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text: str) -> List[str]:\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae54cc-f3c7-446b-93bf-ae8e6bfd2d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['lemmas'] = fb_dev['text'].apply(lemmatization)\n",
    "fb_test['lemmas'] = fb_test['text'].apply(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc535a71-31ff-47b8-9911-b57f46766986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['lemmas'] = tw_dev['text'].apply(lemmatization)\n",
    "tw_test['lemmas'] = tw_test['text'].apply(lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdee704b-2169-40d7-922c-308a670fed3c",
   "metadata": {},
   "source": [
    "## Extraction of the sixth feature: percentage of spelling errors\n",
    "Percentage of spelling errors in the tweet. To do this, a word is compared with a italian vocabulary corpora and if not present then it is a spelling error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3f7233-4f7f-4451-95f0-99b187eb86a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sixth_feature(text: str) -> int:\n",
    "    words = text.split()\n",
    "    return sum(1 for word in words if word.lower() not in word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e752f45f-e856-4415-a34d-9811d7282870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['#wrong_spellings'] = fb_dev['text'].apply(sixth_feature)\n",
    "fb_test['#wrong_spellings'] = fb_test['text'].apply(sixth_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c1d087-0160-4155-a7ab-155f311f9b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['#wrong_spellings'] = tw_dev['text'].apply(sixth_feature)\n",
    "tw_test['#wrong_spellings'] = tw_test['text'].apply(sixth_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2079158-eef6-4abe-9aea-b059128c2720",
   "metadata": {},
   "source": [
    "## Extraction of the seventh feature: number of bad words\n",
    "Number of bad words in the tweet, leveraging a italian bad words corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68053476-ae6c-4cf4-bb4a-b1d7ea4ef443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seventh_feature(text: str) -> int:\n",
    "    words = text.split()\n",
    "    return sum(1 for word in words if word.lower() in bad_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721765ef-f8db-4dd5-a597-9c0728870984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['#bad_words'] = fb_dev['text'].apply(seventh_feature)\n",
    "fb_test['#bad_words'] = fb_test['text'].apply(seventh_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc77f16f-9240-4ab0-9285-63bf2e050c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['#bad_words'] = tw_dev['text'].apply(seventh_feature)\n",
    "tw_test['#bad_words'] = tw_test['text'].apply(seventh_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558447d8-cb7c-4327-9179-c3003476152b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extraction of the eighth feature: percentage of bad words\n",
    "Percentage of bad words in the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d27a998-7bdd-433b-8c26-18f3c4534255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eighth_feature(text: str) -> int:\n",
    "    words = text.split()\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    n_bad_words = sum(1 for word in words if word.lower() in bad_words)\n",
    "    return (n_bad_words*100) // len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ef1a9e-96a6-4327-98ab-98a22bc3e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['%bad_words'] = fb_dev['text'].apply(eighth_feature)\n",
    "fb_test['%bad_words'] = fb_test['text'].apply(eighth_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31359555-6a07-4399-98a3-4ccfc8ed9738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['%bad_words'] = tw_dev['text'].apply(eighth_feature)\n",
    "tw_test['%bad_words'] = tw_test['text'].apply(eighth_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7d232-e843-4bc6-b3cb-85b8857c43fa",
   "metadata": {},
   "source": [
    "## Extraction of the ninth feature: polarity SentiWordNet\n",
    "Polarity of the message using SentiWordNet. Specifically, each message is ytanslated using TextBlob and then the polarity is computed, since SentiWordNet was create to find polarity in English sentences.\n",
    "\n",
    "*Note: the polarity of a sentence is computed as the worst polarity score across words.*\n",
    "\n",
    "Firstly, we store the english translation of each sentence. Then, the polarity is computed w.r.t. the english translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec52755a-d47f-4ab7-983b-832adf9d21d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ita_to_eng(lemmas: List[str]) -> List[str]:\n",
    "    translated_lemmas = []\n",
    "    for lemma in lemmas:\n",
    "        # Translate the text to English\n",
    "        blob = TextBlob(lemma)\n",
    "        try:\n",
    "            translated_lemma = blob.translate(from_lang='it', to='en')\n",
    "            translated_lemmas.append(str(translated_lemma))\n",
    "        except Exception as e:\n",
    "            #print(f'Translation error: {e} - For Text {lemma}')\n",
    "            translated_lemmas.append(str(lemma)) # Append original text in case of errors\n",
    "        \n",
    "    return translated_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540bf20d-5e13-46e2-bd04-1c8363233cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "#fb_dev['lemmas_en'] = fb_dev['lemmas'].apply(ita_to_eng)\n",
    "#fb_test['lemmas_en'] = fb_test['lemmas'].apply(ita_to_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f041d45-5040-4cc7-b8d9-fd4aaec44636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "#tw_dev['lemmas_en'] = tw_dev['lemmas'].apply(ita_to_eng)\n",
    "#tw_test['lemmas_en'] = tw_test['lemmas'].apply(ita_to_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4b55ee-5b28-48ed-9002-057cad7ba4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polarity_sentiwordnet(text: str) -> str:\n",
    "    try:\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        \n",
    "        tokens = TextBlob(text).words\n",
    "        polarity = float('inf')\n",
    "\n",
    "        for token in tokens:\n",
    "            synsets = list(sentiwordnet.senti_synsets(token))\n",
    "\n",
    "            if synsets:\n",
    "                # Get the average polarity for all possible senses\n",
    "                synset = synsets[0]\n",
    "                polarity = min(polarity, synset.pos_score() - synset.neg_score())\n",
    "                \n",
    "        if polarity == float('inf'):\n",
    "            return 0.0\n",
    "\n",
    "        return polarity\n",
    "    except Exception as e:\n",
    "        print(f'Error computing polarity {e} - {text}')\n",
    "        return 0.0  # Return a neutral polarity in case of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc135108-7fc0-445e-a348-52d414f6f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "#fb_dev['polaritySentiWordNet'] = fb_dev['text_en'].apply(get_polarity_sentiwordnet)\n",
    "#fb_test['polaritySentiWordNet'] = fb_test['text_en'].apply(get_polarity_sentiwordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4658f23-46f4-4ee6-befb-08b82dcd3288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "#tw_dev['polaritySentiWordNet'] = tw_dev['text_en'].apply(get_polarity_sentiwordnet)\n",
    "#tw_test['polaritySentiWordNet'] = tw_test['text_en'].apply(get_polarity_sentiwordnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633444db-cea0-45f3-9b0a-e83b148f2d34",
   "metadata": {},
   "source": [
    "## Extraction of tenth feature: polarity TextBlob\n",
    "Polarity TextBlob, where the polarity value is computed using a TextBlob function. As before, the message is first translated to english.\n",
    "\n",
    "*Note: the polarity of a sentence is computed as the worst polarity score across words.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc181904-ec09-4d97-9348-c885656ab222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polarity_textblob(text: str) -> float:\n",
    "    try:\n",
    "        blob = TextBlob(text)\n",
    "        return blob.sentiment.polarity\n",
    "    except Exception as e:\n",
    "        print(f'Error computing polarity {e} - {text}')\n",
    "        return 0.0  # Return the lowest polarity value found, even if it's not neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54efaf3c-d48d-46d4-b17e-02fe8e404601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "#fb_dev['polarityTextBlob'] = fb_dev['text_en'].apply(get_polarity_textblob)\n",
    "#fb_test['polarityTextBlob'] = fb_test['text_en'].apply(get_polarity_textblob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22cb5c-67df-4e6b-85be-25d01b0b03bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "#tw_dev['polarityTextBlob'] = tw_dev['text_en'].apply(get_polarity_textblob)\n",
    "#tw_test['polarityTextBlob'] = tw_test['text_en'].apply(get_polarity_textblob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a9cc0d-2dc1-44c9-aaf7-95f8b30e0876",
   "metadata": {},
   "source": [
    "## Extraction of the eleventh feature: subjectivity TextBlob\n",
    "Subjectivity TextBlob, another value computed using a TextBlob function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2423636-7560-451a-84bf-1b239e1b5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjectivity_textblob(text: str) -> float:\n",
    "    try:\n",
    "        blob = TextBlob(text)\n",
    "        subjectivity = blob.sentiment.subjectivity\n",
    "        return subjectivity\n",
    "    except Exception as e:\n",
    "        return 0.0  # Return a neutral subjectivity in case of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ff939-4cc1-4db2-a9b2-1a2fc831fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['subjectivityTextBlob'] = fb_dev['text_en'].apply(get_subjectivity_textblob)\n",
    "fb_test['subjectivityTextBlob'] = fb_test['text_en'].apply(get_subjectivity_textblob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5498c38a-0a1b-4b5c-8374-926da17ba99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['subjectivityTextBlob'] = tw_dev['text'].apply(get_subjectivity_textblob)\n",
    "tw_test['subjectivityTextBlob'] = tw_test['text'].apply(get_subjectivity_textblob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d99e2d-96b2-49a8-9a87-b38e284ed2ba",
   "metadata": {},
   "source": [
    "## Extraction of the final feature: polarity with Italian lexicon\n",
    "Polarity based on the *Distributional Polarity Lexicon (IT)* (Castellucci et al, 2016). It consists of polarity scorse for positivity - negativity - neturality.\n",
    "\n",
    "*Note: the polarity of a sentence is computed by taking the highest 'negativity' value across words.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd10ede-6e84-4ca3-897e-a54ca8e09980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_negativity_lexicon(lexicon_file):\n",
    "    negativity_lexicon = {}\n",
    "    with open(lexicon_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                word, scores = parts\n",
    "                positive, negative, neutral = map(float, scores.split(','))\n",
    "                negativity_lexicon[word] = negative  # Store the negativity score\n",
    "                \n",
    "    return negativity_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4cc612-f5d8-4f1e-9db1-eaa8bbb37bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "negativity_lexicon = load_negativity_lexicon(polarity_lexicon_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dfdb0a-00b2-42ad-ae12-4a95c48964f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polarity_DPL(tokens: List[str], lexicon=negativity_lexicon) -> float:\n",
    "    polarity = 0.0\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in lexicon:\n",
    "            negativity_score = lexicon[token]\n",
    "            polarity = max(polarity, negativity_score)\n",
    "\n",
    "    return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37545e08-c099-479c-9577-fc6b43f07f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['polarityDPL'] = fb_dev['tokens'].apply(get_polarity_DPL)\n",
    "fb_test['polarityDPL'] = fb_test['tokens'].apply(get_polarity_DPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d2e7ff-e745-419e-8ffa-ac9d02d388b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['polarityDPL'] = tw_dev['tokens'].apply(get_polarity_DPL)\n",
    "tw_test['polarityDPL'] = tw_test['tokens'].apply(get_polarity_DPL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64733e37-6102-4b21-ae87-b6b1c86a0c20",
   "metadata": {},
   "source": [
    "## PoS\n",
    "Part of Speech (PoS) aims at grouping words by their grammar class (e.g. noun, verb, adjective, etc.).\n",
    "PoS tagging can provide useful information about a word, and its neighbours, role in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd0031b-523a-4943-8b01-9ffb04c89260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PoS_tagging(text: str) -> List[str]:\n",
    "    pos = []\n",
    "    \n",
    "    tokens = nlp(text)\n",
    "    for token in tokens:\n",
    "        pos.append(token.pos_)\n",
    "        \n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ba570-408d-4e3e-8c5c-f3b2195bef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook\n",
    "fb_dev['PoS'] = fb_dev['text'].apply(PoS_tagging)\n",
    "fb_test['PoS'] = fb_test['text'].apply(PoS_tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165fa7a9-9290-4732-a013-059f1f757281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter\n",
    "tw_dev['PoS'] = tw_dev['text'].apply(PoS_tagging)\n",
    "tw_test['PoS'] = tw_test['text'].apply(PoS_tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35c57be-f200-4be8-9aff-6a31ccf28db4",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff035c2b-32fc-42c7-b9df-32230615ae3a",
   "metadata": {},
   "source": [
    "## Store pre-processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cd2c50-7031-4229-aec0-e0e18371144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook dataset\n",
    "fb_dev.to_csv(fb_dir + 'dev/' + preprocessed_dir + 'fb_dev_preprocessed.csv', index=False)\n",
    "fb_test.to_csv(fb_dir + 'test/' + preprocessed_dir + 'fb_test_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97224250-4aa7-4847-9f52-82013a167bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter dataset\n",
    "tw_dev.to_csv(tw_dir + 'dev/' + preprocessed_dir + 'tw_dev_preprocessed.csv', index=False)\n",
    "tw_test.to_csv(tw_dir + 'test/' + preprocessed_dir + 'tw_test_preprocessed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
